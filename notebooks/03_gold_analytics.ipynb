{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer - Data Serving with SQL & MLlib\n",
    "\n",
    "## Lending Club Loan Data Pipeline\n",
    "\n",
    "**Use Case:** Predict loan default risk and analyze factors affecting loan approval\n",
    "\n",
    "This notebook implements the Gold (Serving) layer of the Medallion Architecture:\n",
    "- Business analytics using **Spark SQL**\n",
    "- Machine Learning using **MLlib** (Loan Default Prediction)\n",
    "- Create aggregated tables ready for dashboards and applications\n",
    "\n",
    "**Note:** Unlike the Silver layer, this notebook uses high-level APIs (DataFrames, SQL, MLlib) as permitted by the project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ML imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, \n",
    "    StringIndexer, \n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    Imputer\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, \n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import (\n",
    "    BinaryClassificationEvaluator,\n",
    "    MulticlassClassificationEvaluator\n",
    ")\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LendingClub-Gold-Layer\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver input path: ../data/medallion/silver/\n",
      "Gold output path: ../data/medallion/gold/\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "SILVER_PATH = \"../data/medallion/silver/\"\n",
    "GOLD_PATH = \"../data/medallion/gold/\"\n",
    "\n",
    "SILVER_ACCEPTED_PATH = os.path.join(SILVER_PATH, \"accepted_loans\")\n",
    "SILVER_REJECTED_PATH = os.path.join(SILVER_PATH, \"rejected_loans\")\n",
    "\n",
    "# Create gold directory\n",
    "os.makedirs(GOLD_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Silver input path: {SILVER_PATH}\")\n",
    "print(f\"Gold output path: {GOLD_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Silver Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted loans: 2,258,994 rows\n",
      "Rejected loans: 27,647,453 rows\n",
      "\n",
      "Accepted loans columns: 30\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data from Silver layer\n",
    "loans_df = spark.read.parquet(SILVER_ACCEPTED_PATH)\n",
    "rejected_df = spark.read.parquet(SILVER_REJECTED_PATH)\n",
    "\n",
    "print(f\"Accepted loans: {loans_df.count():,} rows\")\n",
    "print(f\"Rejected loans: {rejected_df.count():,} rows\")\n",
    "print(f\"\\nAccepted loans columns: {len(loans_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_amnt: float (nullable = true)\n",
      " |-- term: integer (nullable = true)\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- installment: float (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_length: integer (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: float (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_status_binary: integer (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- open_acc: float (nullable = true)\n",
      " |-- pub_rec: float (nullable = true)\n",
      " |-- revol_bal: float (nullable = true)\n",
      " |-- revol_util: float (nullable = true)\n",
      " |-- total_acc: float (nullable = true)\n",
      " |-- fico_range_low: float (nullable = true)\n",
      " |-- fico_range_high: float (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- delinq_2yrs: float (nullable = true)\n",
      " |-- inq_last_6mths: float (nullable = true)\n",
      " |-- mort_acc: float (nullable = true)\n",
      " |-- pub_rec_bankruptcies: float (nullable = true)\n",
      " |-- fico_avg: float (nullable = true)\n",
      " |-- loan_to_income: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show schema\n",
    "loans_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----------+-----------+------------------+--------+\n",
      "|loan_amnt|int_rate|grade|annual_inc|loan_status|loan_status_binary|fico_avg|\n",
      "+---------+--------+-----+----------+-----------+------------------+--------+\n",
      "|   3600.0|   13.99|    C|   55000.0| Fully Paid|                 0|   677.0|\n",
      "|  24700.0|   11.99|    C|   65000.0| Fully Paid|                 0|   717.0|\n",
      "|  20000.0|   10.78|    B|   63000.0| Fully Paid|                 0|   697.0|\n",
      "|  35000.0|   14.85|    C|  110000.0|    Current|                 0|   787.0|\n",
      "|  10400.0|   22.45|    F|  104433.0| Fully Paid|                 0|   697.0|\n",
      "|  11950.0|   13.44|    C|   34000.0| Fully Paid|                 0|   692.0|\n",
      "|  20000.0|    9.17|    B|  180000.0| Fully Paid|                 0|   682.0|\n",
      "|  20000.0|    8.49|    B|   85000.0| Fully Paid|                 0|   707.0|\n",
      "|  10000.0|    6.49|    A|   85000.0| Fully Paid|                 0|   687.0|\n",
      "|   8000.0|   11.48|    B|   42000.0| Fully Paid|                 0|   702.0|\n",
      "+---------+--------+-----+----------+-----------+------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview data\n",
    "loans_df.select(\n",
    "    'loan_amnt', 'int_rate', 'grade', 'annual_inc', \n",
    "    'loan_status', 'loan_status_binary', 'fico_avg'\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp views created: 'loans', 'rejected'\n"
     ]
    }
   ],
   "source": [
    "# Register as SQL temp view for queries\n",
    "loans_df.createOrReplaceTempView(\"loans\")\n",
    "rejected_df.createOrReplaceTempView(\"rejected\")\n",
    "\n",
    "print(\"Temp views created: 'loans', 'rejected'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part A: Business Analytics with Spark SQL\n",
    "\n",
    "This section demonstrates SQL capabilities for business intelligence and reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------+-----------------+-----------------+--------------+\n",
      "|total_loans| total_funded|avg_loan_amount|avg_interest_rate|avg_annual_income|avg_fico_score|\n",
      "+-----------+-------------+---------------+-----------------+-----------------+--------------+\n",
      "|    2258994|3.39840788E10|        15043.9|            13.09|         77969.52|         701.0|\n",
      "+-----------+-------------+---------------+-----------------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(annual_inc), 2) as avg_annual_income,\n",
    "        ROUND(AVG(fico_avg), 0) as avg_fico_score\n",
    "    FROM loans\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+------------------+-------+----------+\n",
      "|loan_status                                        |loan_status_binary|count  |percentage|\n",
      "+---------------------------------------------------+------------------+-------+----------+\n",
      "|Fully Paid                                         |0                 |1076457|47.65     |\n",
      "|Current                                            |0                 |877044 |38.82     |\n",
      "|Charged Off                                        |1                 |268491 |11.89     |\n",
      "|Late (31-120 days)                                 |1                 |21444  |0.95      |\n",
      "|In Grace Period                                    |0                 |8427   |0.37      |\n",
      "|Late (16-30 days)                                  |1                 |4346   |0.19      |\n",
      "|Does not meet the credit policy. Status:Fully Paid |0                 |1984   |0.09      |\n",
      "|Does not meet the credit policy. Status:Charged Off|1                 |761    |0.03      |\n",
      "|Default                                            |1                 |40     |0.00      |\n",
      "+---------------------------------------------------+------------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loan status distribution\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        loan_status,\n",
    "        loan_status_binary,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM loans\n",
    "    GROUP BY loan_status, loan_status_binary\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Default Rate Analysis by Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+------------+-----------------+---------------+-------------+\n",
      "|grade|total_loans|defaults|default_rate|avg_interest_rate|avg_loan_amount| total_funded|\n",
      "+-----+-----------+--------+------------+-----------------+---------------+-------------+\n",
      "|    A|     432747|   15876|        3.67|             7.08|       14599.73| 6.31798985E9|\n",
      "|    B|     663130|   58429|        8.81|            10.68|       14170.49| 9.39687965E9|\n",
      "|    C|     649552|   94795|       14.59|            14.14|       15034.86|   9.765923E9|\n",
      "|    D|     324099|   66970|       20.66|            18.14|       15709.13|  5.0913131E9|\n",
      "|    E|     135542|   38760|       28.60|            21.83|       17452.09|2.365490575E9|\n",
      "|    F|      41773|   15349|       36.74|            25.45|       19123.56| 7.98848475E8|\n",
      "|    G|      12151|    4903|       40.35|            28.07|       20379.73|  2.4763415E8|\n",
      "+-----+-----------+--------+------------+-----------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default rate by grade - KEY BUSINESS METRIC\n",
    "default_by_grade = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        grade,\n",
    "        COUNT(*) as total_loans,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded\n",
    "    FROM loans\n",
    "    GROUP BY grade\n",
    "    ORDER BY grade\n",
    "\"\"\")\n",
    "\n",
    "default_by_grade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//default_rate_by_grade\n"
     ]
    }
   ],
   "source": [
    "# Save as Gold table\n",
    "default_by_grade.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/default_rate_by_grade\")\n",
    "print(f\"Saved: {GOLD_PATH}/default_rate_by_grade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Default Rate Analysis by Sub-Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+--------+------------+-----------------+--------+\n",
      "|grade|sub_grade|total_loans|defaults|default_rate|avg_interest_rate|avg_fico|\n",
      "+-----+---------+-----------+--------+------------+-----------------+--------+\n",
      "|    A|       A1|      86720|    1596|        1.84|              5.6|   748.0|\n",
      "|    A|       A2|      69523|    1977|        2.84|             6.55|   741.0|\n",
      "|    A|       A3|      73130|    2394|        3.27|             7.09|   730.0|\n",
      "|    A|       A4|      95809|    4017|        4.19|             7.56|   725.0|\n",
      "|    A|       A5|     107565|    5892|        5.48|             8.19|   717.0|\n",
      "|    B|       B1|     125256|    8297|        6.62|             9.08|   709.0|\n",
      "|    B|       B2|     126536|    9353|        7.39|             9.97|   705.0|\n",
      "|    B|       B3|     131439|   11681|        8.89|            10.71|   701.0|\n",
      "|    B|       B4|     139704|   13690|        9.80|            11.37|   699.0|\n",
      "|    B|       B5|     140195|   15408|       10.99|            12.01|   696.0|\n",
      "|    C|       C1|     145803|   17896|       12.27|            12.78|   693.0|\n",
      "|    C|       C2|     131008|   18074|       13.80|            13.54|   692.0|\n",
      "|    C|       C3|     129092|   18664|       14.46|             14.1|   691.0|\n",
      "|    C|       C4|     127019|   20569|       16.19|            14.88|   690.0|\n",
      "|    C|       C5|     116630|   19592|       16.80|            15.77|   689.0|\n",
      "|    D|       D1|      81726|   15605|       19.09|            16.66|   687.0|\n",
      "|    D|       D2|      72829|   14479|       19.88|             17.6|   686.0|\n",
      "|    D|       D3|      64735|   13274|       20.51|            18.39|   686.0|\n",
      "|    D|       D4|      56840|   12585|       22.14|            19.07|   685.0|\n",
      "|    D|       D5|      47969|   11027|       22.99|            20.06|   685.0|\n",
      "|    E|       E1|      33556|    9118|       27.17|            20.33|   685.0|\n",
      "|    E|       E2|      29910|    8544|       28.57|            21.01|   684.0|\n",
      "|    E|       E3|      26686|    7708|       28.88|            21.88|   685.0|\n",
      "|    E|       E4|      22738|    6781|       29.82|            22.78|   684.0|\n",
      "|    E|       E5|      22652|    6609|       29.18|            24.11|   684.0|\n",
      "+-----+---------+-----------+--------+------------+-----------------+--------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default rate by sub-grade - more granular view\n",
    "default_by_subgrade = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        grade,\n",
    "        sub_grade,\n",
    "        COUNT(*) as total_loans,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(fico_avg), 0) as avg_fico\n",
    "    FROM loans\n",
    "    GROUP BY grade, sub_grade\n",
    "    ORDER BY grade, sub_grade\n",
    "\"\"\")\n",
    "\n",
    "default_by_subgrade.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//default_rate_by_subgrade\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "default_by_subgrade.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/default_rate_by_subgrade\")\n",
    "print(f\"Saved: {GOLD_PATH}/default_rate_by_subgrade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------+--------+------------+----------+--------+\n",
      "|state|total_loans| total_funded|defaults|default_rate|avg_income|avg_fico|\n",
      "+-----+-----------+-------------+--------+------------+----------+--------+\n",
      "|   CA|     314298|4.803618425E9|   42244|       13.44|  83875.68|   700.0|\n",
      "|   NY|     186324|2.766027925E9|   26742|       14.35|  81083.59|   701.0|\n",
      "|   TX|     186144|  2.9275107E9|   24092|       12.94|   82813.7|   701.0|\n",
      "|   FL|     161893|2.331124025E9|   22779|       14.07|  73215.91|   699.0|\n",
      "|   IL|      91113|1.409321675E9|   10242|       11.24|  79937.53|   701.0|\n",
      "|   NJ|      83091|1.315369975E9|   11198|       13.48|  88641.03|   701.0|\n",
      "|   PA|      76882|1.130945525E9|   10380|       13.50|  73995.17|   701.0|\n",
      "|   OH|      75073|1.076166525E9|    9676|       12.89|  69377.09|   701.0|\n",
      "|   GA|      74153|  1.1359375E9|    8833|       11.91|  77902.72|   700.0|\n",
      "|   VA|      62907|1.011978375E9|    8340|       13.26|  85166.59|   701.0|\n",
      "|   NC|      62683| 9.29493175E8|    8609|       13.73|  73908.57|   701.0|\n",
      "|   MI|      58745|   8.411527E8|    7795|       13.27|  71691.47|   700.0|\n",
      "|   MD|      53987|   8.564501E8|    7419|       13.74|  86952.82|   700.0|\n",
      "|   AZ|      53711| 7.79659375E8|    7011|       13.05|  74278.13|   699.0|\n",
      "|   MA|      51763|  8.1133185E8|    6483|       12.52|  82822.25|   703.0|\n",
      "+-----+-----------+-------------+--------+------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default rate by state - TOP 10 states by loan volume\n",
    "state_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        addr_state as state,\n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(annual_inc), 2) as avg_income,\n",
    "        ROUND(AVG(fico_avg), 0) as avg_fico\n",
    "    FROM loans\n",
    "    WHERE addr_state IS NOT NULL\n",
    "    GROUP BY addr_state\n",
    "    ORDER BY total_loans DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "state_analysis.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//loan_analysis_by_state\n"
     ]
    }
   ],
   "source": [
    "# Full state analysis for Gold layer\n",
    "full_state_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        addr_state as state,\n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(annual_inc), 2) as avg_income,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(fico_avg), 0) as avg_fico\n",
    "    FROM loans\n",
    "    WHERE addr_state IS NOT NULL\n",
    "    GROUP BY addr_state\n",
    "    ORDER BY total_loans DESC\n",
    "\"\"\")\n",
    "\n",
    "full_state_analysis.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/loan_analysis_by_state\")\n",
    "print(f\"Saved: {GOLD_PATH}/loan_analysis_by_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loan Purpose Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+---------------+--------+------------+---------------+-----------------+\n",
      "|purpose           |total_loans|total_funded   |defaults|default_rate|avg_loan_amount|avg_interest_rate|\n",
      "+------------------+-----------+---------------+--------+------------+---------------+-----------------+\n",
      "|debt_consolidation|1276906    |2.0384028775E10|180261  |14.12       |15963.61       |13.52            |\n",
      "|credit_card       |516646     |7.913234825E9  |54878   |10.62       |15316.55       |11.7             |\n",
      "|home_improvement  |150320     |2.203918675E9  |17338   |11.53       |14661.51       |12.62            |\n",
      "|other             |139330     |1.460089175E9  |18417   |13.22       |10479.36       |14.24            |\n",
      "|major_purchase    |50415      |6.393208E8     |6173    |12.24       |12681.16       |12.76            |\n",
      "|medical           |27460      |2.601001E8     |3753    |13.67       |9471.96        |13.63            |\n",
      "|small_business    |24673      |4.05632325E8   |5059    |20.50       |16440.33       |15.26            |\n",
      "|car               |24003      |2.254797E8     |2378    |9.91        |9393.81        |12.18            |\n",
      "|vacation          |15520      |9.8642875E7    |1929    |12.43       |6355.86        |13.45            |\n",
      "|moving            |15372      |1.2885785E8    |2449    |15.93       |8382.63        |14.74            |\n",
      "|house             |14126      |2.2176955E8    |1827    |12.93       |15699.39       |14.4             |\n",
      "|wedding           |2355       |2.467065E7     |292     |12.40       |10475.86       |14.15            |\n",
      "|renewable_energy  |1444       |1.55289E7      |240     |16.62       |10754.09       |14.73            |\n",
      "|educational       |424        |2804600.0      |88      |20.75       |6614.62        |12.14            |\n",
      "+------------------+-----------+---------------+--------+------------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default rate by loan purpose\n",
    "purpose_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        purpose,\n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate\n",
    "    FROM loans\n",
    "    WHERE purpose IS NOT NULL\n",
    "    GROUP BY purpose\n",
    "    ORDER BY total_loans DESC\n",
    "\"\"\")\n",
    "\n",
    "purpose_analysis.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//loan_analysis_by_purpose\n"
     ]
    }
   ],
   "source": [
    "purpose_analysis.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/loan_analysis_by_purpose\")\n",
    "print(f\"Saved: {GOLD_PATH}/loan_analysis_by_purpose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------------------+--------+------------+---------------+-----------------+\n",
      "|year|total_loans|total_funded_millions|defaults|default_rate|avg_loan_amount|avg_interest_rate|\n",
      "+----+-----------+---------------------+--------+------------+---------------+-----------------+\n",
      "|2007|        599|                 4.95|     158|       26.38|        8267.57|            11.85|\n",
      "|2008|       2393|                21.12|     496|       20.73|        8825.43|            12.06|\n",
      "|2009|       5281|                51.93|     723|       13.69|        9833.03|            12.44|\n",
      "|2010|      12537|               131.99|    1757|       14.01|       10528.24|            11.99|\n",
      "|2011|      21721|               261.68|    3297|       15.18|        12047.5|            12.22|\n",
      "|2012|      53367|               718.41|    8644|       16.20|       13461.71|            13.64|\n",
      "|2013|     134814|              1982.77|   21027|       15.60|       14707.41|            14.53|\n",
      "|2014|     235629|              3503.84|   41569|       17.64|       14870.16|            13.77|\n",
      "|2015|     421093|              6417.58|   77441|       18.39|        15240.3|             12.6|\n",
      "|2016|     434350|              6399.74|   73689|       16.97|       14734.06|            13.04|\n",
      "|2017|     443083|              6575.89|   48621|       10.97|       14841.22|            13.24|\n",
      "|2018|     494127|              7914.17|   17660|        3.57|       16016.47|            12.73|\n",
      "+----+-----------+---------------------+--------+------------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loan trends over time (by issue year)\n",
    "time_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUBSTRING(issue_d, 1, 4) as year,\n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt) / 1000000, 2) as total_funded_millions,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate\n",
    "    FROM loans\n",
    "    WHERE issue_d IS NOT NULL\n",
    "    GROUP BY SUBSTRING(issue_d, 1, 4)\n",
    "    ORDER BY year\n",
    "\"\"\")\n",
    "\n",
    "time_analysis.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//loan_trends_by_year\n"
     ]
    }
   ],
   "source": [
    "time_analysis.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/loan_trends_by_year\")\n",
    "print(f\"Saved: {GOLD_PATH}/loan_trends_by_year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Risk Segmentation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 340:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------+------------+---------------+-----------------+----------+-------+\n",
      "|fico_segment    |total_loans|defaults|default_rate|avg_loan_amount|avg_interest_rate|avg_income|avg_dti|\n",
      "+----------------+-----------+--------+------------+---------------+-----------------+----------+-------+\n",
      "|Excellent (750+)|207174     |10960   |5.29        |16067.47       |8.83             |84230.11  |16.61  |\n",
      "|Good (700-749)  |747729     |73072   |9.77        |16328.71       |11.72            |82026.92  |19.37  |\n",
      "|Fair (650-699)  |1303860    |210976  |16.18       |14145.98       |14.55            |74652.42  |18.87  |\n",
      "|Poor (600-649)  |231        |74      |32.03       |6454.22        |15.32            |52731.55  |13.3   |\n",
      "+----------------+-----------+--------+------------+---------------+-----------------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create risk segments based on FICO score\n",
    "risk_segments = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN fico_avg >= 750 THEN 'Excellent (750+)'\n",
    "            WHEN fico_avg >= 700 THEN 'Good (700-749)'\n",
    "            WHEN fico_avg >= 650 THEN 'Fair (650-699)'\n",
    "            WHEN fico_avg >= 600 THEN 'Poor (600-649)'\n",
    "            ELSE 'Very Poor (<600)'\n",
    "        END as fico_segment,\n",
    "        COUNT(*) as total_loans,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(annual_inc), 2) as avg_income,\n",
    "        ROUND(AVG(dti), 2) as avg_dti\n",
    "    FROM loans\n",
    "    WHERE fico_avg IS NOT NULL\n",
    "    GROUP BY \n",
    "        CASE \n",
    "            WHEN fico_avg >= 750 THEN 'Excellent (750+)'\n",
    "            WHEN fico_avg >= 700 THEN 'Good (700-749)'\n",
    "            WHEN fico_avg >= 650 THEN 'Fair (650-699)'\n",
    "            WHEN fico_avg >= 600 THEN 'Poor (600-649)'\n",
    "            ELSE 'Very Poor (<600)'\n",
    "        END\n",
    "    ORDER BY default_rate\n",
    "\"\"\")\n",
    "\n",
    "risk_segments.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//risk_segments_by_fico\n"
     ]
    }
   ],
   "source": [
    "risk_segments.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/risk_segments_by_fico\")\n",
    "print(f\"Saved: {GOLD_PATH}/risk_segments_by_fico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced SQL: Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+------------+---------+\n",
      "|grade|state|loan_count|default_rate|risk_rank|\n",
      "+-----+-----+----------+------------+---------+\n",
      "|    A|   NM|      2238|        5.05|        1|\n",
      "|    A|   MS|      2081|        4.71|        2|\n",
      "|    A|   OK|      3718|        4.57|        3|\n",
      "|    A|   SD|       780|        4.49|        4|\n",
      "|    A|   LA|      4848|        4.35|        5|\n",
      "|    A|   AR|      2924|        4.34|        6|\n",
      "|    A|   AL|      4483|        4.33|        7|\n",
      "|    A|   NY|     34038|        4.31|        8|\n",
      "|    A|   NJ|     16455|        4.23|        9|\n",
      "|    A|   NV|      5811|        4.15|       10|\n",
      "|    A|   FL|     29527|        4.12|       11|\n",
      "|    A|   NC|     11767|        4.02|       12|\n",
      "|    A|   MT|      1297|        4.01|       13|\n",
      "|    A|   AK|       975|        4.00|       14|\n",
      "|    A|   AZ|     10623|        3.99|       15|\n",
      "|    A|   DE|      1220|        3.85|       16|\n",
      "|    A|   CA|     62412|        3.82|       17|\n",
      "|    A|   MN|      7867|        3.79|       18|\n",
      "|    A|   KY|      4061|        3.77|       19|\n",
      "|    A|   MD|     10037|        3.75|       20|\n",
      "|    A|   PA|     14168|        3.71|       21|\n",
      "|    A|   TX|     36718|        3.71|       21|\n",
      "|    A|   VA|     12496|        3.67|       23|\n",
      "|    A|   MA|     11779|        3.66|       24|\n",
      "|    A|   IN|      6663|        3.62|       25|\n",
      "|    A|   MI|     10673|        3.54|       26|\n",
      "|    A|   RI|      1943|        3.40|       27|\n",
      "|    A|   MO|      6960|        3.36|       28|\n",
      "|    A|   HI|      1675|        3.34|       29|\n",
      "|    A|   WI|      6127|        3.33|       30|\n",
      "+-----+-----+----------+------------+---------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rank states by default rate within each grade\n",
    "spark.sql(\"\"\"\n",
    "    WITH state_grade_stats AS (\n",
    "        SELECT \n",
    "            grade,\n",
    "            addr_state as state,\n",
    "            COUNT(*) as loan_count,\n",
    "            ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate\n",
    "        FROM loans\n",
    "        WHERE addr_state IS NOT NULL\n",
    "        GROUP BY grade, addr_state\n",
    "        HAVING COUNT(*) >= 100  -- Filter for statistical significance\n",
    "    )\n",
    "    SELECT \n",
    "        grade,\n",
    "        state,\n",
    "        loan_count,\n",
    "        default_rate,\n",
    "        RANK() OVER (PARTITION BY grade ORDER BY default_rate DESC) as risk_rank\n",
    "    FROM state_grade_stats\n",
    "    ORDER BY grade, risk_rank\n",
    "\"\"\").show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part B: Machine Learning with MLlib\n",
    "\n",
    "This section builds a loan default prediction model using Spark MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prepare Data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 14\n",
      "Categorical features: 5\n",
      "Target: loan_status_binary\n"
     ]
    }
   ],
   "source": [
    "# Select features for the model\n",
    "# Numeric features\n",
    "numeric_features = [\n",
    "    'loan_amnt',\n",
    "    'int_rate',\n",
    "    'installment',\n",
    "    'annual_inc',\n",
    "    'dti',\n",
    "    'open_acc',\n",
    "    'pub_rec',\n",
    "    'revol_bal',\n",
    "    'revol_util',\n",
    "    'total_acc',\n",
    "    'fico_avg',\n",
    "    'loan_to_income',\n",
    "    'delinq_2yrs',\n",
    "    'inq_last_6mths'\n",
    "]\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = [\n",
    "    'term',\n",
    "    'grade',\n",
    "    'home_ownership',\n",
    "    'verification_status',\n",
    "    'purpose'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'loan_status_binary'\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Dataset: 2,258,994 rows\n",
      "+---------+--------+-----------+----------+-----+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+------------------+------------------+\n",
      "|loan_amnt|int_rate|installment|annual_inc|  dti|open_acc|pub_rec|revol_bal|revol_util|total_acc|fico_avg|loan_to_income|delinq_2yrs|inq_last_6mths|term|grade|home_ownership|verification_status|           purpose|loan_status_binary|\n",
      "+---------+--------+-----------+----------+-----+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+------------------+------------------+\n",
      "|   3600.0|   13.99|     123.03|   55000.0| 5.91|     7.0|    0.0|   2765.0|      29.7|     13.0|   677.0|    0.06545454|        0.0|           1.0|  36|    C|      MORTGAGE|       Not Verified|debt_consolidation|                 0|\n",
      "|  24700.0|   11.99|     820.28|   65000.0|16.06|    22.0|    0.0|  21470.0|      19.2|     38.0|   717.0|          0.38|        1.0|           4.0|  36|    C|      MORTGAGE|       Not Verified|    small_business|                 0|\n",
      "|  20000.0|   10.78|     432.66|   63000.0|10.78|     6.0|    0.0|   7869.0|      56.2|     18.0|   697.0|    0.31746033|        0.0|           0.0|  60|    B|      MORTGAGE|       Not Verified|  home_improvement|                 0|\n",
      "|  35000.0|   14.85|      829.9|  110000.0|17.06|    13.0|    0.0|   7802.0|      11.6|     17.0|   787.0|     0.3181818|        0.0|           0.0|  60|    C|      MORTGAGE|    Source Verified|debt_consolidation|                 0|\n",
      "|  10400.0|   22.45|     289.91|  104433.0|25.37|    12.0|    0.0|  21929.0|      64.5|     35.0|   697.0|    0.09958538|        1.0|           3.0|  60|    F|      MORTGAGE|    Source Verified|    major_purchase|                 0|\n",
      "+---------+--------+-----------+----------+-----+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select columns and filter nulls in target\n",
    "all_features = numeric_features + categorical_features + [target]\n",
    "\n",
    "ml_df = loans_df.select(all_features).filter(F.col(target).isNotNull())\n",
    "\n",
    "print(f\"ML Dataset: {ml_df.count():,} rows\")\n",
    "ml_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|loan_status_binary|  count|\n",
      "+------------------+-------+\n",
      "|                 1| 295082|\n",
      "|                 0|1963912|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution (important for imbalanced data)\n",
    "ml_df.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts per column:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 364:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+----------+---+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+-------+------------------+\n",
      "|loan_amnt|int_rate|installment|annual_inc|dti|open_acc|pub_rec|revol_bal|revol_util|total_acc|fico_avg|loan_to_income|delinq_2yrs|inq_last_6mths|term|grade|home_ownership|verification_status|purpose|loan_status_binary|\n",
      "+---------+--------+-----------+----------+---+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+-------+------------------+\n",
      "|0        |0       |0          |0         |52 |25      |25     |0        |1793      |25       |0       |0             |25         |26            |0   |0    |50            |0                  |0      |0                 |\n",
      "+---------+--------+-----------+----------+---+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for nulls in features\n",
    "print(\"Null counts per column:\")\n",
    "null_counts = ml_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in ml_df.columns])\n",
    "null_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer configured for 14 numeric features\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in numeric columns\n",
    "# Fill with median using Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_features,\n",
    "    outputCols=[f\"{c}_imputed\" for c in numeric_features],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "\n",
    "imputed_numeric_features = [f\"{c}_imputed\" for c in numeric_features]\n",
    "print(f\"Imputer configured for {len(numeric_features)} numeric features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 StringIndexers\n"
     ]
    }
   ],
   "source": [
    "# String Indexers for categorical features\n",
    "# This converts strings to numeric indices\n",
    "\n",
    "indexers = []\n",
    "indexed_cat_features = []\n",
    "\n",
    "for cat_col in categorical_features:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=cat_col, \n",
    "        outputCol=f\"{cat_col}_indexed\",\n",
    "        handleInvalid=\"keep\"  # Handle unseen labels\n",
    "    )\n",
    "    indexers.append(indexer)\n",
    "    indexed_cat_features.append(f\"{cat_col}_indexed\")\n",
    "\n",
    "print(f\"Created {len(indexers)} StringIndexers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 OneHotEncoders\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoders for categorical features\n",
    "encoders = []\n",
    "encoded_cat_features = []\n",
    "\n",
    "for cat_col in categorical_features:\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=f\"{cat_col}_indexed\",\n",
    "        outputCol=f\"{cat_col}_encoded\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoders.append(encoder)\n",
    "    encoded_cat_features.append(f\"{cat_col}_encoded\")\n",
    "\n",
    "print(f\"Created {len(encoders)} OneHotEncoders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorAssembler will combine 19 feature columns\n"
     ]
    }
   ],
   "source": [
    "# Assemble all features into a single vector\n",
    "all_feature_cols = imputed_numeric_features + encoded_cat_features\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_feature_cols,\n",
    "    outputCol=\"features_unscaled\",\n",
    "    handleInvalid=\"skip\"  # Skip rows with nulls\n",
    ")\n",
    "\n",
    "print(f\"VectorAssembler will combine {len(all_feature_cols)} feature columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler configured\n"
     ]
    }
   ],
   "source": [
    "# Scale features (important for Logistic Regression)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=False  # Don't center for sparse data\n",
    ")\n",
    "\n",
    "print(\"StandardScaler configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1,807,149 rows (80.0%)\n",
      "Test set: 451,845 rows (20.0%)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache for performance\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"Training set: {train_count:,} rows ({train_count/(train_count+test_count)*100:.1f}%)\")\n",
    "print(f\"Test set: {test_count:,} rows ({test_count/(train_count+test_count)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution:\n",
      "+------------------+-------+\n",
      "|loan_status_binary|  count|\n",
      "+------------------+-------+\n",
      "|                 1| 235812|\n",
      "|                 0|1571337|\n",
      "+------------------+-------+\n",
      "\n",
      "Test set class distribution:\n",
      "+------------------+------+\n",
      "|loan_status_binary| count|\n",
      "+------------------+------+\n",
      "|                 1| 59270|\n",
      "|                 0|392575|\n",
      "+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify class distribution in splits\n",
    "print(\"Training set class distribution:\")\n",
    "train_df.groupBy(target).count().show()\n",
    "\n",
    "print(\"Test set class distribution:\")\n",
    "test_df.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Pipeline: 14 stages\n"
     ]
    }
   ],
   "source": [
    "# Create Logistic Regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target,\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.8  # L1/L2 mix\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "lr_pipeline = Pipeline(stages=[\n",
    "    imputer,\n",
    "    *indexers,\n",
    "    *encoders,\n",
    "    assembler,\n",
    "    scaler,\n",
    "    lr\n",
    "])\n",
    "\n",
    "print(f\"Logistic Regression Pipeline: {len(lr_pipeline.getStages())} stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "CPU times: user 100 ms, sys: 36.9 ms, total: 137 ms\n",
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr_model = lr_pipeline.fit(train_df)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+--------+------------------+----------+-----------------------------------------+\n",
      "|loan_amnt|int_rate|grade|fico_avg|loan_status_binary|prediction|probability                              |\n",
      "+---------+--------+-----+--------+------------------+----------+-----------------------------------------+\n",
      "|600.0    |13.24   |D    |752.0   |0                 |0.0       |[0.8832807611679021,0.1167192388320979]  |\n",
      "|900.0    |12.92   |D    |717.0   |0                 |0.0       |[0.8911017040293526,0.10889829597064737] |\n",
      "|1000.0   |5.31    |A    |827.0   |0                 |0.0       |[0.9687015311729185,0.03129846882708154] |\n",
      "|1000.0   |5.32    |A    |772.0   |0                 |0.0       |[0.9642980348211304,0.03570196517886959] |\n",
      "|1000.0   |5.32    |A    |772.0   |1                 |0.0       |[0.9625904754212486,0.03740952457875135] |\n",
      "|1000.0   |5.32    |A    |687.0   |0                 |0.0       |[0.9483839573031319,0.05161604269686815] |\n",
      "|1000.0   |6.03    |A    |752.0   |0                 |0.0       |[0.9600433607088017,0.039956639291198304]|\n",
      "|1000.0   |6.19    |A    |782.0   |0                 |0.0       |[0.9595852599822806,0.040414740017719386]|\n",
      "|1000.0   |6.49    |A    |772.0   |0                 |0.0       |[0.9563231461541659,0.043676853845834085]|\n",
      "|1000.0   |6.49    |A    |702.0   |0                 |0.0       |[0.946774730017013,0.053225269982987045] |\n",
      "+---------+--------+-----+--------+------------------+----------+-----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Show sample predictions\n",
    "lr_predictions.select(\n",
    "    'loan_amnt', 'int_rate', 'grade', 'fico_avg',\n",
    "    target, 'prediction', 'probability'\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 462:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOGISTIC REGRESSION RESULTS\n",
      "==================================================\n",
      "AUC-ROC: 0.6893\n",
      "AUC-PR:  0.2359\n",
      "Accuracy: 0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=target,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=target,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "lr_auc = evaluator_auc.evaluate(lr_predictions)\n",
    "lr_pr = evaluator_pr.evaluate(lr_predictions)\n",
    "lr_accuracy = evaluator_accuracy.evaluate(lr_predictions)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LOGISTIC REGRESSION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"AUC-ROC: {lr_auc:.4f}\")\n",
    "print(f\"AUC-PR:  {lr_pr:.4f}\")\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 464:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+------+\n",
      "|loan_status_binary|prediction| count|\n",
      "+------------------+----------+------+\n",
      "|                 0|       0.0|392575|\n",
      "|                 1|       0.0| 59269|\n",
      "|                 1|       1.0|     1|\n",
      "+------------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "lr_predictions.groupBy(target, 'prediction').count().orderBy(target, 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Pipeline: 13 stages\n"
     ]
    }
   ],
   "source": [
    "# Create Random Forest model (doesn't need scaling)\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features_unscaled\",  # Use unscaled features\n",
    "    labelCol=target,\n",
    "    numTrees=20,\n",
    "    maxDepth=5,\n",
    "    minInstancesPerNode=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Build pipeline (without scaler)\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    imputer,\n",
    "    *indexers,\n",
    "    *encoders,\n",
    "    assembler,\n",
    "    rf\n",
    "])\n",
    "\n",
    "print(f\"Random Forest Pipeline: {len(rf_pipeline.getStages())} stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "CPU times: user 92.5 ms, sys: 35.6 ms, total: 128 ms\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model = rf_pipeline.fit(train_df)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 527:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RANDOM FOREST RESULTS\n",
      "==================================================\n",
      "AUC-ROC: 0.5633\n",
      "AUC-PR:  0.2061\n",
      "Accuracy: 0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_auc = evaluator_auc.evaluate(rf_predictions)\n",
    "rf_pr = evaluator_pr.evaluate(rf_predictions)\n",
    "rf_accuracy = evaluator_accuracy.evaluate(rf_predictions)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RANDOM FOREST RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"AUC-ROC: {rf_auc:.4f}\")\n",
    "print(f\"AUC-PR:  {rf_pr:.4f}\")\n",
    "print(f\"Accuracy: {rf_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 529:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+------+\n",
      "|loan_status_binary|prediction| count|\n",
      "+------------------+----------+------+\n",
      "|                 0|       0.0|392575|\n",
      "|                 1|       0.0| 59270|\n",
      "+------------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "rf_predictions.groupBy(target, 'prediction').count().orderBy(target, 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Feature Importances (by index):\n",
      "Index    Importance  \n",
      "--------------------\n",
      "23       0.4730\n",
      "33       0.2802\n",
      "22       0.2159\n",
      "11       0.0104\n",
      "7        0.0100\n",
      "3        0.0040\n",
      "26       0.0024\n",
      "35       0.0022\n",
      "10       0.0015\n",
      "12       0.0005\n",
      "0        0.0000\n",
      "1        0.0000\n",
      "2        0.0000\n",
      "4        0.0000\n",
      "5        0.0000\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance (Random Forest)\n",
    "rf_model_final = rf_model.stages[-1]\n",
    "feature_importance = rf_model_final.featureImportances\n",
    "\n",
    "# Get feature names\n",
    "# Note: This is an approximation since OneHot expands features\n",
    "print(\"\\nTop Feature Importances (by index):\")\n",
    "importance_list = [(i, float(imp)) for i, imp in enumerate(feature_importance)]\n",
    "sorted_importance = sorted(importance_list, key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "print(f\"{'Index':<8} {'Importance':<12}\")\n",
    "print(\"-\" * 20)\n",
    "for idx, imp in sorted_importance:\n",
    "    print(f\"{idx:<8} {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON\n",
      "============================================================\n",
      "Metric          Logistic Regression  Random Forest       \n",
      "------------------------------------------------------------\n",
      "AUC-ROC         0.6893               0.5633              \n",
      "AUC-PR          0.2359               0.2061              \n",
      "Accuracy        0.8688               0.8688              \n",
      "============================================================\n",
      "\n",
      "Best Model: Logistic Regression (AUC: 0.6893)\n"
     ]
    }
   ],
   "source": [
    "# Compare models\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<15} {'Logistic Regression':<20} {'Random Forest':<20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'AUC-ROC':<15} {lr_auc:<20.4f} {rf_auc:<20.4f}\")\n",
    "print(f\"{'AUC-PR':<15} {lr_pr:<20.4f} {rf_pr:<20.4f}\")\n",
    "print(f\"{'Accuracy':<15} {lr_accuracy:<20.4f} {rf_accuracy:<20.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = \"Random Forest\" if rf_auc > lr_auc else \"Logistic Regression\"\n",
    "best_model = rf_model if rf_auc > lr_auc else lr_model\n",
    "best_auc = max(rf_auc, lr_auc)\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} (AUC: {best_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Hyperparameter Tuning with Cross-Validation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid size: 4 combinations\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "# Note: This can be time-consuming, so we use a small grid\n",
    "\n",
    "# Create a new RF for tuning\n",
    "rf_tune = RandomForestClassifier(\n",
    "    featuresCol=\"features_unscaled\",\n",
    "    labelCol=target,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Pipeline for tuning\n",
    "rf_tune_pipeline = Pipeline(stages=[\n",
    "    imputer,\n",
    "    *indexers,\n",
    "    *encoders,\n",
    "    assembler,\n",
    "    rf_tune\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_tune.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf_tune.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Parameter grid size: {len(paramGrid)} combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CV on sample of 362,229 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation complete!\n",
      "CPU times: user 2.1 s, sys: 1.32 s, total: 3.42 s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=rf_tune_pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator_auc,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Sample data for faster tuning (optional - remove for full tuning)\n",
    "train_sample = train_df.sample(0.2, seed=42)\n",
    "print(f\"Training CV on sample of {train_sample.count():,} rows...\")\n",
    "\n",
    "# Fit cross-validator\n",
    "cv_model = cv.fit(train_sample)\n",
    "print(\"Cross-validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation AUC scores:\n",
      "  Config 1: 0.6915\n",
      "  Config 2: 0.7031\n",
      "  Config 3: 0.6915\n",
      "  Config 4: 0.7036\n",
      "\n",
      "Best CV AUC: 0.7036\n"
     ]
    }
   ],
   "source": [
    "# Best model from CV\n",
    "print(\"Cross-validation AUC scores:\")\n",
    "for i, score in enumerate(cv_model.avgMetrics):\n",
    "    print(f\"  Config {i+1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest CV AUC: {max(cv_model.avgMetrics):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV Model Test AUC: 0.7026\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best CV model on test set\n",
    "cv_predictions = cv_model.transform(test_df)\n",
    "cv_auc = evaluator_auc.evaluate(cv_predictions)\n",
    "print(f\"Best CV Model Test AUC: {cv_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Save Models and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ../data/medallion/gold//models/default_prediction_model\n"
     ]
    }
   ],
   "source": [
    "# Save best model\n",
    "MODEL_PATH = f\"{GOLD_PATH}/models/default_prediction_model\"\n",
    "best_model.write().overwrite().save(MODEL_PATH)\n",
    "print(f\"Model saved to: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1287:==============>                                         (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: ../data/medallion/gold//predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "# Use the predictions from the best model\n",
    "best_predictions = rf_predictions if rf_auc > lr_auc else lr_predictions\n",
    "\n",
    "predictions_to_save = best_predictions.select(\n",
    "    'loan_amnt', 'int_rate', 'term', 'grade',\n",
    "    'annual_inc', 'dti', 'fico_avg', 'purpose', 'home_ownership',\n",
    "    target, 'prediction', 'probability'\n",
    ")\n",
    "\n",
    "PREDICTIONS_PATH = f\"{GOLD_PATH}/predictions\"\n",
    "predictions_to_save.write.mode(\"overwrite\").parquet(PREDICTIONS_PATH)\n",
    "print(f\"Predictions saved to: {PREDICTIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Create Risk Scoring Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1288:==============>                                         (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+-------------------+\n",
      "| risk_category| count|avg_prob|actual_default_rate|\n",
      "+--------------+------+--------+-------------------+\n",
      "|      Low Risk|155016|  0.0685|             0.0535|\n",
      "|   Medium Risk|271977|  0.1502|              0.159|\n",
      "|     High Risk| 24851|  0.3008|             0.3108|\n",
      "|Very High Risk|     1|  0.7076|                1.0|\n",
      "+--------------+------+--------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a risk scoring summary for business use\n",
    "# Extract probability of default from the probability vector\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# UDF to extract probability of default (class 1)\n",
    "@udf(FloatType())\n",
    "def extract_prob_default(probability):\n",
    "    return float(probability[1])\n",
    "\n",
    "risk_scores = best_predictions.withColumn(\n",
    "    \"default_probability\", \n",
    "    extract_prob_default(F.col(\"probability\"))\n",
    ")\n",
    "\n",
    "# Create risk categories\n",
    "risk_scores = risk_scores.withColumn(\n",
    "    \"risk_category\",\n",
    "    F.when(F.col(\"default_probability\") < 0.1, \"Low Risk\")\n",
    "    .when(F.col(\"default_probability\") < 0.25, \"Medium Risk\")\n",
    "    .when(F.col(\"default_probability\") < 0.5, \"High Risk\")\n",
    "    .otherwise(\"Very High Risk\")\n",
    ")\n",
    "\n",
    "# Show distribution\n",
    "risk_scores.groupBy(\"risk_category\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.round(F.avg(\"default_probability\"), 4).alias(\"avg_prob\"),\n",
    "    F.round(F.avg(target), 4).alias(\"actual_default_rate\")\n",
    ").orderBy(\"avg_prob\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1291:==========================================>             (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk scores saved to: ../data/medallion/gold//risk_scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save risk scores\n",
    "risk_score_table = risk_scores.select(\n",
    "    'loan_amnt', 'int_rate', 'grade', 'fico_avg', 'annual_inc',\n",
    "    'purpose', target, 'default_probability', 'risk_category'\n",
    ")\n",
    "\n",
    "RISK_SCORES_PATH = f\"{GOLD_PATH}/risk_scores\"\n",
    "risk_score_table.write.mode(\"overwrite\").parquet(RISK_SCORES_PATH)\n",
    "print(f\"Risk scores saved to: {RISK_SCORES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Gold Layer Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "drwxrwxr-x 11 ubuntu ubuntu 4096 Nov 25 20:13 .\n",
      "drwxrwxr-x  5 ubuntu ubuntu 4096 Nov 25 19:59 ..\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 25 20:03 default_rate_by_grade\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 25 20:03 default_rate_by_subgrade\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 25 20:03 loan_analysis_by_purpose\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 25 20:03 loan_analysis_by_state\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 25 20:04 loan_trends_by_year\n",
      "drwxr-xr-x  3 ubuntu ubuntu 4096 Nov 25 20:09 models\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 25 20:11 predictions\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 25 20:13 risk_scores\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 25 20:04 risk_segments_by_fico\n"
     ]
    }
   ],
   "source": [
    "# List all Gold layer outputs\n",
    "!ls -la {GOLD_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GOLD LAYER SUMMARY\n",
      "======================================================================\n",
      "\n",
      "--- Part A: SQL Analytics ---\n",
      "Tables created for business intelligence:\n",
      "  1. default_rate_by_grade     - Default rates by loan grade\n",
      "  2. default_rate_by_subgrade  - Default rates by sub-grade\n",
      "  3. loan_analysis_by_state    - Geographic analysis\n",
      "  4. loan_analysis_by_purpose  - Analysis by loan purpose\n",
      "  5. loan_trends_by_year       - Time series analysis\n",
      "  6. risk_segments_by_fico     - FICO-based risk segments\n",
      "\n",
      "--- Part B: Machine Learning ---\n",
      "Models trained:\n",
      "  1. Logistic Regression - AUC: 0.6893\n",
      "  2. Random Forest       - AUC: 0.5633\n",
      "\n",
      "Best model: Logistic Regression\n",
      "\n",
      "ML outputs:\n",
      "  - models/default_prediction_model - Trained ML pipeline\n",
      "  - predictions                     - Test set predictions\n",
      "  - risk_scores                     - Risk scoring table\n",
      "\n",
      "--- Technologies Used ---\n",
      "  - Spark SQL: Complex queries, aggregations, window functions\n",
      "  - MLlib: Pipeline, VectorAssembler, StringIndexer, OneHotEncoder\n",
      "  - MLlib: LogisticRegression, RandomForestClassifier\n",
      "  - MLlib: CrossValidator, BinaryClassificationEvaluator\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"GOLD LAYER SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n--- Part A: SQL Analytics ---\")\n",
    "print(\"Tables created for business intelligence:\")\n",
    "print(f\"  1. default_rate_by_grade     - Default rates by loan grade\")\n",
    "print(f\"  2. default_rate_by_subgrade  - Default rates by sub-grade\")\n",
    "print(f\"  3. loan_analysis_by_state    - Geographic analysis\")\n",
    "print(f\"  4. loan_analysis_by_purpose  - Analysis by loan purpose\")\n",
    "print(f\"  5. loan_trends_by_year       - Time series analysis\")\n",
    "print(f\"  6. risk_segments_by_fico     - FICO-based risk segments\")\n",
    "\n",
    "print(\"\\n--- Part B: Machine Learning ---\")\n",
    "print(\"Models trained:\")\n",
    "print(f\"  1. Logistic Regression - AUC: {lr_auc:.4f}\")\n",
    "print(f\"  2. Random Forest       - AUC: {rf_auc:.4f}\")\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "print(\"\\nML outputs:\")\n",
    "print(f\"  - models/default_prediction_model - Trained ML pipeline\")\n",
    "print(f\"  - predictions                     - Test set predictions\")\n",
    "print(f\"  - risk_scores                     - Risk scoring table\")\n",
    "\n",
    "print(\"\\n--- Technologies Used ---\")\n",
    "print(\"  - Spark SQL: Complex queries, aggregations, window functions\")\n",
    "print(\"  - MLlib: Pipeline, VectorAssembler, StringIndexer, OneHotEncoder\")\n",
    "print(\"  - MLlib: LogisticRegression, RandomForestClassifier\")\n",
    "print(\"  - MLlib: CrossValidator, BinaryClassificationEvaluator\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached DataFrames unpersisted.\n",
      "\n",
      "Gold layer complete!\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "\n",
    "print(\"Cached DataFrames unpersisted.\")\n",
    "print(\"\\nGold layer complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "The Gold layer provides:\n",
    "\n",
    "1. **Business Analytics** - Ready-to-use aggregated tables for dashboards\n",
    "2. **ML Model** - Trained loan default prediction model\n",
    "3. **Risk Scoring** - Probability-based risk categorization\n",
    "\n",
    "These outputs can be:\n",
    "- Connected to BI tools (Tableau, Power BI)\n",
    "- Served via REST API for applications\n",
    "- Used for real-time scoring of new loan applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
