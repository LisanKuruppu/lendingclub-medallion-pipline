{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer - Data Serving with SQL & MLlib\n",
    "\n",
    "## Lending Club Loan Data Pipeline\n",
    "\n",
    "**Use Case:** Predict loan default risk and analyze factors affecting loan approval\n",
    "\n",
    "This notebook implements the Gold (Serving) layer of the Medallion Architecture:\n",
    "- Business analytics using **Spark SQL**\n",
    "- Machine Learning using **MLlib** (Loan Default Prediction)\n",
    "- Create aggregated tables ready for dashboards and applications\n",
    "\n",
    "**Note:** Unlike the Silver layer, this notebook uses high-level APIs (DataFrames, SQL, MLlib) as permitted by the project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ML imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, \n",
    "    StringIndexer, \n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    Imputer\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, \n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import (\n",
    "    BinaryClassificationEvaluator,\n",
    "    MulticlassClassificationEvaluator\n",
    ")\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/27 12:44:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Spark UI available at: http://spark-master:4040\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"LendingClub-Gold-Layer\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI available at: {sc.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver input path: ../data/medallion/silver/\n",
      "Gold output path: ../data/medallion/gold/\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "SILVER_PATH = \"../data/medallion/silver/\"\n",
    "GOLD_PATH = \"../data/medallion/gold/\"\n",
    "\n",
    "SILVER_ACCEPTED_PATH = os.path.join(SILVER_PATH, \"accepted_loans\")\n",
    "SILVER_REJECTED_PATH = os.path.join(SILVER_PATH, \"rejected_loans\")\n",
    "\n",
    "# Create gold directory\n",
    "os.makedirs(GOLD_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Silver input path: {SILVER_PATH}\")\n",
    "print(f\"Gold output path: {GOLD_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Silver Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted loans: 2,231,965 rows\n",
      "Rejected loans: 20,116,218 rows\n",
      "\n",
      "Accepted loans columns: 30\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data from Silver layer\n",
    "loans_df = spark.read.parquet(SILVER_ACCEPTED_PATH)\n",
    "rejected_df = spark.read.parquet(SILVER_REJECTED_PATH)\n",
    "\n",
    "print(f\"Accepted loans: {loans_df.count():,} rows\")\n",
    "print(f\"Rejected loans: {rejected_df.count():,} rows\")\n",
    "print(f\"\\nAccepted loans columns: {len(loans_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_amnt: float (nullable = true)\n",
      " |-- term: integer (nullable = true)\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- installment: float (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_length: integer (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: float (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_status_binary: integer (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- open_acc: float (nullable = true)\n",
      " |-- pub_rec: float (nullable = true)\n",
      " |-- revol_bal: float (nullable = true)\n",
      " |-- revol_util: float (nullable = true)\n",
      " |-- total_acc: float (nullable = true)\n",
      " |-- fico_range_low: float (nullable = true)\n",
      " |-- fico_range_high: float (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- delinq_2yrs: float (nullable = true)\n",
      " |-- inq_last_6mths: float (nullable = true)\n",
      " |-- mort_acc: float (nullable = true)\n",
      " |-- pub_rec_bankruptcies: float (nullable = true)\n",
      " |-- fico_avg: float (nullable = true)\n",
      " |-- loan_to_income: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show schema\n",
    "loans_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----------+-----------+------------------+--------+\n",
      "|loan_amnt|int_rate|grade|annual_inc|loan_status|loan_status_binary|fico_avg|\n",
      "+---------+--------+-----+----------+-----------+------------------+--------+\n",
      "|  10000.0|   12.12|    B|   50000.0|Charged Off|                 1|   687.0|\n",
      "|  30750.0|   12.12|    B|   77000.0| Fully Paid|                 0|    NULL|\n",
      "|   2650.0|   12.12|    B|   20000.0| Fully Paid|                 0|    NULL|\n",
      "|   1200.0|   12.12|    B|   13000.0| Fully Paid|                 0|   742.0|\n",
      "|   2500.0|   14.09|    B|   40000.0| Fully Paid|                 0|   682.0|\n",
      "|   4900.0|     7.9|    A|   28000.0| Fully Paid|                 0|   727.0|\n",
      "|  15000.0|   12.12|    B|   95000.0| Fully Paid|                 0|    NULL|\n",
      "|  10000.0|   12.12|    B|   30000.0| Fully Paid|                 0|   697.0|\n",
      "|   5000.0|   14.33|    C|   21000.0| Fully Paid|                 0|   672.0|\n",
      "|  18000.0|   18.49|    D|  110000.0|Charged Off|                 1|   702.0|\n",
      "+---------+--------+-----+----------+-----------+------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview data\n",
    "loans_df.select(\n",
    "    'loan_amnt', 'int_rate', 'grade', 'annual_inc', \n",
    "    'loan_status', 'loan_status_binary', 'fico_avg'\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp views created: 'loans', 'rejected'\n"
     ]
    }
   ],
   "source": [
    "# Register as SQL temp view for queries\n",
    "loans_df.createOrReplaceTempView(\"loans\")\n",
    "rejected_df.createOrReplaceTempView(\"rejected\")\n",
    "\n",
    "print(\"Temp views created: 'loans', 'rejected'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part A: Business Analytics with Spark SQL\n",
    "\n",
    "This section demonstrates SQL capabilities for business intelligence and reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                            (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------------+-----------------+-----------------+--------------+\n",
      "|total_loans|   total_funded|avg_loan_amount|avg_interest_rate|avg_annual_income|avg_fico_score|\n",
      "+-----------+---------------+---------------+-----------------+-----------------+--------------+\n",
      "|    2231965|3.3501844975E10|       15010.02|             13.1|         77601.46|         700.0|\n",
      "+-----------+---------------+---------------+-----------------+-----------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(annual_inc), 2) as avg_annual_income,\n",
    "        ROUND(AVG(fico_avg), 0) as avg_fico_score\n",
    "    FROM loans\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:========================================================(16 + 0) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+------------------+-------+----------+\n",
      "|loan_status                                        |loan_status_binary|count  |percentage|\n",
      "+---------------------------------------------------+------------------+-------+----------+\n",
      "|Fully Paid                                         |0                 |1059591|47.47     |\n",
      "|Current                                            |0                 |869764 |38.97     |\n",
      "|Charged Off                                        |1                 |265946 |11.92     |\n",
      "|Late (31-120 days)                                 |1                 |21326  |0.96      |\n",
      "|In Grace Period                                    |0                 |8369   |0.37      |\n",
      "|Late (16-30 days)                                  |1                 |4325   |0.19      |\n",
      "|Does not meet the credit policy. Status:Fully Paid |0                 |1873   |0.08      |\n",
      "|Does not meet the credit policy. Status:Charged Off|1                 |732    |0.03      |\n",
      "|Default                                            |1                 |39     |0.00      |\n",
      "+---------------------------------------------------+------------------+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loan status distribution\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        loan_status,\n",
    "        loan_status_binary,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM loans\n",
    "    GROUP BY loan_status, loan_status_binary\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Default Rate Analysis by Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+------------+-----------------+---------------+-------------+\n",
      "|grade|total_loans|defaults|default_rate|avg_interest_rate|avg_loan_amount| total_funded|\n",
      "+-----+-----------+--------+------------+-----------------+---------------+-------------+\n",
      "|    A|     426139|   15691|        3.68|             7.08|       14559.68| 6.20444575E9|\n",
      "|    B|     654756|   57843|        8.83|            10.67|       14135.15|9.255076375E9|\n",
      "|    C|     642765|   93971|       14.62|            14.14|       15003.03|9.643423525E9|\n",
      "|    D|     320918|   66389|       20.69|            18.14|       15679.36|  5.0317904E9|\n",
      "|    E|     134102|   38417|       28.65|            21.83|       17412.39| 2.33503685E9|\n",
      "|    F|      41272|   15200|       36.83|            25.47|       19081.31|   7.875238E8|\n",
      "|    G|      12013|    4857|       40.43|            28.11|       20356.97| 2.44548275E8|\n",
      "+-----+-----------+--------+------------+-----------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default rate by grade - KEY BUSINESS METRIC\n",
    "default_by_grade = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        grade,\n",
    "        COUNT(*) as total_loans,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded\n",
    "    FROM loans\n",
    "    GROUP BY grade\n",
    "    ORDER BY grade\n",
    "\"\"\")\n",
    "\n",
    "default_by_grade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//default_rate_by_grade\n"
     ]
    }
   ],
   "source": [
    "# Save as Gold table\n",
    "default_by_grade.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/default_rate_by_grade\")\n",
    "print(f\"Saved: {GOLD_PATH}/default_rate_by_grade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Default Rate Analysis by Sub-Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+--------+------------+-----------------+--------+\n",
      "|grade|sub_grade|total_loans|defaults|default_rate|avg_interest_rate|avg_fico|\n",
      "+-----+---------+-----------+--------+------------+-----------------+--------+\n",
      "|    A|       A1|      85215|    1580|        1.85|              5.6|   747.0|\n",
      "|    A|       A2|      68445|    1950|        2.85|             6.55|   741.0|\n",
      "|    A|       A3|      72075|    2359|        3.27|             7.09|   730.0|\n",
      "|    A|       A4|      94388|    3976|        4.21|             7.56|   725.0|\n",
      "|    A|       A5|     106016|    5826|        5.50|             8.19|   716.0|\n",
      "|    B|       B1|     123615|    8207|        6.64|             9.08|   708.0|\n",
      "|    B|       B2|     124895|    9251|        7.41|             9.97|   705.0|\n",
      "|    B|       B3|     129649|   11552|        8.91|             10.7|   701.0|\n",
      "|    B|       B4|     137960|   13549|        9.82|            11.37|   699.0|\n",
      "|    B|       B5|     138637|   15284|       11.02|            12.01|   696.0|\n",
      "|    C|       C1|     144222|   17746|       12.30|            12.78|   693.0|\n",
      "|    C|       C2|     129576|   17906|       13.82|            13.53|   692.0|\n",
      "|    C|       C3|     127780|   18501|       14.48|             14.1|   691.0|\n",
      "|    C|       C4|     125699|   20396|       16.23|            14.88|   690.0|\n",
      "|    C|       C5|     115488|   19422|       16.82|            15.77|   689.0|\n",
      "|    D|       D1|      80926|   15472|       19.12|            16.66|   687.0|\n",
      "|    D|       D2|      72168|   14370|       19.91|             17.6|   686.0|\n",
      "|    D|       D3|      64115|   13161|       20.53|            18.39|   686.0|\n",
      "|    D|       D4|      56220|   12465|       22.17|            19.08|   685.0|\n",
      "|    D|       D5|      47489|   10921|       23.00|            20.07|   685.0|\n",
      "|    E|       E1|      33218|    9045|       27.23|            20.34|   685.0|\n",
      "|    E|       E2|      29580|    8463|       28.61|            21.02|   684.0|\n",
      "|    E|       E3|      26382|    7646|       28.98|            21.89|   684.0|\n",
      "|    E|       E4|      22493|    6721|       29.88|            22.78|   684.0|\n",
      "|    E|       E5|      22429|    6542|       29.17|            24.12|   684.0|\n",
      "+-----+---------+-----------+--------+------------+-----------------+--------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Default rate by sub-grade - more granular view\n",
    "default_by_subgrade = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        grade,\n",
    "        sub_grade,\n",
    "        COUNT(*) as total_loans,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(fico_avg), 0) as avg_fico\n",
    "    FROM loans\n",
    "    GROUP BY grade, sub_grade\n",
    "    ORDER BY grade, sub_grade\n",
    "\"\"\")\n",
    "\n",
    "default_by_subgrade.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//default_rate_by_subgrade\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "default_by_subgrade.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/default_rate_by_subgrade\")\n",
    "print(f\"Saved: {GOLD_PATH}/default_rate_by_subgrade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------+--------+------------+----------+--------+\n",
      "|state|total_loans| total_funded|defaults|default_rate|avg_income|avg_fico|\n",
      "+-----+-----------+-------------+--------+------------+----------+--------+\n",
      "|   CA|     303285| 4.63607205E9|   40821|       13.46|  83594.96|   700.0|\n",
      "|   TX|     181279| 2.84866395E9|   23514|       12.97|  82596.21|   701.0|\n",
      "|   NY|     180893| 2.68261715E9|   25989|       14.37|  80752.71|   701.0|\n",
      "|   FL|     157442|2.266215575E9|   22074|       14.02|  73001.78|   699.0|\n",
      "|   IL|      88439|1.366682175E9|    9935|       11.23|  79607.03|   701.0|\n",
      "|   NJ|      80543|   1.273376E9|   10843|       13.46|  88319.72|   701.0|\n",
      "|   PA|      74607|  1.0963789E9|   10073|       13.50|  73704.66|   701.0|\n",
      "|   OH|      73026| 1.04453825E9|    9411|       12.89|  69120.56|   700.0|\n",
      "|   GA|      71927|1.100590825E9|    8559|       11.90|  77571.05|   699.0|\n",
      "|   NC|      61021|   9.036662E8|    8374|       13.72|  73637.53|   701.0|\n",
      "|   VA|      60649|   9.755454E8|    8066|       13.30|  84888.74|   701.0|\n",
      "|   MI|      57331|  8.2034585E8|    7602|       13.26|  71552.71|   699.0|\n",
      "|   MD|      52329| 8.29924675E8|    7195|       13.75|   86670.2|   700.0|\n",
      "|   AZ|      52272|  7.5821535E8|    6798|       13.01|  74140.64|   699.0|\n",
      "|   MA|      49866| 7.80737375E8|    6290|       12.61|  82496.77|   702.0|\n",
      "+-----+-----------+-------------+--------+------------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Default rate by state - TOP 10 states by loan volume\n",
    "state_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        addr_state as state,\n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(annual_inc), 2) as avg_income,\n",
    "        ROUND(AVG(fico_avg), 0) as avg_fico\n",
    "    FROM loans\n",
    "    WHERE addr_state IS NOT NULL\n",
    "    GROUP BY addr_state\n",
    "    ORDER BY total_loans DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "state_analysis.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//loan_analysis_by_state\n"
     ]
    }
   ],
   "source": [
    "# Full state analysis for Gold layer\n",
    "full_state_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        addr_state as state,\n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(annual_inc), 2) as avg_income,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(fico_avg), 0) as avg_fico\n",
    "    FROM loans\n",
    "    WHERE addr_state IS NOT NULL\n",
    "    GROUP BY addr_state\n",
    "    ORDER BY total_loans DESC\n",
    "\"\"\")\n",
    "\n",
    "full_state_analysis.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/loan_analysis_by_state\")\n",
    "print(f\"Saved: {GOLD_PATH}/loan_analysis_by_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loan Purpose Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------+--------+------------+---------------+-----------------+\n",
      "|purpose           |total_loans|total_funded  |defaults|default_rate|avg_loan_amount|avg_interest_rate|\n",
      "+------------------+-----------+--------------+--------+------------+---------------+-----------------+\n",
      "|debt_consolidation|1241832    |1.980251725E10|175366  |14.12       |15946.21       |13.53            |\n",
      "|credit_card       |503099     |7.69429535E9  |53532   |10.64       |15293.8        |11.69            |\n",
      "|home_improvement  |145791     |2.13439705E9  |16835   |11.55       |14640.12       |12.64            |\n",
      "|other             |135726     |1.421758975E9 |17863   |13.16       |10475.21       |14.25            |\n",
      "|major_purchase    |48778      |6.21745575E8  |6008    |12.32       |12746.43       |12.8             |\n",
      "|medical           |26841      |2.538256E8    |3666    |13.66       |9456.64        |13.65            |\n",
      "|car               |23000      |2.17284725E8  |2286    |9.94        |9447.16        |12.24            |\n",
      "|small_business    |22890      |3.7740885E8   |4596    |20.08       |16487.94       |15.36            |\n",
      "|vacation          |15228      |9.6751225E7   |1900    |12.48       |6353.51        |13.46            |\n",
      "|moving            |14782      |1.23661175E8  |2362    |15.98       |8365.66        |14.8             |\n",
      "|house             |13640      |2.142279E8    |1755    |12.87       |15705.86       |14.44            |\n",
      "|wedding           |1664       |1.7302E7      |210     |12.62       |10397.84       |14.72            |\n",
      "|renewable_energy  |1372       |1.4767375E7   |228     |16.62       |10763.39       |14.85            |\n",
      "|educational       |195        |1199625.0     |34      |17.44       |6151.92        |12.21            |\n",
      "|<br>\"             |171        |2603975.0     |30      |17.54       |15227.92       |13.99            |\n",
      "+------------------+-----------+--------------+--------+------------+---------------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default rate by loan purpose\n",
    "purpose_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        purpose,\n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt), 2) as total_funded,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate\n",
    "    FROM loans\n",
    "    WHERE purpose IS NOT NULL\n",
    "    GROUP BY purpose\n",
    "    ORDER BY total_loans DESC\n",
    "\"\"\")\n",
    "\n",
    "purpose_analysis.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//loan_analysis_by_purpose\n"
     ]
    }
   ],
   "source": [
    "purpose_analysis.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/loan_analysis_by_purpose\")\n",
    "print(f\"Saved: {GOLD_PATH}/loan_analysis_by_purpose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------------------+--------+------------+---------------+-----------------+\n",
      "|year|total_loans|total_funded_millions|defaults|default_rate|avg_loan_amount|avg_interest_rate|\n",
      "+----+-----------+---------------------+--------+------------+---------------+-----------------+\n",
      "|2007|        556|                  4.5|     148|       26.62|        8087.37|            11.76|\n",
      "|2008|       2268|                19.91|     470|       20.72|        8780.04|            12.05|\n",
      "|2009|       4977|                48.63|     693|       13.92|        9769.96|            12.44|\n",
      "|2010|      11964|               125.53|    1696|       14.18|       10492.67|            12.01|\n",
      "|2011|      20540|               246.84|    3163|       15.40|       12017.58|            12.25|\n",
      "|2012|      50678|               679.77|    8315|       16.41|       13413.47|            13.65|\n",
      "|2013|     130064|              1906.55|   20412|       15.69|       14658.54|            14.54|\n",
      "|2014|     233196|              3457.11|   41268|       17.70|       14824.92|            13.78|\n",
      "|2015|     417032|              6338.27|   76959|       18.45|       15198.53|            12.61|\n",
      "|2016|     430683|              6329.29|   73308|       17.02|       14695.94|            13.05|\n",
      "|2017|     439604|               6507.2|   48370|       11.00|        14802.4|            13.25|\n",
      "|2018|     490403|              7838.25|   17566|        3.58|       15983.28|            12.73|\n",
      "+----+-----------+---------------------+--------+------------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loan trends over time (by issue year)\n",
    "time_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUBSTRING(issue_d, 1, 4) as year,\n",
    "        COUNT(*) as total_loans,\n",
    "        ROUND(SUM(loan_amnt) / 1000000, 2) as total_funded_millions,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate\n",
    "    FROM loans\n",
    "    WHERE issue_d IS NOT NULL\n",
    "    GROUP BY SUBSTRING(issue_d, 1, 4)\n",
    "    ORDER BY year\n",
    "\"\"\")\n",
    "\n",
    "time_analysis.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//loan_trends_by_year\n"
     ]
    }
   ],
   "source": [
    "time_analysis.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/loan_trends_by_year\")\n",
    "print(f\"Saved: {GOLD_PATH}/loan_trends_by_year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Risk Segmentation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------+------------+---------------+-----------------+----------+-------+\n",
      "|fico_segment    |total_loans|defaults|default_rate|avg_loan_amount|avg_interest_rate|avg_income|avg_dti|\n",
      "+----------------+-----------+--------+------------+---------------+-----------------+----------+-------+\n",
      "|Excellent (750+)|199513     |10490   |5.26        |16083.43       |8.85             |83875.33  |16.78  |\n",
      "|Good (700-749)  |723495     |70562   |9.75        |16329.0        |11.73            |81770.08  |19.47  |\n",
      "|Fair (650-699)  |1271154    |205475  |16.16       |14129.09       |14.54            |74425.5   |18.92  |\n",
      "|Very Poor (<600)|493        |88      |17.85       |14048.17       |14.94            |76527.56  |NULL   |\n",
      "|Poor (600-649)  |108        |34      |31.48       |6280.09        |15.41            |51071.57  |14.07  |\n",
      "+----------------+-----------+--------+------------+---------------+-----------------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create risk segments based on FICO score\n",
    "risk_segments = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN fico_avg >= 750 THEN 'Excellent (750+)'\n",
    "            WHEN fico_avg >= 700 THEN 'Good (700-749)'\n",
    "            WHEN fico_avg >= 650 THEN 'Fair (650-699)'\n",
    "            WHEN fico_avg >= 600 THEN 'Poor (600-649)'\n",
    "            ELSE 'Very Poor (<600)'\n",
    "        END as fico_segment,\n",
    "        COUNT(*) as total_loans,\n",
    "        SUM(loan_status_binary) as defaults,\n",
    "        ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate,\n",
    "        ROUND(AVG(loan_amnt), 2) as avg_loan_amount,\n",
    "        ROUND(AVG(int_rate), 2) as avg_interest_rate,\n",
    "        ROUND(AVG(annual_inc), 2) as avg_income,\n",
    "        ROUND(AVG(dti), 2) as avg_dti\n",
    "    FROM loans\n",
    "    WHERE fico_avg IS NOT NULL\n",
    "    GROUP BY \n",
    "        CASE \n",
    "            WHEN fico_avg >= 750 THEN 'Excellent (750+)'\n",
    "            WHEN fico_avg >= 700 THEN 'Good (700-749)'\n",
    "            WHEN fico_avg >= 650 THEN 'Fair (650-699)'\n",
    "            WHEN fico_avg >= 600 THEN 'Poor (600-649)'\n",
    "            ELSE 'Very Poor (<600)'\n",
    "        END\n",
    "    ORDER BY default_rate\n",
    "\"\"\")\n",
    "\n",
    "risk_segments.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/medallion/gold//risk_segments_by_fico\n"
     ]
    }
   ],
   "source": [
    "risk_segments.write.mode(\"overwrite\").parquet(f\"{GOLD_PATH}/risk_segments_by_fico\")\n",
    "print(f\"Saved: {GOLD_PATH}/risk_segments_by_fico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced SQL: Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+----------+------------+---------+\n",
      "|grade|             state|loan_count|default_rate|risk_rank|\n",
      "+-----+------------------+----------+------------+---------+\n",
      "|    A|Debt Consolidation|       101|        8.91|        1|\n",
      "|    A|Debt consolidation|       107|        6.54|        2|\n",
      "|    A|                NM|      2175|        4.92|        3|\n",
      "|    A|                MS|      2059|        4.71|        4|\n",
      "|    A|debt_consolidation|       328|        4.57|        5|\n",
      "|    A|                OK|      3605|        4.47|        6|\n",
      "|    A|                LA|      4716|        4.35|        7|\n",
      "|    A|                AR|      2855|        4.34|        8|\n",
      "|    A|                SD|       763|        4.33|        9|\n",
      "|    A|                NY|     32822|        4.28|       10|\n",
      "|    A|                AL|      4348|        4.28|       10|\n",
      "|    A|                NJ|     15950|        4.19|       12|\n",
      "|    A|                NV|      5677|        4.07|       13|\n",
      "|    A|                FL|     28578|        4.06|       14|\n",
      "|    A|                AK|       949|        4.00|       15|\n",
      "|    A|                AZ|     10304|        4.00|       15|\n",
      "|    A|                NC|     11421|        3.95|       17|\n",
      "|    A|                MT|      1258|        3.82|       18|\n",
      "|    A|                CA|     59852|        3.82|       18|\n",
      "|    A|                MN|      7623|        3.80|       20|\n",
      "|    A|                KY|      3945|        3.80|       20|\n",
      "|    A|                DE|      1196|        3.76|       22|\n",
      "|    A|                MD|      9662|        3.75|       23|\n",
      "|    A|                TX|     35626|        3.70|       24|\n",
      "|    A|                PA|     13669|        3.69|       25|\n",
      "|    A|                VA|     11974|        3.69|       25|\n",
      "|    A|                MA|     11320|        3.66|       27|\n",
      "|    A|       credit_card|       165|        3.64|       28|\n",
      "|    A|                IN|      6544|        3.62|       29|\n",
      "|    A|                MI|     10367|        3.48|       30|\n",
      "+-----+------------------+----------+------------+---------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rank states by default rate within each grade\n",
    "spark.sql(\"\"\"\n",
    "    WITH state_grade_stats AS (\n",
    "        SELECT \n",
    "            grade,\n",
    "            addr_state as state,\n",
    "            COUNT(*) as loan_count,\n",
    "            ROUND(SUM(loan_status_binary) * 100.0 / COUNT(*), 2) as default_rate\n",
    "        FROM loans\n",
    "        WHERE addr_state IS NOT NULL\n",
    "        GROUP BY grade, addr_state\n",
    "        HAVING COUNT(*) >= 100  -- Filter for statistical significance\n",
    "    )\n",
    "    SELECT \n",
    "        grade,\n",
    "        state,\n",
    "        loan_count,\n",
    "        default_rate,\n",
    "        RANK() OVER (PARTITION BY grade ORDER BY default_rate DESC) as risk_rank\n",
    "    FROM state_grade_stats\n",
    "    ORDER BY grade, risk_rank\n",
    "\"\"\").show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part B: Machine Learning with MLlib\n",
    "\n",
    "This section builds a loan default prediction model using Spark MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prepare Data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 14\n",
      "Categorical features: 5\n",
      "Target: loan_status_binary\n"
     ]
    }
   ],
   "source": [
    "# Select features for the model\n",
    "# Numeric features\n",
    "numeric_features = [\n",
    "    'loan_amnt',\n",
    "    'int_rate',\n",
    "    'installment',\n",
    "    'annual_inc',\n",
    "    'dti',\n",
    "    'open_acc',\n",
    "    'pub_rec',\n",
    "    'revol_bal',\n",
    "    'revol_util',\n",
    "    'total_acc',\n",
    "    'fico_avg',\n",
    "    'loan_to_income',\n",
    "    'delinq_2yrs',\n",
    "    'inq_last_6mths'\n",
    "]\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = [\n",
    "    'term',\n",
    "    'grade',\n",
    "    'home_ownership',\n",
    "    'verification_status',\n",
    "    'purpose'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'loan_status_binary'\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Dataset: 2,231,965 rows\n",
      "+---------+--------+-----------+----------+-----+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+--------------------+------------------+\n",
      "|loan_amnt|int_rate|installment|annual_inc|  dti|open_acc|pub_rec|revol_bal|revol_util|total_acc|fico_avg|loan_to_income|delinq_2yrs|inq_last_6mths|term|grade|home_ownership|verification_status|             purpose|loan_status_binary|\n",
      "+---------+--------+-----------+----------+-----+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+--------------------+------------------+\n",
      "|  10000.0|   12.12|     332.72|   50000.0|17.71|    21.0|    0.0|  24820.0|      42.8|     39.0|   687.0|           0.2|        1.0|           0.0|  36|    B|      MORTGAGE|       Not Verified|         credit_card|                 1|\n",
      "|  30750.0|   12.12|    1023.11|   77000.0| NULL|    NULL|    8.0|      0.0|   20823.0|     85.3|    NULL|    0.39935064|      16.33|         729.0|  36|    B|          RENT|           Verified|and get me back o...|                 0|\n",
      "|   2650.0|   12.12|      88.17|   20000.0| NULL|    56.0|   NULL|     12.0|       0.0|   4317.0|    NULL|        0.1325|       NULL|         690.0|  36|    B|          RENT|       Not Verified|I don't make enou...|                 0|\n",
      "|   1200.0|   12.12|      39.93|   13000.0|17.54|     3.0|    0.0|    167.0|      16.7|      6.0|   742.0|   0.092307694|        0.0|           3.0|  36|    B|          RENT|       Not Verified|  debt_consolidation|                 0|\n",
      "|   2500.0|   14.09|      85.56|   40000.0|22.38|    12.0|    0.0|   1418.0|      34.6|     36.0|   682.0|        0.0625|        0.0|           3.0|  36|    B|      MORTGAGE|       Not Verified|    home_improvement|                 0|\n",
      "+---------+--------+-----------+----------+-----+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select columns and filter nulls in target\n",
    "all_features = numeric_features + categorical_features + [target]\n",
    "\n",
    "ml_df = loans_df.select(all_features).filter(F.col(target).isNotNull())\n",
    "\n",
    "print(f\"ML Dataset: {ml_df.count():,} rows\")\n",
    "ml_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- FIX FOR EMPTY STRINGS ---\n",
    "# # The Silver layer accidentally allowed empty strings \"\" (from whitespace).\n",
    "# # We must convert these to \"Unknown\" or None before ML processing.\n",
    "\n",
    "# print(\"Sanitizing empty strings in categorical features...\")\n",
    "\n",
    "# for col_name in categorical_features:\n",
    "#     # Replace empty strings or whitespace-only strings with \"Unknown\"\n",
    "#     ml_df = ml_df.withColumn(\n",
    "#         col_name, \n",
    "#         F.when(F.trim(F.col(col_name)) == \"\", \"Unknown\")\n",
    "#         .otherwise(F.col(col_name))\n",
    "#     )\n",
    "\n",
    "# # Also ensure no nulls exist (filling them just in case)\n",
    "# ml_df = ml_df.fillna(\"Unknown\", subset=categorical_features)\n",
    "\n",
    "# print(\"Sanitization complete. Empty strings replaced with 'Unknown'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|loan_status_binary|  count|\n",
      "+------------------+-------+\n",
      "|                 1| 292368|\n",
      "|                 0|1939597|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution (important for imbalanced data)\n",
    "ml_df.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts per column:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+----------+-----+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+-------+------------------+\n",
      "|loan_amnt|int_rate|installment|annual_inc|dti  |open_acc|pub_rec|revol_bal|revol_util|total_acc|fico_avg|loan_to_income|delinq_2yrs|inq_last_6mths|term|grade|home_ownership|verification_status|purpose|loan_status_binary|\n",
      "+---------+--------+-----------+----------+-----+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+-------+------------------+\n",
      "|0        |0       |0          |0         |37711|24274   |12047  |6342     |5611      |2548     |37202   |0             |19209      |7148          |0   |0    |49            |0                  |43     |0                 |\n",
      "+---------+--------+-----------+----------+-----+--------+-------+---------+----------+---------+--------+--------------+-----------+--------------+----+-----+--------------+-------------------+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for nulls in features\n",
    "print(\"Null counts per column:\")\n",
    "null_counts = ml_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in ml_df.columns])\n",
    "null_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer configured for 14 numeric features\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in numeric columns\n",
    "# Fill with median using Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_features,\n",
    "    outputCols=[f\"{c}_imputed\" for c in numeric_features],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "\n",
    "imputed_numeric_features = [f\"{c}_imputed\" for c in numeric_features]\n",
    "print(f\"Imputer configured for {len(numeric_features)} numeric features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 StringIndexers\n"
     ]
    }
   ],
   "source": [
    "# String Indexers for categorical features\n",
    "# This converts strings to numeric indices\n",
    "\n",
    "indexers = []\n",
    "indexed_cat_features = []\n",
    "\n",
    "for cat_col in categorical_features:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=cat_col, \n",
    "        outputCol=f\"{cat_col}_indexed\",\n",
    "        handleInvalid=\"keep\"  # Handle unseen labels\n",
    "    )\n",
    "    indexers.append(indexer)\n",
    "    indexed_cat_features.append(f\"{cat_col}_indexed\")\n",
    "\n",
    "print(f\"Created {len(indexers)} StringIndexers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 OneHotEncoders\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoders for categorical features\n",
    "encoders = []\n",
    "encoded_cat_features = []\n",
    "\n",
    "for cat_col in categorical_features:\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=f\"{cat_col}_indexed\",\n",
    "        outputCol=f\"{cat_col}_encoded\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoders.append(encoder)\n",
    "    encoded_cat_features.append(f\"{cat_col}_encoded\")\n",
    "\n",
    "print(f\"Created {len(encoders)} OneHotEncoders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorAssembler will combine 19 feature columns\n"
     ]
    }
   ],
   "source": [
    "# Assemble all features into a single vector\n",
    "all_feature_cols = imputed_numeric_features + encoded_cat_features\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_feature_cols,\n",
    "    outputCol=\"features_unscaled\",\n",
    "    handleInvalid=\"skip\"  # Skip rows with nulls\n",
    ")\n",
    "\n",
    "print(f\"VectorAssembler will combine {len(all_feature_cols)} feature columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler configured\n"
     ]
    }
   ],
   "source": [
    "# Scale features (important for Logistic Regression)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=False  # Don't center for sparse data\n",
    ")\n",
    "\n",
    "print(\"StandardScaler configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:====================>                                  (6 + 10) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1,785,954 rows (80.0%)\n",
      "Test set: 446,011 rows (20.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache for performance\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"Training set: {train_count:,} rows ({train_count/(train_count+test_count)*100:.1f}%)\")\n",
    "print(f\"Test set: {test_count:,} rows ({test_count/(train_count+test_count)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution:\n",
      "+------------------+-------+\n",
      "|loan_status_binary|  count|\n",
      "+------------------+-------+\n",
      "|                 1| 234024|\n",
      "|                 0|1551930|\n",
      "+------------------+-------+\n",
      "\n",
      "Test set class distribution:\n",
      "+------------------+------+\n",
      "|loan_status_binary| count|\n",
      "+------------------+------+\n",
      "|                 1| 58344|\n",
      "|                 0|387667|\n",
      "+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify class distribution in splits\n",
    "print(\"Training set class distribution:\")\n",
    "train_df.groupBy(target).count().show()\n",
    "\n",
    "print(\"Test set class distribution:\")\n",
    "test_df.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Pipeline: 14 stages\n"
     ]
    }
   ],
   "source": [
    "# Create Logistic Regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target,\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.8  # L1/L2 mix\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "lr_pipeline = Pipeline(stages=[\n",
    "    imputer,\n",
    "    *indexers,\n",
    "    *encoders,\n",
    "    assembler,\n",
    "    scaler,\n",
    "    lr\n",
    "])\n",
    "\n",
    "print(f\"Logistic Regression Pipeline: {len(lr_pipeline.getStages())} stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "CPU times: user 96.6 ms, sys: 47 ms, total: 144 ms\n",
      "Wall time: 36.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr_model = lr_pipeline.fit(train_df)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+--------+------------------+----------+-----------------------------------------+\n",
      "|loan_amnt|int_rate|grade|fico_avg|loan_status_binary|prediction|probability                              |\n",
      "+---------+--------+-----+--------+------------------+----------+-----------------------------------------+\n",
      "|1000.0   |5.32    |A    |742.0   |0                 |0.0       |[0.9590079632618337,0.040992036738166315]|\n",
      "|1000.0   |5.32    |A    |777.0   |0                 |0.0       |[0.9531010349517048,0.04689896504829516] |\n",
      "|1000.0   |5.32    |A    |802.0   |0                 |0.0       |[0.964517878394326,0.03548212160567399]  |\n",
      "|1000.0   |6.08    |A    |747.0   |0                 |0.0       |[0.9532996313017876,0.04670036869821237] |\n",
      "|1000.0   |6.72    |A    |787.0   |0                 |0.0       |[0.9578190301664191,0.042180969833580884]|\n",
      "|1000.0   |6.97    |A    |757.0   |0                 |0.0       |[0.9538840366282096,0.04611596337179036] |\n",
      "|1000.0   |7.07    |A    |797.0   |0                 |0.0       |[0.9537876172458645,0.04621238275413553] |\n",
      "|1000.0   |7.21    |A    |697.0   |0                 |0.0       |[0.9408570323409235,0.05914296765907645] |\n",
      "|1000.0   |7.35    |A    |757.0   |0                 |0.0       |[0.9544231130614023,0.04557688693859774] |\n",
      "|1000.0   |7.39    |A    |747.0   |0                 |0.0       |[0.9488483310222169,0.051151668977783116]|\n",
      "+---------+--------+-----+--------+------------------+----------+-----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Show sample predictions\n",
    "lr_predictions.select(\n",
    "    'loan_amnt', 'int_rate', 'grade', 'fico_avg',\n",
    "    target, 'prediction', 'probability'\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 227:===============================>                        (9 + 7) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOGISTIC REGRESSION RESULTS\n",
      "==================================================\n",
      "AUC-ROC: 0.6878\n",
      "AUC-PR:  0.2305\n",
      "Accuracy: 0.8691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=target,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=target,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "lr_auc = evaluator_auc.evaluate(lr_predictions)\n",
    "lr_pr = evaluator_pr.evaluate(lr_predictions)\n",
    "lr_accuracy = evaluator_accuracy.evaluate(lr_predictions)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LOGISTIC REGRESSION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"AUC-ROC: {lr_auc:.4f}\")\n",
    "print(f\"AUC-PR:  {lr_pr:.4f}\")\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 229:==================================>                    (10 + 6) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+------+\n",
      "|loan_status_binary|prediction| count|\n",
      "+------------------+----------+------+\n",
      "|                 0|       0.0|387639|\n",
      "|                 0|       1.0|    28|\n",
      "|                 1|       0.0| 58337|\n",
      "|                 1|       1.0|     7|\n",
      "+------------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "lr_predictions.groupBy(target, 'prediction').count().orderBy(target, 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Pipeline: 13 stages\n"
     ]
    }
   ],
   "source": [
    "# Create Random Forest model (doesn't need scaling)\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features_unscaled\",  # Use unscaled features\n",
    "    labelCol=target,\n",
    "    numTrees=20,\n",
    "    maxDepth=5,\n",
    "    minInstancesPerNode=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Build pipeline (without scaler)\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    imputer,\n",
    "    *indexers,\n",
    "    *encoders,\n",
    "    assembler,\n",
    "    rf\n",
    "])\n",
    "\n",
    "print(f\"Random Forest Pipeline: {len(rf_pipeline.getStages())} stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 12:46:24 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.18.110: Command exited with code 137\n",
      "25/11/27 12:47:15 ERROR TaskSchedulerImpl: Lost executor 4 on 192.168.18.110: Command exited with code 137\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "CPU times: user 365 ms, sys: 914 ms, total: 1.28 s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model = rf_pipeline.fit(train_df)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 290:============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RANDOM FOREST RESULTS\n",
      "==================================================\n",
      "AUC-ROC: 0.5000\n",
      "AUC-PR:  0.1308\n",
      "Accuracy: 0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_auc = evaluator_auc.evaluate(rf_predictions)\n",
    "rf_pr = evaluator_pr.evaluate(rf_predictions)\n",
    "rf_accuracy = evaluator_accuracy.evaluate(rf_predictions)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RANDOM FOREST RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"AUC-ROC: {rf_auc:.4f}\")\n",
    "print(f\"AUC-PR:  {rf_pr:.4f}\")\n",
    "print(f\"Accuracy: {rf_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 292:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+------+\n",
      "|loan_status_binary|prediction| count|\n",
      "+------------------+----------+------+\n",
      "|                 0|       0.0|387667|\n",
      "|                 1|       0.0| 58344|\n",
      "+------------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "rf_predictions.groupBy(target, 'prediction').count().orderBy(target, 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Feature Importances (by index):\n",
      "Index    Importance  \n",
      "--------------------\n",
      "0        0.0000\n",
      "1        0.0000\n",
      "2        0.0000\n",
      "3        0.0000\n",
      "4        0.0000\n",
      "5        0.0000\n",
      "6        0.0000\n",
      "7        0.0000\n",
      "8        0.0000\n",
      "9        0.0000\n",
      "10       0.0000\n",
      "11       0.0000\n",
      "12       0.0000\n",
      "13       0.0000\n",
      "14       0.0000\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance (Random Forest)\n",
    "rf_model_final = rf_model.stages[-1]\n",
    "feature_importance = rf_model_final.featureImportances\n",
    "\n",
    "# Get feature names\n",
    "# Note: This is an approximation since OneHot expands features\n",
    "print(\"\\nTop Feature Importances (by index):\")\n",
    "importance_list = [(i, float(imp)) for i, imp in enumerate(feature_importance)]\n",
    "sorted_importance = sorted(importance_list, key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "print(f\"{'Index':<8} {'Importance':<12}\")\n",
    "print(\"-\" * 20)\n",
    "for idx, imp in sorted_importance:\n",
    "    print(f\"{idx:<8} {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON\n",
      "============================================================\n",
      "Metric          Logistic Regression  Random Forest       \n",
      "------------------------------------------------------------\n",
      "AUC-ROC         0.6878               0.5000              \n",
      "AUC-PR          0.2305               0.1308              \n",
      "Accuracy        0.8691               0.8692              \n",
      "============================================================\n",
      "\n",
      "Best Model: Logistic Regression (AUC: 0.6878)\n"
     ]
    }
   ],
   "source": [
    "# Compare models\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<15} {'Logistic Regression':<20} {'Random Forest':<20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'AUC-ROC':<15} {lr_auc:<20.4f} {rf_auc:<20.4f}\")\n",
    "print(f\"{'AUC-PR':<15} {lr_pr:<20.4f} {rf_pr:<20.4f}\")\n",
    "print(f\"{'Accuracy':<15} {lr_accuracy:<20.4f} {rf_accuracy:<20.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = \"Random Forest\" if rf_auc > lr_auc else \"Logistic Regression\"\n",
    "best_model = rf_model if rf_auc > lr_auc else lr_model\n",
    "best_auc = max(rf_auc, lr_auc)\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} (AUC: {best_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Hyperparameter Tuning with Cross-Validation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid size: 4 combinations\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "# Note: This can be time-consuming, so we use a small grid\n",
    "\n",
    "# Create a new RF for tuning\n",
    "rf_tune = RandomForestClassifier(\n",
    "    featuresCol=\"features_unscaled\",\n",
    "    labelCol=target,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Pipeline for tuning\n",
    "rf_tune_pipeline = Pipeline(stages=[\n",
    "    imputer,\n",
    "    *indexers,\n",
    "    *encoders,\n",
    "    assembler,\n",
    "    rf_tune\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_tune.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf_tune.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Parameter grid size: {len(paramGrid)} combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CV on sample of 357,747 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation complete!\n",
      "CPU times: user 2.41 s, sys: 1.34 s, total: 3.75 s\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=rf_tune_pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator_auc,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Sample data for faster tuning (optional - remove for full tuning)\n",
    "train_sample = train_df.sample(0.2, seed=42)\n",
    "print(f\"Training CV on sample of {train_sample.count():,} rows...\")\n",
    "\n",
    "# Fit cross-validator\n",
    "cv_model = cv.fit(train_sample)\n",
    "print(\"Cross-validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation AUC scores:\n",
      "  Config 1: 0.6914\n",
      "  Config 2: 0.6945\n",
      "  Config 3: 0.6915\n",
      "  Config 4: 0.6942\n",
      "\n",
      "Best CV AUC: 0.6945\n"
     ]
    }
   ],
   "source": [
    "# Best model from CV\n",
    "print(\"Cross-validation AUC scores:\")\n",
    "for i, score in enumerate(cv_model.avgMetrics):\n",
    "    print(f\"  Config {i+1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest CV AUC: {max(cv_model.avgMetrics):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV Model Test AUC: 0.6924\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best CV model on test set\n",
    "cv_predictions = cv_model.transform(test_df)\n",
    "cv_auc = evaluator_auc.evaluate(cv_predictions)\n",
    "print(f\"Best CV Model Test AUC: {cv_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Save Models and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ../data/medallion/gold//models/default_prediction_model\n"
     ]
    }
   ],
   "source": [
    "# Save best model\n",
    "MODEL_PATH = f\"{GOLD_PATH}/models/default_prediction_model\"\n",
    "best_model.write().overwrite().save(MODEL_PATH)\n",
    "print(f\"Model saved to: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1088:================>                                     (5 + 11) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: ../data/medallion/gold//predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "# Use the predictions from the best model\n",
    "best_predictions = rf_predictions if rf_auc > lr_auc else lr_predictions\n",
    "\n",
    "predictions_to_save = best_predictions.select(\n",
    "    'loan_amnt', 'int_rate', 'term', 'grade',\n",
    "    'annual_inc', 'dti', 'fico_avg', 'purpose', 'home_ownership',\n",
    "    target, 'prediction', 'probability'\n",
    ")\n",
    "\n",
    "PREDICTIONS_PATH = f\"{GOLD_PATH}/predictions\"\n",
    "predictions_to_save.write.mode(\"overwrite\").parquet(PREDICTIONS_PATH)\n",
    "print(f\"Predictions saved to: {PREDICTIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Create Risk Scoring Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1089:===============================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+-------------------+\n",
      "| risk_category| count|avg_prob|actual_default_rate|\n",
      "+--------------+------+--------+-------------------+\n",
      "|      Low Risk|147438|  0.0686|             0.0522|\n",
      "|   Medium Risk|274452|  0.1498|              0.158|\n",
      "|     High Risk| 24086|  0.3012|             0.3024|\n",
      "|Very High Risk|    35|  0.5969|                0.2|\n",
      "+--------------+------+--------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a risk scoring summary for business use\n",
    "# Extract probability of default from the probability vector\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# UDF to extract probability of default (class 1)\n",
    "@udf(FloatType())\n",
    "def extract_prob_default(probability):\n",
    "    return float(probability[1])\n",
    "\n",
    "risk_scores = best_predictions.withColumn(\n",
    "    \"default_probability\", \n",
    "    extract_prob_default(F.col(\"probability\"))\n",
    ")\n",
    "\n",
    "# Create risk categories\n",
    "risk_scores = risk_scores.withColumn(\n",
    "    \"risk_category\",\n",
    "    F.when(F.col(\"default_probability\") < 0.1, \"Low Risk\")\n",
    "    .when(F.col(\"default_probability\") < 0.25, \"Medium Risk\")\n",
    "    .when(F.col(\"default_probability\") < 0.5, \"High Risk\")\n",
    "    .otherwise(\"Very High Risk\")\n",
    ")\n",
    "\n",
    "# Show distribution\n",
    "risk_scores.groupBy(\"risk_category\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.round(F.avg(\"default_probability\"), 4).alias(\"avg_prob\"),\n",
    "    F.round(F.avg(target), 4).alias(\"actual_default_rate\")\n",
    ").orderBy(\"avg_prob\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1092:===========================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk scores saved to: ../data/medallion/gold//risk_scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save risk scores\n",
    "risk_score_table = risk_scores.select(\n",
    "    'loan_amnt', 'int_rate', 'grade', 'fico_avg', 'annual_inc',\n",
    "    'purpose', target, 'default_probability', 'risk_category'\n",
    ")\n",
    "\n",
    "RISK_SCORES_PATH = f\"{GOLD_PATH}/risk_scores\"\n",
    "risk_score_table.write.mode(\"overwrite\").parquet(RISK_SCORES_PATH)\n",
    "print(f\"Risk scores saved to: {RISK_SCORES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Gold Layer Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "drwxrwxr-x 11 ubuntu ubuntu 4096 Nov 27 12:54 .\n",
      "drwxrwxr-x  5 ubuntu ubuntu 4096 Nov 26 16:25 ..\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 27 12:44 default_rate_by_grade\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 27 12:44 default_rate_by_subgrade\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 27 12:44 loan_analysis_by_purpose\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 27 12:44 loan_analysis_by_state\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 27 12:44 loan_trends_by_year\n",
      "drwxr-xr-x  3 ubuntu ubuntu 4096 Nov 27 12:54 models\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 27 12:54 predictions\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 27 12:54 risk_scores\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4096 Nov 27 12:44 risk_segments_by_fico\n"
     ]
    }
   ],
   "source": [
    "# List all Gold layer outputs\n",
    "!ls -la {GOLD_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GOLD LAYER SUMMARY\n",
      "======================================================================\n",
      "\n",
      "--- Part A: SQL Analytics ---\n",
      "Tables created for business intelligence:\n",
      "  1. default_rate_by_grade     - Default rates by loan grade\n",
      "  2. default_rate_by_subgrade  - Default rates by sub-grade\n",
      "  3. loan_analysis_by_state    - Geographic analysis\n",
      "  4. loan_analysis_by_purpose  - Analysis by loan purpose\n",
      "  5. loan_trends_by_year       - Time series analysis\n",
      "  6. risk_segments_by_fico     - FICO-based risk segments\n",
      "\n",
      "--- Part B: Machine Learning ---\n",
      "Models trained:\n",
      "  1. Logistic Regression - AUC: 0.6878\n",
      "  2. Random Forest       - AUC: 0.5000\n",
      "\n",
      "Best model: Logistic Regression\n",
      "\n",
      "ML outputs:\n",
      "  - models/default_prediction_model - Trained ML pipeline\n",
      "  - predictions                     - Test set predictions\n",
      "  - risk_scores                     - Risk scoring table\n",
      "\n",
      "--- Technologies Used ---\n",
      "  - Spark SQL: Complex queries, aggregations, window functions\n",
      "  - MLlib: Pipeline, VectorAssembler, StringIndexer, OneHotEncoder\n",
      "  - MLlib: LogisticRegression, RandomForestClassifier\n",
      "  - MLlib: CrossValidator, BinaryClassificationEvaluator\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"GOLD LAYER SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n--- Part A: SQL Analytics ---\")\n",
    "print(\"Tables created for business intelligence:\")\n",
    "print(f\"  1. default_rate_by_grade     - Default rates by loan grade\")\n",
    "print(f\"  2. default_rate_by_subgrade  - Default rates by sub-grade\")\n",
    "print(f\"  3. loan_analysis_by_state    - Geographic analysis\")\n",
    "print(f\"  4. loan_analysis_by_purpose  - Analysis by loan purpose\")\n",
    "print(f\"  5. loan_trends_by_year       - Time series analysis\")\n",
    "print(f\"  6. risk_segments_by_fico     - FICO-based risk segments\")\n",
    "\n",
    "print(\"\\n--- Part B: Machine Learning ---\")\n",
    "print(\"Models trained:\")\n",
    "print(f\"  1. Logistic Regression - AUC: {lr_auc:.4f}\")\n",
    "print(f\"  2. Random Forest       - AUC: {rf_auc:.4f}\")\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "print(\"\\nML outputs:\")\n",
    "print(f\"  - models/default_prediction_model - Trained ML pipeline\")\n",
    "print(f\"  - predictions                     - Test set predictions\")\n",
    "print(f\"  - risk_scores                     - Risk scoring table\")\n",
    "\n",
    "print(\"\\n--- Technologies Used ---\")\n",
    "print(\"  - Spark SQL: Complex queries, aggregations, window functions\")\n",
    "print(\"  - MLlib: Pipeline, VectorAssembler, StringIndexer, OneHotEncoder\")\n",
    "print(\"  - MLlib: LogisticRegression, RandomForestClassifier\")\n",
    "print(\"  - MLlib: CrossValidator, BinaryClassificationEvaluator\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached DataFrames unpersisted.\n",
      "\n",
      "Gold layer complete!\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "\n",
    "print(\"Cached DataFrames unpersisted.\")\n",
    "print(\"\\nGold layer complete!\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "The Gold layer provides:\n",
    "\n",
    "1. **Business Analytics** - Ready-to-use aggregated tables for dashboards\n",
    "2. **ML Model** - Trained loan default prediction model\n",
    "3. **Risk Scoring** - Probability-based risk categorization\n",
    "\n",
    "These outputs can be:\n",
    "- Connected to BI tools (Tableau, Power BI)\n",
    "- Served via REST API for applications\n",
    "- Used for real-time scoring of new loan applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
