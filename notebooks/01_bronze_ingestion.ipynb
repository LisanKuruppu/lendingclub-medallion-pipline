{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer - Data Ingestion\n",
    "\n",
    "## Lending Club Loan Data Pipeline\n",
    "\n",
    "**Use Case:** Predict loan default risk and analyze factors affecting loan approval\n",
    "\n",
    "This notebook handles the first layer of the Medallion Architecture:\n",
    "- Load raw CSV data\n",
    "- Minimal transformation (preserve raw state)\n",
    "- Store in efficient Parquet format\n",
    "\n",
    "**Dataset:** Lending Club Loan Data (2007-2018)\n",
    "- `accepted_2007_to_2018Q4.csv` - Approved loans\n",
    "- `rejected_2007_to_2018Q4.csv` - Rejected loan applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from typing import List, Tuple, Any\n",
    "import builtins\n",
    "import findspark\n",
    "import os\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# For Spark (will install if needed)\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import *\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql.types import *\n",
    "    pyspark_available = True\n",
    "except ImportError:\n",
    "    print(\"PySpark not available. Install with: pip install pyspark\")\n",
    "    pyspark_available = False\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/27 13:23:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Spark UI available at: http://spark-master:4040\n"
     ]
    }
   ],
   "source": [
    "if pyspark_available:\n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"LendingClub-Bronze-Layer\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "    # Set log level to reduce noise\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Spark UI available at: {sc.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted loans file exists: True\n",
      "Rejected loans file exists: True\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "RAW_DATA_PATH = \"../data/lendingclub/\"\n",
    "BRONZE_PATH = \"../data/medallion/bronze/\"\n",
    "\n",
    "# Create bronze directory if it doesn't exist\n",
    "os.makedirs(BRONZE_PATH, exist_ok=True)\n",
    "\n",
    "# Input files\n",
    "ACCEPTED_LOANS_FILE = os.path.join(RAW_DATA_PATH, \"accepted_2007_to_2018Q4.csv\")\n",
    "REJECTED_LOANS_FILE = os.path.join(RAW_DATA_PATH, \"rejected_2007_to_2018Q4.csv\")\n",
    "\n",
    "print(f\"Accepted loans file exists: {os.path.exists(ACCEPTED_LOANS_FILE)}\")\n",
    "print(f\"Rejected loans file exists: {os.path.exists(REJECTED_LOANS_FILE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration (Quick Look at Raw Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,member_id,loan_amnt,funded_amnt,funded_amnt_inv,term,int_rate,installment,grade,sub_grade,emp_title,emp_length,home_ownership,annual_inc,verification_status,issue_d,loan_status,pymnt_plan,url,desc,purpose,title,zip_code,addr_state,dti,delinq_2yrs,earliest_cr_line,fico_range_low,fico_range_high,inq_last_6mths,mths_since_last_delinq,mths_since_last_record,open_acc,pub_rec,revol_bal,revol_util,total_acc,initial_list_status,out_prncp,out_prncp_inv,total_pymnt,total_pymnt_inv,total_rec_prncp,total_rec_int,total_rec_late_fee,recoveries,collection_recovery_fee,last_pymnt_d,last_pymnt_amnt,next_pymnt_d,last_credit_pull_d,last_fico_range_high,last_fico_range_low,collections_12_mths_ex_med,mths_since_last_major_derog,policy_code,application_type,annual_inc_joint,dti_joint,verification_status_joint,acc_now_delinq,tot_coll_amt,tot_cur_bal,open_acc_6m,open_act_il,open_il_12m,open_il_24m,mths_since_rcnt_il,total_bal_il,il_util,open_rv_12m,open_rv_24m,max_bal_bc,all_util,total_rev_hi_lim,inq_fi,total_cu_tl,inq_last_12m,acc_open_past_24mths,avg_cur_bal,bc_open_to_buy,bc_util,chargeoff_within_12_mths,delinq_amnt,mo_sin_old_il_acct,mo_sin_old_rev_tl_op,mo_sin_rcnt_rev_tl_op,mo_sin_rcnt_tl,mort_acc,mths_since_recent_bc,mths_since_recent_bc_dlq,mths_since_recent_inq,mths_since_recent_revol_delinq,num_accts_ever_120_pd,num_actv_bc_tl,num_actv_rev_tl,num_bc_sats,num_bc_tl,num_il_tl,num_op_rev_tl,num_rev_accts,num_rev_tl_bal_gt_0,num_sats,num_tl_120dpd_2m,num_tl_30dpd,num_tl_90g_dpd_24m,num_tl_op_past_12m,pct_tl_nvr_dlq,percent_bc_gt_75,pub_rec_bankruptcies,tax_liens,tot_hi_cred_lim,total_bal_ex_mort,total_bc_limit,total_il_high_credit_limit,revol_bal_joint,sec_app_fico_range_low,sec_app_fico_range_high,sec_app_earliest_cr_line,sec_app_inq_last_6mths,sec_app_mort_acc,sec_app_open_acc,sec_app_revol_util,sec_app_open_act_il,sec_app_num_rev_accts,sec_app_chargeoff_within_12_mths,sec_app_collections_12_mths_ex_med,sec_app_mths_since_last_major_derog,hardship_flag,hardship_type,hardship_reason,hardship_status,deferral_term,hardship_amount,hardship_start_date,hardship_end_date,payment_plan_start_date,hardship_length,hardship_dpd,hardship_loan_status,orig_projected_additional_accrued_interest,hardship_payoff_balance_amount,hardship_last_payment_amount,disbursement_method,debt_settlement_flag,debt_settlement_flag_date,settlement_status,settlement_date,settlement_amount,settlement_percentage,settlement_term\n",
      "68407277,,3600.0,3600.0,3600.0, 36 months,13.99,123.03,C,C4,leadman,10+ years,MORTGAGE,55000.0,Not Verified,Dec-2015,Fully Paid,n,https://lendingclub.com/browse/loanDetail.action?loan_id=68407277,,debt_consolidation,Debt consolidation,190xx,PA,5.91,0.0,Aug-2003,675.0,679.0,1.0,30.0,,7.0,0.0,2765.0,29.7,13.0,w,0.0,0.0,4421.723916800001,4421.72,3600.0,821.72,0.0,0.0,0.0,Jan-2019,122.67,,Mar-2019,564.0,560.0,0.0,30.0,1.0,Individual,,,,0.0,722.0,144904.0,2.0,2.0,0.0,1.0,21.0,4981.0,36.0,3.0,3.0,722.0,34.0,9300.0,3.0,1.0,4.0,4.0,20701.0,1506.0,37.2,0.0,0.0,148.0,128.0,3.0,3.0,1.0,4.0,69.0,4.0,69.0,2.0,2.0,4.0,2.0,5.0,3.0,4.0,9.0,4.0,7.0,0.0,0.0,0.0,3.0,76.9,0.0,0.0,0.0,178050.0,7746.0,2400.0,13734.0,,,,,,,,,,,,,,N,,,,,,,,,,,,,,,Cash,N,,,,,,\n",
      "68355089,,24700.0,24700.0,24700.0, 36 months,11.99,820.28,C,C1,Engineer,10+ years,MORTGAGE,65000.0,Not Verified,Dec-2015,Fully Paid,n,https://lendingclub.com/browse/loanDetail.action?loan_id=68355089,,small_business,Business,577xx,SD,16.06,1.0,Dec-1999,715.0,719.0,4.0,6.0,,22.0,0.0,21470.0,19.2,38.0,w,0.0,0.0,25679.66,25679.66,24700.0,979.66,0.0,0.0,0.0,Jun-2016,926.35,,Mar-2019,699.0,695.0,0.0,,1.0,Individual,,,,0.0,0.0,204396.0,1.0,1.0,0.0,1.0,19.0,18005.0,73.0,2.0,3.0,6472.0,29.0,111800.0,0.0,0.0,6.0,4.0,9733.0,57830.0,27.1,0.0,0.0,113.0,192.0,2.0,2.0,4.0,2.0,,0.0,6.0,0.0,5.0,5.0,13.0,17.0,6.0,20.0,27.0,5.0,22.0,0.0,0.0,0.0,2.0,97.4,7.7,0.0,0.0,314017.0,39475.0,79300.0,24667.0,,,,,,,,,,,,,,N,,,,,,,,,,,,,,,Cash,N,,,,,,\n",
      "68341763,,20000.0,20000.0,20000.0, 60 months,10.78,432.66,B,B4,truck driver,10+ years,MORTGAGE,63000.0,Not Verified,Dec-2015,Fully Paid,n,https://lendingclub.com/browse/loanDetail.action?loan_id=68341763,,home_improvement,,605xx,IL,10.78,0.0,Aug-2000,695.0,699.0,0.0,,,6.0,0.0,7869.0,56.2,18.0,w,0.0,0.0,22705.924293878397,22705.92,20000.0,2705.92,0.0,0.0,0.0,Jun-2017,15813.3,,Mar-2019,704.0,700.0,0.0,,1.0,Joint App,71000.0,13.85,Not Verified,0.0,0.0,189699.0,0.0,1.0,0.0,4.0,19.0,10827.0,73.0,0.0,2.0,2081.0,65.0,14000.0,2.0,5.0,1.0,6.0,31617.0,2737.0,55.9,0.0,0.0,125.0,184.0,14.0,14.0,5.0,101.0,,10.0,,0.0,2.0,3.0,2.0,4.0,6.0,4.0,7.0,3.0,6.0,0.0,0.0,0.0,0.0,100.0,50.0,0.0,0.0,218418.0,18696.0,6200.0,14877.0,,,,,,,,,,,,,,N,,,,,,,,,,,,,,,Cash,N,,,,,,\n",
      "66310712,,35000.0,35000.0,35000.0, 60 months,14.85,829.9,C,C5,Information Systems Officer,10+ years,MORTGAGE,110000.0,Source Verified,Dec-2015,Current,n,https://lendingclub.com/browse/loanDetail.action?loan_id=66310712,,debt_consolidation,Debt consolidation,076xx,NJ,17.06,0.0,Sep-2008,785.0,789.0,0.0,,,13.0,0.0,7802.0,11.6,17.0,w,15897.65,15897.65,31464.01,31464.01,19102.35,12361.66,0.0,0.0,0.0,Feb-2019,829.9,Apr-2019,Mar-2019,679.0,675.0,0.0,,1.0,Individual,,,,0.0,0.0,301500.0,1.0,1.0,0.0,1.0,23.0,12609.0,70.0,1.0,1.0,6987.0,45.0,67300.0,0.0,1.0,0.0,2.0,23192.0,54962.0,12.1,0.0,0.0,36.0,87.0,2.0,2.0,1.0,2.0,,,,0.0,4.0,5.0,8.0,10.0,2.0,10.0,13.0,5.0,13.0,0.0,0.0,0.0,1.0,100.0,0.0,0.0,0.0,381215.0,52226.0,62500.0,18000.0,,,,,,,,,,,,,,N,,,,,,,,,,,,,,,Cash,N,,,,,,\n"
     ]
    }
   ],
   "source": [
    "# Quick peek at the raw files using shell commands\n",
    "# This helps understand the structure before loading into Spark\n",
    "!head -5 {ACCEPTED_LOANS_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.3G\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.6G Nov 26 13:25 accepted_2007_to_2018Q4.csv\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.7G Nov 26 13:28 rejected_2007_to_2018Q4.csv\n"
     ]
    }
   ],
   "source": [
    "# Check file sizes\n",
    "!ls -lh {RAW_DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260702 ../data/lendingclub/accepted_2007_to_2018Q4.csv\n",
      "27648742 ../data/lendingclub/rejected_2007_to_2018Q4.csv\n"
     ]
    }
   ],
   "source": [
    "# Count lines in files (to know what we're dealing with)\n",
    "!wc -l {ACCEPTED_LOANS_FILE}\n",
    "!wc -l {REJECTED_LOANS_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ingest Accepted Loans Data (Using RDDs)\n",
    "\n",
    "**Note:** We use RDDs for Bronze layer as required by the project.\n",
    "This demonstrates understanding of low-level Spark operations and MapReduce concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Accepted Loans with RDD ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 151\n",
      "First 5 columns: ['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==================================================>      (44 + 6) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data rows (excluding header): 2260701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RDD-based ingestion for Bronze layer\n",
    "# Read raw CSV file as text lines using RDD\n",
    "\n",
    "if pyspark_available:\n",
    "    print(\"=== Loading Accepted Loans with RDD ===\")\n",
    "    \n",
    "    # Load file as RDD of text lines\n",
    "    raw_rdd = spark.sparkContext.textFile(ACCEPTED_LOANS_FILE)\n",
    "    \n",
    "    # Extract header (first line)\n",
    "    header = raw_rdd.first()\n",
    "    header_cols = header.split(\",\")\n",
    "    print(f\"Number of columns: {len(header_cols)}\")\n",
    "    print(f\"First 5 columns: {header_cols[:5]}\")\n",
    "    \n",
    "    # Filter out header row\n",
    "    data_rdd = raw_rdd.filter(lambda row: row != header)\n",
    "    \n",
    "    print(f\"Raw data rows (excluding header): {data_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze RDD created: 2260701 records\n",
      "\n",
      "Sample record:\n",
      "Keys: ['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade']...\n"
     ]
    }
   ],
   "source": [
    "# Define parsing function for Bronze layer\n",
    "# This uses MapReduce concept: map each CSV line to a dictionary\n",
    "\n",
    "def parse_accepted_loan(line):\n",
    "    \"\"\"\n",
    "    Parse a CSV line into a dictionary with metadata.\n",
    "    Bronze layer: minimal transformation, preserve raw data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Simple CSV parsing (note: doesn't handle quoted commas - that's OK for Bronze)\n",
    "        values = line.split(\",\")\n",
    "        \n",
    "        # Create record as dictionary\n",
    "        record = {}\n",
    "        for i, col_name in enumerate(header_cols):\n",
    "            record[col_name] = values[i] if i < len(values) else None\n",
    "        \n",
    "        # Add Bronze layer metadata\n",
    "        record['_ingestion_timestamp'] = time.time()\n",
    "        record['_source_file'] = 'accepted_2007_to_2018Q4.csv'\n",
    "        record['_data_source'] = 'lending_club'\n",
    "        record['_status'] = 'valid'\n",
    "        \n",
    "        return record\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Error handling: preserve raw data for debugging\n",
    "        return {\n",
    "            '_raw_data': line,\n",
    "            '_ingestion_timestamp': time.time(),\n",
    "            '_source_file': 'accepted_2007_to_2018Q4.csv',\n",
    "            '_data_source': 'lending_club',\n",
    "            '_status': 'parse_error',\n",
    "            '_error_message': str(e)\n",
    "        }\n",
    "\n",
    "# Apply parsing using map() - this is the MapReduce pattern\n",
    "accepted_bronze_rdd = data_rdd.map(parse_accepted_loan)\n",
    "\n",
    "print(f\"Bronze RDD created: {accepted_bronze_rdd.count()} records\")\n",
    "\n",
    "# Show sample record\n",
    "print(\"\\nSample record:\")\n",
    "sample = accepted_bronze_rdd.take(1)[0]\n",
    "print(f\"Keys: {list(sample.keys())[:10]}...\")  # Show first 10 keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Check:\n",
      "  Valid records: 2,260,701\n",
      "  Parse errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=================================================>       (43 + 7) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Error rate: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Data quality check: count valid vs error records\n",
    "valid_count = accepted_bronze_rdd.filter(lambda r: r.get('_status') == 'valid').count()\n",
    "error_count = accepted_bronze_rdd.filter(lambda r: r.get('_status') == 'parse_error').count()\n",
    "\n",
    "print(f\"\\nData Quality Check:\")\n",
    "print(f\"  Valid records: {valid_count:,}\")\n",
    "print(f\"  Parse errors: {error_count:,}\")\n",
    "print(f\"  Error rate: {error_count/accepted_bronze_rdd.count()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample valid records (first 3):\n",
      "\n",
      "Record 1:\n",
      "  loan_amnt: 3600.0\n",
      "  term:  36 months\n",
      "  int_rate: 13.99\n",
      "  grade: C\n",
      "  loan_status: Fully Paid\n",
      "  _ingestion_timestamp: 1764249851.0851123\n",
      "  _status: valid\n",
      "\n",
      "Record 2:\n",
      "  loan_amnt: 24700.0\n",
      "  term:  36 months\n",
      "  int_rate: 11.99\n",
      "  grade: C\n",
      "  loan_status: Fully Paid\n",
      "  _ingestion_timestamp: 1764249851.0851736\n",
      "  _status: valid\n",
      "\n",
      "Record 3:\n",
      "  loan_amnt: 20000.0\n",
      "  term:  60 months\n",
      "  int_rate: 10.78\n",
      "  grade: B\n",
      "  loan_status: Fully Paid\n",
      "  _ingestion_timestamp: 1764249851.085191\n",
      "  _status: valid\n"
     ]
    }
   ],
   "source": [
    "# Show sample records using RDD operations\n",
    "print(\"Sample valid records (first 3):\")\n",
    "for i, record in enumerate(accepted_bronze_rdd.filter(lambda r: r.get('_status') == 'valid').take(3), 1):\n",
    "    print(f\"\\nRecord {i}:\")\n",
    "    # Show subset of fields for readability\n",
    "    sample_fields = ['loan_amnt', 'term', 'int_rate', 'grade', 'loan_status', '_ingestion_timestamp', '_status']\n",
    "    for field in sample_fields:\n",
    "        if field in record:\n",
    "            print(f\"  {field}: {record[field]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ingest Rejected Loans Data (Using RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Rejected Loans with RDD ===\n",
      "Number of columns: 9\n",
      "Columns: ['Amount Requested', 'Application Date', 'Loan Title', 'Risk_Score', 'Debt-To-Income Ratio', 'Zip Code', 'State', 'Employment Length', 'Policy Code']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=================================================>       (47 + 7) / 54]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data rows: 27648741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RDD-based ingestion for rejected loans\n",
    "if pyspark_available:\n",
    "    print(\"=== Loading Rejected Loans with RDD ===\")\n",
    "    \n",
    "    # Load file as RDD\n",
    "    rejected_raw_rdd = spark.sparkContext.textFile(REJECTED_LOANS_FILE)\n",
    "    \n",
    "    # Extract header\n",
    "    rejected_header = rejected_raw_rdd.first()\n",
    "    rejected_header_cols = rejected_header.split(\",\")\n",
    "    print(f\"Number of columns: {len(rejected_header_cols)}\")\n",
    "    print(f\"Columns: {rejected_header_cols}\")\n",
    "    \n",
    "    # Filter out header\n",
    "    rejected_data_rdd = rejected_raw_rdd.filter(lambda row: row != rejected_header)\n",
    "    print(f\"Raw data rows: {rejected_data_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==================================================>     (49 + 5) / 54]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze RDD created: 27648741 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define parsing function for rejected loans\n",
    "def parse_rejected_loan(line):\n",
    "    \"\"\"Parse rejected loan CSV line into dictionary.\"\"\"\n",
    "    try:\n",
    "        values = line.split(\",\")\n",
    "        \n",
    "        record = {}\n",
    "        for i, col_name in enumerate(rejected_header_cols):\n",
    "            record[col_name] = values[i] if i < len(values) else None\n",
    "        \n",
    "        # Add metadata\n",
    "        record['_ingestion_timestamp'] = time.time()\n",
    "        record['_source_file'] = 'rejected_2007_to_2018Q4.csv'\n",
    "        record['_data_source'] = 'lending_club'\n",
    "        record['_status'] = 'valid'\n",
    "        \n",
    "        return record\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            '_raw_data': line,\n",
    "            '_ingestion_timestamp': time.time(),\n",
    "            '_source_file': 'rejected_2007_to_2018Q4.csv',\n",
    "            '_data_source': 'lending_club',\n",
    "            '_status': 'parse_error',\n",
    "            '_error_message': str(e)\n",
    "        }\n",
    "\n",
    "# Apply parsing using map()\n",
    "rejected_bronze_rdd = rejected_data_rdd.map(parse_rejected_loan)\n",
    "\n",
    "print(f\"Bronze RDD created: {rejected_bronze_rdd.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==================================================>     (49 + 5) / 54]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rejected Loans Quality Check:\n",
      "  Valid records: 27,648,741\n",
      "  Parse errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Data quality check for rejected loans\n",
    "rejected_valid = rejected_bronze_rdd.filter(lambda r: r.get('_status') == 'valid').count()\n",
    "rejected_errors = rejected_bronze_rdd.filter(lambda r: r.get('_status') == 'parse_error').count()\n",
    "\n",
    "print(f\"\\nRejected Loans Quality Check:\")\n",
    "print(f\"  Valid records: {rejected_valid:,}\")\n",
    "print(f\"  Parse errors: {rejected_errors:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Checks (Bronze Level - Using RDD Operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Accepted Loans - Bronze Quality Report ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 2,260,701\n",
      "Partitions: 50\n",
      "\n",
      "Null/Empty counts in key columns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loan_amnt: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  term: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  int_rate: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grade: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loan_status: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# MapReduce pattern: check for null/empty values in key columns using RDDs\n",
    "\n",
    "# Quality checks for ACCEPTED loans\n",
    "key_columns_accepted = ['loan_amnt', 'term', 'int_rate', 'grade', 'loan_status']\n",
    "\n",
    "print(\"=== Accepted Loans - Bronze Quality Report ===\")\n",
    "print(f\"Total records: {accepted_bronze_rdd.count():,}\")\n",
    "print(f\"Partitions: {accepted_bronze_rdd.getNumPartitions()}\")\n",
    "\n",
    "print(\"\\nNull/Empty counts in key columns:\")\n",
    "for col_name in key_columns_accepted:\n",
    "    # Use filter and count - MapReduce pattern\n",
    "    null_count = accepted_bronze_rdd.filter(\n",
    "        lambda r: r.get('_status') == 'valid' and (r.get(col_name) is None or r.get(col_name) == '')\n",
    "    ).count()\n",
    "    print(f\"  {col_name}: {null_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rejected Loans - Bronze Quality Report ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 27,648,741\n",
      "Partitions: 54\n",
      "\n",
      "Null/Empty counts in key columns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Amount Requested: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Application Date: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loan Title: 1,303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Risk_Score: 18,497,546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Debt-To-Income Ratio: 74\n",
      "\n",
      "Sample rejected loan records (first 2):\n",
      "\n",
      "Rejected Loan 1:\n",
      "  Amount Requested: 1000.0\n",
      "  Application Date: 2007-05-26\n",
      "  Loan Title: Wedding Covered but No Honeymoon\n",
      "  Risk_Score: 693.0\n",
      "  Debt-To-Income Ratio: 10%\n",
      "\n",
      "Rejected Loan 2:\n",
      "  Amount Requested: 1000.0\n",
      "  Application Date: 2007-05-26\n",
      "  Loan Title: Consolidating Debt\n",
      "  Risk_Score: 703.0\n",
      "  Debt-To-Income Ratio: 10%\n"
     ]
    }
   ],
   "source": [
    "# Quality checks for REJECTED loans\n",
    "# Rejected loans have different columns than accepted loans\n",
    "key_columns_rejected = ['Amount Requested', 'Application Date', 'Loan Title', 'Risk_Score', 'Debt-To-Income Ratio']\n",
    "\n",
    "print(\"\\n=== Rejected Loans - Bronze Quality Report ===\")\n",
    "print(f\"Total records: {rejected_bronze_rdd.count():,}\")\n",
    "print(f\"Partitions: {rejected_bronze_rdd.getNumPartitions()}\")\n",
    "\n",
    "print(\"\\nNull/Empty counts in key columns:\")\n",
    "for col_name in key_columns_rejected:\n",
    "    # Use filter and count - MapReduce pattern\n",
    "    null_count = rejected_bronze_rdd.filter(\n",
    "        lambda r: r.get('_status') == 'valid' and (r.get(col_name) is None or r.get(col_name) == '')\n",
    "    ).count()\n",
    "    print(f\"  {col_name}: {null_count:,}\")\n",
    "\n",
    "# Additional check: sample rejected loan records\n",
    "print(\"\\nSample rejected loan records (first 2):\")\n",
    "for i, record in enumerate(rejected_bronze_rdd.filter(lambda r: r.get('_status') == 'valid').take(2), 1):\n",
    "    print(f\"\\nRejected Loan {i}:\")\n",
    "    for field in key_columns_rejected:\n",
    "        if field in record:\n",
    "            print(f\"  {field}: {record[field]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save to Bronze Layer (Parquet Format)\n",
    "\n",
    "**Note:** We convert RDD to DataFrame only for efficient storage in Parquet format.\n",
    "This is acceptable as it's just for persistence, not for processing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Saving Accepted Loans to Bronze ===\n",
      "Removing existing directory: ../data/medallion/bronze/accepted_loans\n",
      "Directory cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accepted loans saved to: ../data/medallion/bronze/accepted_loans\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Helper function to clean directory before saving\n",
    "def clean_output_directory(path):\n",
    "    \"\"\"Remove existing directory to prevent duplicate files.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Removing existing directory: {path}\")\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Directory cleaned.\")\n",
    "\n",
    "# Save accepted loans to Bronze layer\n",
    "BRONZE_ACCEPTED_PATH = os.path.join(BRONZE_PATH, \"accepted_loans\")\n",
    "\n",
    "print(\"=== Saving Accepted Loans to Bronze ===\")\n",
    "clean_output_directory(BRONZE_ACCEPTED_PATH)\n",
    "\n",
    "# Convert RDD to DataFrame for Parquet storage\n",
    "accepted_bronze_df = spark.createDataFrame(accepted_bronze_rdd)\n",
    "\n",
    "accepted_bronze_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(BRONZE_ACCEPTED_PATH)\n",
    "\n",
    "print(f\"Accepted loans saved to: {BRONZE_ACCEPTED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saving Rejected Loans to Bronze ===\n",
      "Removing existing directory: ../data/medallion/bronze/rejected_loans\n",
      "Directory cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:===================================================>    (50 + 4) / 54]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rejected loans saved to: ../data/medallion/bronze/rejected_loans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save rejected loans to Bronze layer\n",
    "BRONZE_REJECTED_PATH = os.path.join(BRONZE_PATH, \"rejected_loans\")\n",
    "\n",
    "print(\"\\n=== Saving Rejected Loans to Bronze ===\")\n",
    "clean_output_directory(BRONZE_REJECTED_PATH)\n",
    "\n",
    "# Convert RDD to DataFrame for Parquet storage\n",
    "rejected_bronze_df = spark.createDataFrame(rejected_bronze_rdd)\n",
    "\n",
    "rejected_bronze_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(BRONZE_REJECTED_PATH)\n",
    "\n",
    "print(f\"Rejected loans saved to: {BRONZE_REJECTED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28K\n",
      "drwxr-xr-x 2 ubuntu ubuntu 12K Nov 27 13:25 accepted_loans\n",
      "drwxr-xr-x 2 ubuntu ubuntu 16K Nov 27 13:25 rejected_loans\n"
     ]
    }
   ],
   "source": [
    "# Verify the saved files\n",
    "!ls -lh {BRONZE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421M\t../data/medallion/bronze/accepted_loans\n",
      "295M\t../data/medallion/bronze/rejected_loans\n"
     ]
    }
   ],
   "source": [
    "# Check parquet file sizes (should be smaller than CSV due to compression)\n",
    "!du -sh {BRONZE_ACCEPTED_PATH}\n",
    "!du -sh {BRONZE_REJECTED_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verification - Read Back from Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted loans (verified): 2260701 rows\n",
      "Rejected loans (verified): 27648741 rows\n"
     ]
    }
   ],
   "source": [
    "# Verify we can read the data back\n",
    "accepted_verify = spark.read.parquet(BRONZE_ACCEPTED_PATH)\n",
    "rejected_verify = spark.read.parquet(BRONZE_REJECTED_PATH)\n",
    "\n",
    "print(f\"Accepted loans (verified): {accepted_verify.count()} rows\")\n",
    "print(f\"Rejected loans (verified): {rejected_verify.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------+--------------------+--------------------+\n",
      "|loan_amnt|grade|loan_status|_ingestion_timestamp|        _source_file|\n",
      "+---------+-----+-----------+--------------------+--------------------+\n",
      "|  11200.0|    C|    Current|1.7642499284482965E9|accepted_2007_to_...|\n",
      "|  12000.0|    C|    Current|1.7642499284483583E9|accepted_2007_to_...|\n",
      "|  11000.0|    C| Fully Paid|1.7642499284483843E9|accepted_2007_to_...|\n",
      "|  15000.0|    B|    Current|1.7642499284484448E9|accepted_2007_to_...|\n",
      "|   7000.0|    D|    Current|1.7642499284484687E9|accepted_2007_to_...|\n",
      "+---------+-----+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample with metadata columns\n",
    "accepted_verify.select(\n",
    "    'loan_amnt', 'grade', 'loan_status', \n",
    "    '_ingestion_timestamp', '_source_file'\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BRONZE LAYER INGESTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Data Source: Lending Club (2007-2018)\n",
      "\n",
      "Accepted Loans:\n",
      "  - Rows: 2,260,701\n",
      "  - Columns: 155\n",
      "  - Output: ../data/medallion/bronze/accepted_loans\n",
      "\n",
      "Rejected Loans:\n",
      "  - Rows: 27,648,741\n",
      "  - Columns: 13\n",
      "  - Output: ../data/medallion/bronze/rejected_loans\n",
      "\n",
      "Format: Parquet (columnar, compressed)\n",
      "Metadata added: _ingestion_timestamp, _source_file, _data_source\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate summary for the report\n",
    "print(\"=\" * 60)\n",
    "print(\"BRONZE LAYER INGESTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nData Source: Lending Club (2007-2018)\")\n",
    "print(f\"\\nAccepted Loans:\")\n",
    "print(f\"  - Rows: {accepted_verify.count():,}\")\n",
    "print(f\"  - Columns: {len(accepted_verify.columns)}\")\n",
    "print(f\"  - Output: {BRONZE_ACCEPTED_PATH}\")\n",
    "print(f\"\\nRejected Loans:\")\n",
    "print(f\"  - Rows: {rejected_verify.count():,}\")\n",
    "print(f\"  - Columns: {len(rejected_verify.columns)}\")\n",
    "print(f\"  - Output: {BRONZE_REJECTED_PATH}\")\n",
    "print(f\"\\nFormat: Parquet (columnar, compressed)\")\n",
    "print(f\"Metadata added: _ingestion_timestamp, _source_file, _data_source\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session (optional - keep running if continuing to Silver)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The Bronze layer is complete. The data is now stored in Parquet format with minimal transformation.\n",
    "\n",
    "**Continue to:** `02_silver_cleaning.ipynb` for data cleaning using MapReduce operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
