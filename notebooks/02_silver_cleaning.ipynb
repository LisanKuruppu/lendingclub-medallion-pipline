{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer - Data Cleaning with MapReduce\n",
    "\n",
    "## Lending Club Loan Data Pipeline\n",
    "\n",
    "**Use Case:** Predict loan default risk and analyze factors affecting loan approval\n",
    "\n",
    "This notebook implements the Silver (Cleaned) layer of the Medallion Architecture:\n",
    "- Clean and transform data using **RDD MapReduce operations** (no DataFrames/SQL)\n",
    "- Handle missing values, type conversions, and data standardization\n",
    "- Profile and tune performance\n",
    "\n",
    "**Important:** As per project requirements, this notebook uses basic MapReduce routines in Spark (map, filter, reduce, reduceByKey, etc.) - NOT DataFrames or SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/27 13:05:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/27 13:05:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Spark UI available at: http://spark-master:4041\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "\n",
    "# Initialize Spark Session with tuned configuration\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"LendingClub-Silver-Layer\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI available at: {sc.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze input path: ../data/medallion/bronze/\n",
      "Silver output path: ../data/medallion/silver/\n"
     ]
    }
   ],
   "source": [
    "# Define paths (matching Bronze notebook)\n",
    "BRONZE_PATH = \"../data/medallion/bronze/\"\n",
    "SILVER_PATH = \"../data/medallion/silver/\"\n",
    "\n",
    "BRONZE_ACCEPTED_PATH = os.path.join(BRONZE_PATH, \"accepted_loans\")\n",
    "BRONZE_REJECTED_PATH = os.path.join(BRONZE_PATH, \"rejected_loans\")\n",
    "\n",
    "# Create silver directory\n",
    "os.makedirs(SILVER_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Bronze input path: {BRONZE_PATH}\")\n",
    "print(f\"Silver output path: {SILVER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Bronze Data and Convert to RDD\n",
    "\n",
    "We load the Parquet data into a DataFrame first (for efficient reading), then immediately convert to RDD for MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted loans columns: 155\n",
      "Rejected loans columns: 13\n",
      "\n",
      "Accepted loans RDD partitions: 18\n",
      "Rejected loans RDD partitions: 18\n"
     ]
    }
   ],
   "source": [
    "# Load Bronze data and convert to RDD\n",
    "# We read parquet (efficient) then convert to RDD of Row objects\n",
    "\n",
    "accepted_df = spark.read.parquet(BRONZE_ACCEPTED_PATH)\n",
    "rejected_df = spark.read.parquet(BRONZE_REJECTED_PATH)\n",
    "\n",
    "# Get column names for reference\n",
    "accepted_columns = accepted_df.columns\n",
    "rejected_columns = rejected_df.columns\n",
    "\n",
    "print(f\"Accepted loans columns: {len(accepted_columns)}\")\n",
    "print(f\"Rejected loans columns: {len(rejected_columns)}\")\n",
    "\n",
    "# Convert to RDD - each element is a Row object\n",
    "accepted_rdd = accepted_df.rdd\n",
    "rejected_rdd = rejected_df.rdd\n",
    "\n",
    "print(f\"\\nAccepted loans RDD partitions: {accepted_rdd.getNumPartitions()}\")\n",
    "print(f\"Rejected loans RDD partitions: {rejected_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample row type: <class 'pyspark.sql.types.Row'>\n",
      "\n",
      "Sample row as dict (first 10 fields):\n",
      "  _data_source: lending_club (type: str)\n",
      "  _ingestion_timestamp: 1764244137.230629 (type: float)\n",
      "  _source_file: accepted_2007_to_2018Q4.csv (type: str)\n",
      "  _status: valid (type: str)\n",
      "  acc_now_delinq: 0.0 (type: str)\n",
      "  acc_open_past_24mths: 4.0 (type: str)\n",
      "  addr_state: NV (type: str)\n",
      "  all_util: 70.0 (type: str)\n",
      "  annual_inc: 84000.0 (type: str)\n",
      "  annual_inc_joint:  (type: str)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Examine sample row structure\n",
    "sample_row = accepted_rdd.first()\n",
    "print(\"Sample row type:\", type(sample_row))\n",
    "print(\"\\nSample row as dict (first 10 fields):\")\n",
    "sample_dict = sample_row.asDict()\n",
    "for i, (k, v) in enumerate(sample_dict.items()):\n",
    "    if i < 10:\n",
    "        print(f\"  {k}: {v} (type: {type(v).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Data Quality Issues\n",
    "\n",
    "Before cleaning, let's identify the specific issues in the data using MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 27 key columns for cleaning\n"
     ]
    }
   ],
   "source": [
    "# Select key columns for our use case (loan default prediction)\n",
    "# These are the columns we'll clean and use in the Gold layer\n",
    "\n",
    "KEY_COLUMNS = [\n",
    "    # Loan characteristics\n",
    "    'loan_amnt',        # Loan amount\n",
    "    'term',             # Loan term (36 or 60 months)\n",
    "    'int_rate',         # Interest rate\n",
    "    'installment',      # Monthly installment\n",
    "    'grade',            # Loan grade (A-G)\n",
    "    'sub_grade',        # Loan sub-grade (A1-G5)\n",
    "    \n",
    "    # Borrower information\n",
    "    'emp_length',       # Employment length\n",
    "    'home_ownership',   # Home ownership status\n",
    "    'annual_inc',       # Annual income\n",
    "    'verification_status',  # Income verification\n",
    "    \n",
    "    # Loan purpose and status\n",
    "    'purpose',          # Loan purpose\n",
    "    'loan_status',      # Current loan status (TARGET)\n",
    "    'issue_d',          # Issue date\n",
    "    \n",
    "    # Credit history\n",
    "    'dti',              # Debt-to-income ratio\n",
    "    'earliest_cr_line', # Earliest credit line\n",
    "    'open_acc',         # Open credit accounts\n",
    "    'pub_rec',          # Public records\n",
    "    'revol_bal',        # Revolving balance\n",
    "    'revol_util',       # Revolving utilization\n",
    "    'total_acc',        # Total accounts\n",
    "    'fico_range_low',   # FICO score low\n",
    "    'fico_range_high',  # FICO score high\n",
    "    \n",
    "    # Additional useful features\n",
    "    'addr_state',       # State\n",
    "    'delinq_2yrs',      # Delinquencies in 2 years\n",
    "    'inq_last_6mths',   # Inquiries in last 6 months\n",
    "    'mort_acc',         # Mortgage accounts\n",
    "    'pub_rec_bankruptcies'  # Bankruptcies\n",
    "]\n",
    "\n",
    "print(f\"Selected {len(KEY_COLUMNS)} key columns for cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2,260,701\n",
      "\n",
      "Null/Missing values per column:\n",
      "--------------------------------------------------\n",
      "emp_length               :    146,938 ( 6.50%)\n",
      "mort_acc                 :     53,470 ( 2.37%)\n",
      "open_acc                 :     44,417 ( 1.96%)\n",
      "pub_rec_bankruptcies     :     21,252 ( 0.94%)\n",
      "purpose                  :     21,010 ( 0.93%)\n",
      "pub_rec                  :     12,308 ( 0.54%)\n",
      "revol_bal                :      5,805 ( 0.26%)\n",
      "revol_util               :      4,931 ( 0.22%)\n",
      "total_acc                :      1,927 ( 0.09%)\n",
      "dti                      :      1,754 ( 0.08%)\n",
      "earliest_cr_line         :         57 ( 0.00%)\n",
      "delinq_2yrs              :         54 ( 0.00%)\n",
      "inq_last_6mths           :         53 ( 0.00%)\n",
      "addr_state               :         48 ( 0.00%)\n",
      "fico_range_low           :         48 ( 0.00%)\n",
      "home_ownership           :         42 ( 0.00%)\n",
      "fico_range_high          :         40 ( 0.00%)\n",
      "annual_inc               :         38 ( 0.00%)\n",
      "term                     :         33 ( 0.00%)\n",
      "int_rate                 :         33 ( 0.00%)\n",
      "grade                    :         33 ( 0.00%)\n",
      "verification_status      :         33 ( 0.00%)\n",
      "loan_amnt                :         33 ( 0.00%)\n",
      "loan_status              :         33 ( 0.00%)\n",
      "sub_grade                :         33 ( 0.00%)\n",
      "issue_d                  :         33 ( 0.00%)\n",
      "installment              :         33 ( 0.00%)\n",
      "CPU times: user 45.6 ms, sys: 8.27 ms, total: 53.9 ms\n",
      "Wall time: 40.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Profile null values using MapReduce\n",
    "# Map: emit (column_name, 1) if value is null/empty, else (column_name, 0)\n",
    "# Reduce: sum to get total nulls per column\n",
    "\n",
    "def count_nulls_mapper(row):\n",
    "    \"\"\"Map function to count nulls for each column\"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    results = []\n",
    "    for col in KEY_COLUMNS:\n",
    "        if col in row_dict:\n",
    "            value = row_dict[col]\n",
    "            # Check for null, None, empty string, or 'null' string\n",
    "            is_null = (value is None or \n",
    "                      value == '' or \n",
    "                      str(value).lower() == 'null' or\n",
    "                      str(value).lower() == 'nan')\n",
    "            results.append((col, 1 if is_null else 0))\n",
    "    return results\n",
    "\n",
    "# Use flatMap since mapper returns multiple pairs\n",
    "null_counts = accepted_rdd \\\n",
    "    .flatMap(count_nulls_mapper) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "total_rows = accepted_rdd.count()\n",
    "\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(\"\\nNull/Missing values per column:\")\n",
    "print(\"-\" * 50)\n",
    "for col, count in sorted(null_counts, key=lambda x: x[1], reverse=True):\n",
    "    pct = (count / total_rows) * 100\n",
    "    print(f\"{col:25s}: {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "term:\n",
      "   36 months                    :  1,609,754 (71.21%)\n",
      "   60 months                    :    650,914 (28.79%)\n",
      "                                :         33 ( 0.00%)\n",
      "\n",
      "grade:\n",
      "  B                             :    663,557 (29.35%)\n",
      "  C                             :    650,053 (28.75%)\n",
      "  A                             :    433,027 (19.15%)\n",
      "  D                             :    324,424 (14.35%)\n",
      "  E                             :    135,639 ( 6.00%)\n",
      "  F                             :     41,800 ( 1.85%)\n",
      "  G                             :     12,168 ( 0.54%)\n",
      "                                :         33 ( 0.00%)\n",
      "\n",
      "sub_grade:\n",
      "  C1                            :    145,903 ( 6.45%)\n",
      "  B5                            :    140,288 ( 6.21%)\n",
      "  B4                            :    139,793 ( 6.18%)\n",
      "  B3                            :    131,514 ( 5.82%)\n",
      "  C2                            :    131,116 ( 5.80%)\n",
      "  C3                            :    129,193 ( 5.71%)\n",
      "  C4                            :    127,115 ( 5.62%)\n",
      "  B2                            :    126,621 ( 5.60%)\n",
      "  B1                            :    125,341 ( 5.54%)\n",
      "  C5                            :    116,726 ( 5.16%)\n",
      "\n",
      "emp_length:\n",
      "  10+ years                     :    739,268 (32.70%)\n",
      "  2 years                       :    200,964 ( 8.89%)\n",
      "  < 1 year                      :    187,525 ( 8.29%)\n",
      "  3 years                       :    178,318 ( 7.89%)\n",
      "                                :    146,938 ( 6.50%)\n",
      "  1 year                        :    146,496 ( 6.48%)\n",
      "  5 years                       :    137,900 ( 6.10%)\n",
      "  4 years                       :    134,757 ( 5.96%)\n",
      "  6 years                       :    101,178 ( 4.48%)\n",
      "  7 years                       :     91,330 ( 4.04%)\n",
      "\n",
      "home_ownership:\n",
      "  MORTGAGE                      :  1,096,525 (48.50%)\n",
      "  RENT                          :    885,030 (39.15%)\n",
      "  OWN                           :    250,867 (11.10%)\n",
      "  10+ years                     :      8,419 ( 0.37%)\n",
      "  2 years                       :      2,623 ( 0.12%)\n",
      "  < 1 year                      :      2,396 ( 0.11%)\n",
      "  3 years                       :      2,345 ( 0.10%)\n",
      "  1 year                        :      1,855 ( 0.08%)\n",
      "  4 years                       :      1,786 ( 0.08%)\n",
      "  5 years                       :      1,739 ( 0.08%)\n",
      "\n",
      "verification_status:\n",
      "  Source Verified               :    876,650 (38.78%)\n",
      "  Not Verified                  :    736,142 (32.56%)\n",
      "  Verified                      :    620,847 (27.46%)\n",
      "  60000.0                       :        589 ( 0.03%)\n",
      "  120000.0                      :        553 ( 0.02%)\n",
      "  80000.0                       :        549 ( 0.02%)\n",
      "  100000.0                      :        546 ( 0.02%)\n",
      "  150000.0                      :        545 ( 0.02%)\n",
      "  90000.0                       :        492 ( 0.02%)\n",
      "  110000.0                      :        480 ( 0.02%)\n",
      "\n",
      "purpose:\n",
      "  debt_consolidation            :  1,242,803 (54.97%)\n",
      "  credit_card                   :    503,424 (22.27%)\n",
      "  home_improvement              :    145,928 ( 6.45%)\n",
      "  other                         :    135,833 ( 6.01%)\n",
      "  major_purchase                :     48,808 ( 2.16%)\n",
      "  medical                       :     26,861 ( 1.19%)\n",
      "  car                           :     22,983 ( 1.02%)\n",
      "  small_business                :     22,906 ( 1.01%)\n",
      "                                :     21,010 ( 0.93%)\n",
      "  vacation                      :     15,228 ( 0.67%)\n",
      "\n",
      "loan_status:\n",
      "  Fully Paid                    :  1,059,885 (46.88%)\n",
      "  Current                       :    871,037 (38.53%)\n",
      "  Charged Off                   :    266,014 (11.77%)\n",
      "  Late (31-120 days)            :     21,349 ( 0.94%)\n",
      "  In Grace Period               :      8,378 ( 0.37%)\n",
      "  Late (16-30 days)             :      4,328 ( 0.19%)\n",
      "  Does not meet the credit policy. Status:Fully Paid:      1,877 ( 0.08%)\n",
      "  Does not meet the credit policy. Status:Charged Off:        732 ( 0.03%)\n",
      "  Sep-2013                      :        566 ( 0.03%)\n",
      "  Aug-2013                      :        549 ( 0.02%)\n",
      "CPU times: user 49.4 ms, sys: 21.4 ms, total: 70.9 ms\n",
      "Wall time: 18.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Profile unique values for categorical columns using MapReduce\n",
    "\n",
    "categorical_cols = ['term', 'grade', 'sub_grade', 'emp_length', \n",
    "                    'home_ownership', 'verification_status', 'purpose', 'loan_status']\n",
    "\n",
    "def extract_categorical_mapper(row):\n",
    "    \"\"\"Extract categorical column values\"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    results = []\n",
    "    for col in categorical_cols:\n",
    "        if col in row_dict:\n",
    "            value = str(row_dict[col]) if row_dict[col] is not None else 'NULL'\n",
    "            results.append(((col, value), 1))\n",
    "    return results\n",
    "\n",
    "# Count occurrences of each value per column\n",
    "categorical_counts = accepted_rdd \\\n",
    "    .flatMap(extract_categorical_mapper) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "# Organize by column\n",
    "cat_summary = defaultdict(dict)\n",
    "for (col, value), count in categorical_counts:\n",
    "    cat_summary[col][value] = count\n",
    "\n",
    "# Display\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    values = cat_summary[col]\n",
    "    for value, count in sorted(values.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        pct = (count / total_rows) * 100\n",
    "        print(f\"  {value:30s}: {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Cleaning Functions\n",
    "\n",
    "Now we define the cleaning functions that will be applied via MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cleaning functions:\n",
      "  clean_percentage('13.99%') = 13.99\n",
      "  clean_term(' 36 months') = 36\n",
      "  clean_emp_length('10+ years') = 10\n",
      "  clean_emp_length('< 1 year') = 0\n",
      "  clean_emp_length('5 years') = 5\n",
      "  clean_date('Dec-2015') = 2015-12-01\n",
      "  clean_string('   ') = None (Expected: None)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning utility functions\n",
    "\n",
    "def clean_percentage(value):\n",
    "    \"\"\"Clean percentage values like '13.99%' -> 13.99\"\"\"\n",
    "    if value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        cleaned = str(value).replace('%', '').strip()\n",
    "        return float(cleaned)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"Clean currency values like '$1,234.56' -> 1234.56\"\"\"\n",
    "    if value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        cleaned = str(value).replace('$', '').replace(',', '').strip()\n",
    "        return float(cleaned)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_term(value):\n",
    "    \"\"\"Clean term values like ' 36 months' -> 36\"\"\"\n",
    "    if value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        # Extract numeric part\n",
    "        match = re.search(r'(\\d+)', str(value))\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_emp_length(value):\n",
    "    \"\"\"Clean employment length:\n",
    "    '10+ years' -> 10\n",
    "    '< 1 year' -> 0\n",
    "    '5 years' -> 5\n",
    "    'n/a' -> None\n",
    "    \"\"\"\n",
    "    if value is None or value == '' or str(value).lower() == 'n/a':\n",
    "        return None\n",
    "    value_str = str(value).lower()\n",
    "    if '10+' in value_str:\n",
    "        return 10\n",
    "    if '< 1' in value_str:\n",
    "        return 0\n",
    "    try:\n",
    "        match = re.search(r'(\\d+)', value_str)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_numeric(value):\n",
    "    \"\"\"Clean generic numeric values\"\"\"\n",
    "    if value is None or value == '' or str(value).lower() in ['null', 'nan', 'none']:\n",
    "        return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_date(value):\n",
    "    \"\"\"Clean date values like 'Dec-2015' -> '2015-12-01'\"\"\"\n",
    "    if value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        # Parse format like 'Dec-2015'\n",
    "        dt = datetime.strptime(str(value), '%b-%Y')\n",
    "        return dt.strftime('%Y-%m-%d')\n",
    "    except (ValueError, TypeError):\n",
    "        try:\n",
    "            # Try alternate format 'Dec-15'\n",
    "            dt = datetime.strptime(str(value), '%b-%y')\n",
    "            return dt.strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# --- FIXED FUNCTION ---\n",
    "def clean_string(value):\n",
    "    \"\"\"Clean string values - strip whitespace, standardize nulls\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    \n",
    "    # Strip whitespace FIRST\n",
    "    cleaned = str(value).strip()\n",
    "    \n",
    "    # Check if empty OR is a null-word\n",
    "    if not cleaned or cleaned.lower() in ['null', 'nan', 'none']:\n",
    "        return None\n",
    "        \n",
    "    return cleaned\n",
    "# ----------------------\n",
    "\n",
    "# Test cleaning functions\n",
    "print(\"Testing cleaning functions:\")\n",
    "print(f\"  clean_percentage('13.99%') = {clean_percentage('13.99%')}\")\n",
    "print(f\"  clean_term(' 36 months') = {clean_term(' 36 months')}\")\n",
    "print(f\"  clean_emp_length('10+ years') = {clean_emp_length('10+ years')}\")\n",
    "print(f\"  clean_emp_length('< 1 year') = {clean_emp_length('< 1 year')}\")\n",
    "print(f\"  clean_emp_length('5 years') = {clean_emp_length('5 years')}\")\n",
    "print(f\"  clean_date('Dec-2015') = {clean_date('Dec-2015')}\")\n",
    "print(f\"  clean_string('   ') = {clean_string('   ')} (Expected: None)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Cleaning via MapReduce\n",
    "\n",
    "This is the core cleaning step using **map** transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loan_status_binary(loan_status):\n",
    "    \"\"\"\n",
    "    Create binary target variable for loan default prediction.\n",
    "    1 = Default (bad loan)\n",
    "    0 = Paid/Current (good loan)\n",
    "    None = Unknown/Exclude\n",
    "    \"\"\"\n",
    "    if loan_status is None:\n",
    "        return None\n",
    "    \n",
    "    status = str(loan_status).lower().strip()\n",
    "    \n",
    "    # Bad loans (default = 1)\n",
    "    bad_statuses = ['charged off', 'default', 'late (31-120 days)', \n",
    "                   'late (16-30 days)', 'does not meet the credit policy. status:charged off']\n",
    "    \n",
    "    # Good loans (default = 0)\n",
    "    good_statuses = ['fully paid', 'current', \n",
    "                    'does not meet the credit policy. status:fully paid',\n",
    "                    'in grace period']\n",
    "    \n",
    "    if status in bad_statuses:\n",
    "        return 1\n",
    "    elif status in good_statuses:\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Exclude unclear statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned sample row:\n",
      "  loan_amnt: 11200.0\n",
      "  term: 60\n",
      "  int_rate: 15.04\n",
      "  installment: 266.69\n",
      "  grade: C\n",
      "  sub_grade: C4\n",
      "  emp_length: 5\n",
      "  home_ownership: MORTGAGE\n",
      "  annual_inc: 84000.0\n",
      "  verification_status: Not Verified\n",
      "  purpose: debt_consolidation\n",
      "  loan_status: Current\n",
      "  loan_status_binary: 0\n",
      "  issue_d: 2018-04-01\n",
      "  dti: 22.1\n",
      "  earliest_cr_line: 2002-02-01\n",
      "  open_acc: 13.0\n",
      "  pub_rec: 0.0\n",
      "  revol_bal: 33431.0\n",
      "  revol_util: 62.1\n",
      "  total_acc: 16.0\n",
      "  fico_range_low: 725.0\n",
      "  fico_range_high: 729.0\n",
      "  addr_state: NV\n",
      "  delinq_2yrs: 0.0\n",
      "  inq_last_6mths: 0.0\n",
      "  mort_acc: 1.0\n",
      "  pub_rec_bankruptcies: 0.0\n",
      "  fico_avg: 727.0\n",
      "  loan_to_income: 0.13333333333333333\n"
     ]
    }
   ],
   "source": [
    "def clean_accepted_loan_row(row):\n",
    "    \"\"\"\n",
    "    Main cleaning function applied to each row via map().\n",
    "    Returns a dictionary with cleaned values.\n",
    "    \"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    \n",
    "    cleaned = {\n",
    "        # Loan characteristics\n",
    "        'loan_amnt': clean_numeric(row_dict.get('loan_amnt')),\n",
    "        'term': clean_term(row_dict.get('term')),\n",
    "        'int_rate': clean_percentage(row_dict.get('int_rate')) if '%' in str(row_dict.get('int_rate', '')) else clean_numeric(row_dict.get('int_rate')),\n",
    "        'installment': clean_numeric(row_dict.get('installment')),\n",
    "        'grade': clean_string(row_dict.get('grade')),\n",
    "        'sub_grade': clean_string(row_dict.get('sub_grade')),\n",
    "        \n",
    "        # Borrower information\n",
    "        'emp_length': clean_emp_length(row_dict.get('emp_length')),\n",
    "        'home_ownership': clean_string(row_dict.get('home_ownership')),\n",
    "        'annual_inc': clean_numeric(row_dict.get('annual_inc')),\n",
    "        'verification_status': clean_string(row_dict.get('verification_status')),\n",
    "        \n",
    "        # Loan purpose and status\n",
    "        'purpose': clean_string(row_dict.get('purpose')),\n",
    "        'loan_status': clean_string(row_dict.get('loan_status')),\n",
    "        'loan_status_binary': create_loan_status_binary(row_dict.get('loan_status')),\n",
    "        'issue_d': clean_date(row_dict.get('issue_d')),\n",
    "        \n",
    "        # Credit history\n",
    "        'dti': clean_numeric(row_dict.get('dti')),\n",
    "        'earliest_cr_line': clean_date(row_dict.get('earliest_cr_line')),\n",
    "        'open_acc': clean_numeric(row_dict.get('open_acc')),\n",
    "        'pub_rec': clean_numeric(row_dict.get('pub_rec')),\n",
    "        'revol_bal': clean_numeric(row_dict.get('revol_bal')),\n",
    "        'revol_util': clean_percentage(row_dict.get('revol_util')) if '%' in str(row_dict.get('revol_util', '')) else clean_numeric(row_dict.get('revol_util')),\n",
    "        'total_acc': clean_numeric(row_dict.get('total_acc')),\n",
    "        'fico_range_low': clean_numeric(row_dict.get('fico_range_low')),\n",
    "        'fico_range_high': clean_numeric(row_dict.get('fico_range_high')),\n",
    "        \n",
    "        # Additional features\n",
    "        'addr_state': clean_string(row_dict.get('addr_state')),\n",
    "        'delinq_2yrs': clean_numeric(row_dict.get('delinq_2yrs')),\n",
    "        'inq_last_6mths': clean_numeric(row_dict.get('inq_last_6mths')),\n",
    "        'mort_acc': clean_numeric(row_dict.get('mort_acc')),\n",
    "        'pub_rec_bankruptcies': clean_numeric(row_dict.get('pub_rec_bankruptcies')),\n",
    "    }\n",
    "    \n",
    "    # Calculate derived features\n",
    "    # FICO average\n",
    "    if cleaned['fico_range_low'] and cleaned['fico_range_high']:\n",
    "        cleaned['fico_avg'] = (cleaned['fico_range_low'] + cleaned['fico_range_high']) / 2\n",
    "    else:\n",
    "        cleaned['fico_avg'] = None\n",
    "    \n",
    "    # Loan to income ratio\n",
    "    if cleaned['loan_amnt'] and cleaned['annual_inc'] and cleaned['annual_inc'] > 0:\n",
    "        cleaned['loan_to_income'] = cleaned['loan_amnt'] / cleaned['annual_inc']\n",
    "    else:\n",
    "        cleaned['loan_to_income'] = None\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Test on sample row\n",
    "test_cleaned = clean_accepted_loan_row(sample_row)\n",
    "print(\"Cleaned sample row:\")\n",
    "for k, v in test_cleaned.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying cleaning transformation via map()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned RDD count: 2,260,701\n",
      "CPU times: user 28.5 ms, sys: 7.82 ms, total: 36.3 ms\n",
      "Wall time: 22.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply cleaning transformation using map()\n",
    "# This is the core MapReduce cleaning operation\n",
    "\n",
    "print(\"Applying cleaning transformation via map()...\")\n",
    "\n",
    "cleaned_rdd = accepted_rdd.map(clean_accepted_loan_row)\n",
    "\n",
    "# Cache for reuse (important for performance)\n",
    "cleaned_rdd.cache()\n",
    "\n",
    "# Force evaluation and count\n",
    "cleaned_count = cleaned_rdd.count()\n",
    "print(f\"Cleaned RDD count: {cleaned_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Filter Invalid Records\n",
    "\n",
    "Using **filter** transformation to remove invalid records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_loan_record(row_dict):\n",
    "    \"\"\"\n",
    "    Filter function to identify valid records.\n",
    "    Returns True if record should be kept.\n",
    "    \"\"\"\n",
    "    # Must have loan amount\n",
    "    if row_dict.get('loan_amnt') is None or row_dict['loan_amnt'] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # Must have a determinable loan status for ML\n",
    "    if row_dict.get('loan_status_binary') is None:\n",
    "        return False\n",
    "    \n",
    "    # Must have interest rate\n",
    "    if row_dict.get('int_rate') is None or row_dict['int_rate'] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # Must have grade\n",
    "    if row_dict.get('grade') is None:\n",
    "        return False\n",
    "    \n",
    "    # Must have annual income (and it should be positive)\n",
    "    if row_dict.get('annual_inc') is None or row_dict['annual_inc'] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # Reasonable bounds check\n",
    "    # Interest rate should be between 0 and 50%\n",
    "    if row_dict['int_rate'] > 50:\n",
    "        return False\n",
    "    \n",
    "    # Annual income should be reasonable (< $10M)\n",
    "    if row_dict['annual_inc'] > 10000000:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying filter transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records after filtering: 2,231,965\n",
      "Records removed: 28,736 (1.27%)\n",
      "CPU times: user 15.4 ms, sys: 2.27 ms, total: 17.7 ms\n",
      "Wall time: 2.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply filter transformation\n",
    "print(\"Applying filter transformation...\")\n",
    "\n",
    "filtered_rdd = cleaned_rdd.filter(is_valid_loan_record)\n",
    "\n",
    "# Cache filtered RDD\n",
    "filtered_rdd.cache()\n",
    "\n",
    "filtered_count = filtered_rdd.count()\n",
    "removed_count = cleaned_count - filtered_count\n",
    "\n",
    "print(f\"Records after filtering: {filtered_count:,}\")\n",
    "print(f\"Records removed: {removed_count:,} ({removed_count/cleaned_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Profile Cleaned Data with MapReduce Aggregations\n",
    "\n",
    "Using **reduceByKey**, **aggregateByKey**, and other MapReduce operations to profile the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==================================>                     (11 + 7) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Status Distribution:\n",
      "  Paid/Current (0):  1,939,597 (86.90%)\n",
      "  Default (1):    292,368 (13.10%)\n",
      "CPU times: user 9.32 ms, sys: 6.46 ms, total: 15.8 ms\n",
      "Wall time: 940 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute statistics using MapReduce\n",
    "# Count by loan_status_binary using map + reduceByKey\n",
    "\n",
    "status_counts = filtered_rdd \\\n",
    "    .map(lambda x: (x['loan_status_binary'], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Loan Status Distribution:\")\n",
    "for status, count in sorted(status_counts):\n",
    "    label = \"Default\" if status == 1 else \"Paid/Current\"\n",
    "    pct = count / filtered_count * 100\n",
    "    print(f\"  {label} ({status}): {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==============================================>         (15 + 3) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Distribution:\n",
      "  Grade A:    426,139 (19.09%)\n",
      "  Grade B:    654,756 (29.34%)\n",
      "  Grade C:    642,765 (28.80%)\n",
      "  Grade D:    320,918 (14.38%)\n",
      "  Grade E:    134,102 ( 6.01%)\n",
      "  Grade F:     41,272 ( 1.85%)\n",
      "  Grade G:     12,013 ( 0.54%)\n",
      "CPU times: user 13.8 ms, sys: 5.03 ms, total: 18.8 ms\n",
      "Wall time: 995 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Grade distribution using MapReduce\n",
    "\n",
    "grade_counts = filtered_rdd \\\n",
    "    .map(lambda x: (x['grade'], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Grade Distribution:\")\n",
    "for grade, count in sorted(grade_counts):\n",
    "    pct = count / filtered_count * 100\n",
    "    print(f\"  Grade {grade}: {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Rate by Grade:\n",
      "--------------------------------------------------\n",
      "  Grade A:  3.68% default rate (15,691 / 426,139)\n",
      "  Grade B:  8.83% default rate (57,843 / 654,756)\n",
      "  Grade C: 14.62% default rate (93,971 / 642,765)\n",
      "  Grade D: 20.69% default rate (66,389 / 320,918)\n",
      "  Grade E: 28.65% default rate (38,417 / 134,102)\n",
      "  Grade F: 36.83% default rate (15,200 / 41,272)\n",
      "  Grade G: 40.43% default rate (4,857 / 12,013)\n",
      "CPU times: user 12.1 ms, sys: 2.68 ms, total: 14.8 ms\n",
      "Wall time: 1.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Default rate by grade using MapReduce\n",
    "# Map: (grade, (default_status, 1))\n",
    "# Reduce: (grade, (total_defaults, total_count))\n",
    "\n",
    "def grade_default_mapper(row):\n",
    "    grade = row['grade']\n",
    "    is_default = row['loan_status_binary']\n",
    "    return (grade, (is_default, 1))\n",
    "\n",
    "def grade_default_reducer(a, b):\n",
    "    return (a[0] + b[0], a[1] + b[1])\n",
    "\n",
    "default_by_grade = filtered_rdd \\\n",
    "    .map(grade_default_mapper) \\\n",
    "    .reduceByKey(grade_default_reducer) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Default Rate by Grade:\")\n",
    "print(\"-\" * 50)\n",
    "for grade, (defaults, total) in sorted(default_by_grade):\n",
    "    rate = defaults / total * 100\n",
    "    print(f\"  Grade {grade}: {rate:5.2f}% default rate ({defaults:,} / {total:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Amount Statistics:\n",
      "  Min: $500.00\n",
      "  Max: $40,000.00\n",
      "  Avg: $15,010.02\n",
      "  Count: 2,231,965\n",
      "CPU times: user 5.56 ms, sys: 2.58 ms, total: 8.14 ms\n",
      "Wall time: 699 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute basic statistics for numeric columns using aggregate\n",
    "# Using aggregate to compute min, max, sum, count in one pass\n",
    "\n",
    "def stats_seq_op(acc, row):\n",
    "    \"\"\"Sequential operation: update accumulator with new row\"\"\"\n",
    "    value = row.get('loan_amnt')\n",
    "    if value is not None:\n",
    "        min_val = min(acc[0], value) if acc[0] is not None else value\n",
    "        max_val = max(acc[1], value) if acc[1] is not None else value\n",
    "        sum_val = acc[2] + value\n",
    "        count_val = acc[3] + 1\n",
    "        return (min_val, max_val, sum_val, count_val)\n",
    "    return acc\n",
    "\n",
    "def stats_comb_op(acc1, acc2):\n",
    "    \"\"\"Combine operation: merge two accumulators\"\"\"\n",
    "    min_val = min(acc1[0], acc2[0]) if acc1[0] is not None and acc2[0] is not None else acc1[0] or acc2[0]\n",
    "    max_val = max(acc1[1], acc2[1]) if acc1[1] is not None and acc2[1] is not None else acc1[1] or acc2[1]\n",
    "    sum_val = acc1[2] + acc2[2]\n",
    "    count_val = acc1[3] + acc2[3]\n",
    "    return (min_val, max_val, sum_val, count_val)\n",
    "\n",
    "# Initial accumulator: (min, max, sum, count)\n",
    "zero_value = (None, None, 0.0, 0)\n",
    "\n",
    "loan_amnt_stats = filtered_rdd.aggregate(zero_value, stats_seq_op, stats_comb_op)\n",
    "\n",
    "min_val, max_val, sum_val, count_val = loan_amnt_stats\n",
    "avg_val = sum_val / count_val if count_val > 0 else 0\n",
    "\n",
    "print(\"Loan Amount Statistics:\")\n",
    "print(f\"  Min: ${min_val:,.2f}\")\n",
    "print(f\"  Max: ${max_val:,.2f}\")\n",
    "print(f\"  Avg: ${avg_val:,.2f}\")\n",
    "print(f\"  Count: {count_val:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Column Statistics:\n",
      "======================================================================\n",
      "Column                   Min          Max          Avg        Count\n",
      "----------------------------------------------------------------------\n",
      "loan_amnt             500.00    40,000.00    15,010.02    2,231,965\n",
      "int_rate                5.31        30.99        13.10    2,231,965\n",
      "annual_inc              0.36 9,930,475.00    77,601.46    2,231,965\n",
      "dti                    -1.00     2,800.00        18.91    2,194,254\n",
      "fico_avg                0.72     2,271.12       700.27    2,194,763\n",
      "CPU times: user 8.43 ms, sys: 4.87 ms, total: 13.3 ms\n",
      "Wall time: 1.06 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute stats for multiple columns using a single pass with aggregate\n",
    "\n",
    "numeric_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_avg']\n",
    "\n",
    "def multi_stats_seq_op(acc, row):\n",
    "    \"\"\"Update stats for all numeric columns\"\"\"\n",
    "    result = dict(acc)\n",
    "    for col in numeric_cols:\n",
    "        value = row.get(col)\n",
    "        if value is not None:\n",
    "            stats = result[col]\n",
    "            min_val = min(stats[0], value) if stats[0] is not None else value\n",
    "            max_val = max(stats[1], value) if stats[1] is not None else value\n",
    "            sum_val = stats[2] + value\n",
    "            count_val = stats[3] + 1\n",
    "            result[col] = (min_val, max_val, sum_val, count_val)\n",
    "    return result\n",
    "\n",
    "def multi_stats_comb_op(acc1, acc2):\n",
    "    \"\"\"Combine stats from two accumulators\"\"\"\n",
    "    result = {}\n",
    "    for col in numeric_cols:\n",
    "        s1, s2 = acc1[col], acc2[col]\n",
    "        min_val = min(s1[0], s2[0]) if s1[0] is not None and s2[0] is not None else s1[0] or s2[0]\n",
    "        max_val = max(s1[1], s2[1]) if s1[1] is not None and s2[1] is not None else s1[1] or s2[1]\n",
    "        sum_val = s1[2] + s2[2]\n",
    "        count_val = s1[3] + s2[3]\n",
    "        result[col] = (min_val, max_val, sum_val, count_val)\n",
    "    return result\n",
    "\n",
    "zero_multi = {col: (None, None, 0.0, 0) for col in numeric_cols}\n",
    "\n",
    "multi_stats = filtered_rdd.aggregate(zero_multi, multi_stats_seq_op, multi_stats_comb_op)\n",
    "\n",
    "print(\"Numeric Column Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Column':<15} {'Min':>12} {'Max':>12} {'Avg':>12} {'Count':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for col in numeric_cols:\n",
    "    min_v, max_v, sum_v, count_v = multi_stats[col]\n",
    "    avg_v = sum_v / count_v if count_v > 0 else 0\n",
    "    print(f\"{col:<15} {min_v:>12,.2f} {max_v:>12,.2f} {avg_v:>12,.2f} {count_v:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Profiling and Tuning\n",
    "\n",
    "Let's profile and tune our MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Partition Analysis ===\n",
      "Original RDD partitions: 18\n",
      "Cleaned RDD partitions: 18\n",
      "Filtered RDD partitions: 18\n",
      "\n",
      "--- Detailed Partition Analysis (using glom) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:========================================>               (13 + 5) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 18\n",
      "Total records: 2,231,965\n",
      "Min partition size: 43,842\n",
      "Max partition size: 143,010\n",
      "Avg partition size: 123,998\n",
      "Skew Ratio (Max/Min): 3.26x\n",
      "⚠️ Data is skewed! Some partitions are much larger than others.\n",
      "\n",
      "Raw partition sizes: [81597, 82794, 131248, 132436, 131974, 133400, 133577, 132672, 133310, 131546, 136336, 134565, 135174, 142867, 143010, 139136, 132481, 43842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"=== Partition Analysis ===\")\n",
    "print(f\"Original RDD partitions: {accepted_rdd.getNumPartitions()}\")\n",
    "print(f\"Cleaned RDD partitions: {cleaned_rdd.getNumPartitions()}\")\n",
    "print(f\"Filtered RDD partitions: {filtered_rdd.getNumPartitions()}\")\n",
    "\n",
    "print(\"\\n--- Detailed Partition Analysis (using glom) ---\")\n",
    "\n",
    "# Use glom() to get the list of rows per partition\n",
    "# Then map(len) to just calculate the size of that list\n",
    "# This runs entirely on the workers, sending only 18 integers back to the driver\n",
    "partition_sizes = filtered_rdd.glom().map(len).collect()\n",
    "\n",
    "print(f\"Total partitions: {len(partition_sizes)}\")\n",
    "print(f\"Total records: {sum(partition_sizes):,}\")\n",
    "print(f\"Min partition size: {min(partition_sizes):,}\")\n",
    "print(f\"Max partition size: {max(partition_sizes):,}\")\n",
    "print(f\"Avg partition size: {sum(partition_sizes)/len(partition_sizes):,.0f}\")\n",
    "\n",
    "# Check for Skew\n",
    "skew_ratio = max(partition_sizes) / min(partition_sizes) if min(partition_sizes) > 0 else 0\n",
    "print(f\"Skew Ratio (Max/Min): {skew_ratio:.2f}x\")\n",
    "\n",
    "if skew_ratio > 1.5:\n",
    "    print(\"⚠️ Data is skewed! Some partitions are much larger than others.\")\n",
    "else:\n",
    "    print(\"✅ Data is well balanced across partitions.\")\n",
    "\n",
    "print(f\"\\nRaw partition sizes: {partition_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions:  2 | Time: 4.147s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions:  4 | Time: 3.213s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions:  8 | Time: 2.182s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 16 | Time: 2.168s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 32 | Time: 2.476s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:==================================================>     (58 + 6) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 64 | Time: 2.860s\n",
      "\n",
      "Optimal partition count: 16 (time: 2.168s)\n",
      "CPU times: user 88.1 ms, sys: 40.4 ms, total: 128 ms\n",
      "Wall time: 17.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Experiment with different partition counts\n",
    "# Find optimal partition count based on data size\n",
    "\n",
    "# For ~30M records, let's try different values\n",
    "\n",
    "import time\n",
    "\n",
    "partition_tests = [2, 4, 8, 16, 32, 64]\n",
    "results = []\n",
    "\n",
    "for num_partitions in partition_tests:\n",
    "    # Repartition\n",
    "    test_rdd = filtered_rdd.repartition(num_partitions)\n",
    "    \n",
    "    # Time a simple operation\n",
    "    start = time.time()\n",
    "    \n",
    "    # Perform a MapReduce operation\n",
    "    _ = test_rdd \\\n",
    "        .map(lambda x: (x['grade'], x['loan_amnt'])) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .collect()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    results.append((num_partitions, elapsed))\n",
    "    print(f\"Partitions: {num_partitions:2d} | Time: {elapsed:.3f}s\")\n",
    "\n",
    "# Find best\n",
    "best = min(results, key=lambda x: x[1])\n",
    "print(f\"\\nOptimal partition count: {best[0]} (time: {best[1]:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RDD partitions: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Apply optimal partitioning\n",
    "OPTIMAL_PARTITIONS = 16  # Adjust based on results above\n",
    "\n",
    "optimized_rdd = filtered_rdd.repartition(OPTIMAL_PARTITIONS)\n",
    "optimized_rdd.cache()\n",
    "\n",
    "# Force evaluation\n",
    "_ = optimized_rdd.count()\n",
    "\n",
    "print(f\"Optimized RDD partitions: {optimized_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Process Rejected Loans (Simplified)\n",
    "\n",
    "Apply similar cleaning to rejected loans dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected loans columns:\n",
      "  Amount Requested: 1000.0\n",
      "  Application Date: 2007-05-26\n",
      "  Debt-To-Income Ratio: 10%\n",
      "  Employment Length: 4 years\n",
      "  Loan Title: Wedding Covered but No Honeymoon\n",
      "  Policy Code: 0.0\n",
      "  Risk_Score: 693.0\n",
      "  State: NM\n",
      "  Zip Code: 481xx\n",
      "  _data_source: lending_club\n",
      "  _ingestion_timestamp: 1764244146.2610295\n",
      "  _source_file: rejected_2007_to_2018Q4.csv\n",
      "  _status: valid\n"
     ]
    }
   ],
   "source": [
    "# Check rejected loans structure\n",
    "rejected_sample = rejected_rdd.first()\n",
    "print(\"Rejected loans columns:\")\n",
    "for k, v in rejected_sample.asDict().items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rejected_loan_row(row):\n",
    "    \"\"\"\n",
    "    Clean rejected loan records using map().\n",
    "    \"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    \n",
    "    cleaned = {\n",
    "        'Amount Requested': clean_numeric(row_dict.get('Amount Requested')),\n",
    "        'Application Date': clean_string(row_dict.get('Application Date')),\n",
    "        'Loan Title': clean_string(row_dict.get('Loan Title')),\n",
    "        'Risk_Score': clean_numeric(row_dict.get('Risk_Score')),\n",
    "        'Debt-To-Income Ratio': clean_percentage(row_dict.get('Debt-To-Income Ratio')),\n",
    "        'Zip Code': clean_string(row_dict.get('Zip Code')),\n",
    "        'State': clean_string(row_dict.get('State')),\n",
    "        'Employment Length': clean_emp_length(row_dict.get('Employment Length')),\n",
    "        'Policy Code': clean_string(row_dict.get('Policy Code')),\n",
    "    }\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def is_valid_rejected_record(row_dict):\n",
    "    \"\"\"\n",
    "    Filter valid rejected records with comprehensive data quality checks.\n",
    "    Same criteria as Project.ipynb:\n",
    "    - loan_amount: not null, > 0, <= 100,000\n",
    "    - debt_to_income_ratio: not null, >= 0, <= 100\n",
    "    - risk_score: null OR (>= 300 AND <= 850)\n",
    "    \"\"\"\n",
    "    # Check loan amount\n",
    "    loan_amt = row_dict.get('Amount Requested')\n",
    "    if loan_amt is None or loan_amt <= 0 or loan_amt > 100000:\n",
    "        return False\n",
    "    \n",
    "    # Check debt-to-income ratio (must not be null and within range)\n",
    "    dti = row_dict.get('Debt-To-Income Ratio')\n",
    "    if dti is None or dti < 0 or dti > 100:\n",
    "        return False\n",
    "    \n",
    "    # Check risk score (can be null, but if present must be in valid range)\n",
    "    risk_score = row_dict.get('Risk_Score')\n",
    "    if risk_score is not None and (risk_score < 300 or risk_score > 850):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning rejected loans...\n",
      "Applying data quality filters:\n",
      "  1. Basic cleaning (percentages, strings, employment length)\n",
      "  2. Unrealistic value filtering (amount, DTI, risk_score)\n",
      "  3. Date validation (2007-2018)\n",
      "  4. Deduplication\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial records: 27,648,741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After validation filters: 25,528,260 (removed 2,120,481)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After date validation: 25,528,260 (removed 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 13:14:19 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.18.110: Command exited with code 137\n",
      "25/11/27 13:15:10 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.18.110: Command exited with code 137\n",
      "[Stage 44:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: 20,116,218 (removed 5,412,042)\n",
      "\n",
      "✅ Total removed: 7,532,523 (27.24%)\n",
      "CPU times: user 166 ms, sys: 5.59 s, total: 5.75 s\n",
      "Wall time: 4min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean rejected loans using MapReduce with comprehensive data quality checks\n",
    "\n",
    "print(\"Cleaning rejected loans...\")\n",
    "print(\"Applying data quality filters:\")\n",
    "print(\"  1. Basic cleaning (percentages, strings, employment length)\")\n",
    "print(\"  2. Unrealistic value filtering (amount, DTI, risk_score)\")\n",
    "print(\"  3. Date validation (2007-2018)\")\n",
    "print(\"  4. Deduplication\")\n",
    "\n",
    "# Step 1: Clean using map()\n",
    "cleaned_rejected_rdd = rejected_rdd.map(clean_rejected_loan_row)\n",
    "\n",
    "# Step 2: Filter unrealistic values using filter()\n",
    "initial_count = cleaned_rejected_rdd.count()\n",
    "print(f\"\\nInitial records: {initial_count:,}\")\n",
    "\n",
    "valid_rejected_rdd = cleaned_rejected_rdd.filter(is_valid_rejected_record)\n",
    "after_validation_count = valid_rejected_rdd.count()\n",
    "removed_validation = initial_count - after_validation_count\n",
    "print(f\"After validation filters: {after_validation_count:,} (removed {removed_validation:,})\")\n",
    "\n",
    "# Step 3: Date validation using filter()\n",
    "def is_valid_date_range(row_dict):\n",
    "    \"\"\"Check if application date is within 2007-2018\"\"\"\n",
    "    app_date = row_dict.get('Application Date')\n",
    "    if app_date is None:\n",
    "        return True  # Allow null dates\n",
    "    \n",
    "    try:\n",
    "        # Parse date format: \"yyyy-MM-dd\"\n",
    "        from datetime import datetime\n",
    "        date_obj = datetime.strptime(app_date, '%Y-%m-%d')\n",
    "        year = date_obj.year\n",
    "        return 2007 <= year <= 2018\n",
    "    except:\n",
    "        return False  # Invalid date format\n",
    "\n",
    "date_filtered_rdd = valid_rejected_rdd.filter(is_valid_date_range)\n",
    "after_date_count = date_filtered_rdd.count()\n",
    "removed_dates = after_validation_count - after_date_count\n",
    "print(f\"After date validation: {after_date_count:,} (removed {removed_dates:,})\")\n",
    "\n",
    "# Step 4: Deduplication using MapReduce pattern\n",
    "# Create composite key from key fields, then use reduceByKey to keep first occurrence\n",
    "def create_dedup_key(row_dict):\n",
    "    \"\"\"Create composite key for deduplication\"\"\"\n",
    "    key = (\n",
    "        row_dict.get('Amount Requested'),\n",
    "        row_dict.get('Loan Title'),\n",
    "        row_dict.get('Application Date'),\n",
    "        row_dict.get('State'),\n",
    "        row_dict.get('Zip Code')\n",
    "    )\n",
    "    return (key, row_dict)\n",
    "\n",
    "# Map to (key, row), reduceByKey to keep first, then extract row\n",
    "deduped_rdd = date_filtered_rdd \\\n",
    "    .map(create_dedup_key) \\\n",
    "    .reduceByKey(lambda a, b: a) \\\n",
    "    .map(lambda x: x[1])\n",
    "\n",
    "deduped_rdd.cache()\n",
    "\n",
    "final_rejected_count = deduped_rdd.count()\n",
    "removed_duplicates = after_date_count - final_rejected_count\n",
    "print(f\"After deduplication: {final_rejected_count:,} (removed {removed_duplicates:,})\")\n",
    "\n",
    "print(f\"\\n✅ Total removed: {initial_count - final_rejected_count:,} ({100*(initial_count - final_rejected_count)/initial_count:.2f}%)\")\n",
    "\n",
    "# Update reference to use deduplicated RDD\n",
    "cleaned_rejected_rdd = deduped_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save to Silver Layer\n",
    "\n",
    "Convert cleaned RDDs to DataFrames and save as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for accepted loans Silver layer\n",
    "accepted_silver_schema = StructType([\n",
    "    StructField(\"loan_amnt\", FloatType(), True),\n",
    "    StructField(\"term\", IntegerType(), True),\n",
    "    StructField(\"int_rate\", FloatType(), True),\n",
    "    StructField(\"installment\", FloatType(), True),\n",
    "    StructField(\"grade\", StringType(), True),\n",
    "    StructField(\"sub_grade\", StringType(), True),\n",
    "    StructField(\"emp_length\", IntegerType(), True),\n",
    "    StructField(\"home_ownership\", StringType(), True),\n",
    "    StructField(\"annual_inc\", FloatType(), True),\n",
    "    StructField(\"verification_status\", StringType(), True),\n",
    "    StructField(\"purpose\", StringType(), True),\n",
    "    StructField(\"loan_status\", StringType(), True),\n",
    "    StructField(\"loan_status_binary\", IntegerType(), True),\n",
    "    StructField(\"issue_d\", StringType(), True),\n",
    "    StructField(\"dti\", FloatType(), True),\n",
    "    StructField(\"earliest_cr_line\", StringType(), True),\n",
    "    StructField(\"open_acc\", FloatType(), True),\n",
    "    StructField(\"pub_rec\", FloatType(), True),\n",
    "    StructField(\"revol_bal\", FloatType(), True),\n",
    "    StructField(\"revol_util\", FloatType(), True),\n",
    "    StructField(\"total_acc\", FloatType(), True),\n",
    "    StructField(\"fico_range_low\", FloatType(), True),\n",
    "    StructField(\"fico_range_high\", FloatType(), True),\n",
    "    StructField(\"addr_state\", StringType(), True),\n",
    "    StructField(\"delinq_2yrs\", FloatType(), True),\n",
    "    StructField(\"inq_last_6mths\", FloatType(), True),\n",
    "    StructField(\"mort_acc\", FloatType(), True),\n",
    "    StructField(\"pub_rec_bankruptcies\", FloatType(), True),\n",
    "    StructField(\"fico_avg\", FloatType(), True),\n",
    "    StructField(\"loan_to_income\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver DataFrame created with 30 columns\n",
      "root\n",
      " |-- loan_amnt: float (nullable = true)\n",
      " |-- term: integer (nullable = true)\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- installment: float (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_length: integer (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: float (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_status_binary: integer (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- open_acc: float (nullable = true)\n",
      " |-- pub_rec: float (nullable = true)\n",
      " |-- revol_bal: float (nullable = true)\n",
      " |-- revol_util: float (nullable = true)\n",
      " |-- total_acc: float (nullable = true)\n",
      " |-- fico_range_low: float (nullable = true)\n",
      " |-- fico_range_high: float (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- delinq_2yrs: float (nullable = true)\n",
      " |-- inq_last_6mths: float (nullable = true)\n",
      " |-- mort_acc: float (nullable = true)\n",
      " |-- pub_rec_bankruptcies: float (nullable = true)\n",
      " |-- fico_avg: float (nullable = true)\n",
      " |-- loan_to_income: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert RDD of dicts to RDD of Rows\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def dict_to_row(d):\n",
    "    \"\"\"Convert dictionary to Row with proper type handling\"\"\"\n",
    "    return Row(\n",
    "        loan_amnt=float(d['loan_amnt']) if d['loan_amnt'] is not None else None,\n",
    "        term=int(d['term']) if d['term'] is not None else None,\n",
    "        int_rate=float(d['int_rate']) if d['int_rate'] is not None else None,\n",
    "        installment=float(d['installment']) if d['installment'] is not None else None,\n",
    "        grade=d['grade'],\n",
    "        sub_grade=d['sub_grade'],\n",
    "        emp_length=int(d['emp_length']) if d['emp_length'] is not None else None,\n",
    "        home_ownership=d['home_ownership'],\n",
    "        annual_inc=float(d['annual_inc']) if d['annual_inc'] is not None else None,\n",
    "        verification_status=d['verification_status'],\n",
    "        purpose=d['purpose'],\n",
    "        loan_status=d['loan_status'],\n",
    "        loan_status_binary=int(d['loan_status_binary']) if d['loan_status_binary'] is not None else None,\n",
    "        issue_d=d['issue_d'],\n",
    "        dti=float(d['dti']) if d['dti'] is not None else None,\n",
    "        earliest_cr_line=d['earliest_cr_line'],\n",
    "        open_acc=float(d['open_acc']) if d['open_acc'] is not None else None,\n",
    "        pub_rec=float(d['pub_rec']) if d['pub_rec'] is not None else None,\n",
    "        revol_bal=float(d['revol_bal']) if d['revol_bal'] is not None else None,\n",
    "        revol_util=float(d['revol_util']) if d['revol_util'] is not None else None,\n",
    "        total_acc=float(d['total_acc']) if d['total_acc'] is not None else None,\n",
    "        fico_range_low=float(d['fico_range_low']) if d['fico_range_low'] is not None else None,\n",
    "        fico_range_high=float(d['fico_range_high']) if d['fico_range_high'] is not None else None,\n",
    "        addr_state=d['addr_state'],\n",
    "        delinq_2yrs=float(d['delinq_2yrs']) if d['delinq_2yrs'] is not None else None,\n",
    "        inq_last_6mths=float(d['inq_last_6mths']) if d['inq_last_6mths'] is not None else None,\n",
    "        mort_acc=float(d['mort_acc']) if d['mort_acc'] is not None else None,\n",
    "        pub_rec_bankruptcies=float(d['pub_rec_bankruptcies']) if d['pub_rec_bankruptcies'] is not None else None,\n",
    "        fico_avg=float(d['fico_avg']) if d['fico_avg'] is not None else None,\n",
    "        loan_to_income=float(d['loan_to_income']) if d['loan_to_income'] is not None else None,\n",
    "    )\n",
    "\n",
    "# Transform using map\n",
    "row_rdd = optimized_rdd.map(dict_to_row)\n",
    "\n",
    "# Create DataFrame\n",
    "accepted_silver_df = spark.createDataFrame(row_rdd, schema=accepted_silver_schema)\n",
    "\n",
    "print(f\"Silver DataFrame created with {len(accepted_silver_df.columns)} columns\")\n",
    "accepted_silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----------+------------------+--------+--------------+\n",
      "|loan_amnt|int_rate|grade|annual_inc|loan_status_binary|fico_avg|loan_to_income|\n",
      "+---------+--------+-----+----------+------------------+--------+--------------+\n",
      "|5800.0   |13.11   |B    |20000.0   |1                 |672.0   |0.29          |\n",
      "|8000.0   |14.33   |C    |85000.0   |0                 |687.0   |0.09411765    |\n",
      "|19000.0  |18.75   |D    |55000.0   |1                 |697.0   |0.34545454    |\n",
      "|6000.0   |8.9     |A    |132000.0  |0                 |702.0   |0.045454547   |\n",
      "|11875.0  |7.62    |A    |200000.0  |0                 |737.0   |0.059375      |\n",
      "|10000.0  |13.11   |B    |58000.0   |0                 |NULL    |0.1724138     |\n",
      "|19950.0  |19.05   |D    |62400.0   |0                 |672.0   |0.31971154    |\n",
      "|1500.0   |14.33   |C    |57000.0   |0                 |682.0   |0.02631579    |\n",
      "|24000.0  |13.11   |B    |85000.0   |0                 |NULL    |0.28235295    |\n",
      "|27300.0  |10.16   |B    |70000.0   |0                 |NULL    |0.39          |\n",
      "+---------+--------+-----+----------+------------------+--------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview silver data\n",
    "accepted_silver_df.select(\n",
    "    'loan_amnt', 'int_rate', 'grade', 'annual_inc', \n",
    "    'loan_status_binary', 'fico_avg', 'loan_to_income'\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Saving Accepted Loans to Silver ===\n",
      "Removing existing directory: ../data/medallion/silver/accepted_loans\n",
      "Directory cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Silver data saved to: ../data/medallion/silver/accepted_loans\n",
      "CPU times: user 15 ms, sys: 10.2 ms, total: 25.2 ms\n",
      "Wall time: 9.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import shutil\n",
    "\n",
    "# Helper function to clean directory before saving\n",
    "def clean_output_directory(path):\n",
    "    \"\"\"Remove existing directory to prevent duplicate files.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Removing existing directory: {path}\")\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Directory cleaned.\")\n",
    "\n",
    "# Save to Silver layer\n",
    "SILVER_ACCEPTED_PATH = os.path.join(SILVER_PATH, \"accepted_loans\")\n",
    "\n",
    "print(\"=== Saving Accepted Loans to Silver ===\")\n",
    "clean_output_directory(SILVER_ACCEPTED_PATH)\n",
    "\n",
    "accepted_silver_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(SILVER_ACCEPTED_PATH)\n",
    "\n",
    "print(f\"✅ Silver data saved to: {SILVER_ACCEPTED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saving Rejected Loans to Silver ===\n",
      "Removing existing directory: ../data/medallion/silver/rejected_loans\n",
      "Directory cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=================================================>      (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rejected Silver data saved to: ../data/medallion/silver/rejected_loans\n",
      "CPU times: user 19.1 ms, sys: 6.59 ms, total: 25.7 ms\n",
      "Wall time: 15.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save rejected loans Silver layer\n",
    "SILVER_REJECTED_PATH = os.path.join(SILVER_PATH, \"rejected_loans\")\n",
    "\n",
    "print(\"\\n=== Saving Rejected Loans to Silver ===\")\n",
    "clean_output_directory(SILVER_REJECTED_PATH)\n",
    "\n",
    "rejected_silver_schema = StructType([\n",
    "    StructField(\"amount_requested\", FloatType(), True),\n",
    "    StructField(\"application_date\", StringType(), True),\n",
    "    StructField(\"loan_title\", StringType(), True),\n",
    "    StructField(\"risk_score\", FloatType(), True),\n",
    "    StructField(\"dti\", FloatType(), True),\n",
    "    StructField(\"zip_code\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"emp_length\", IntegerType(), True),\n",
    "    StructField(\"policy_code\", StringType(), True),\n",
    "])\n",
    "\n",
    "def rejected_dict_to_row(d):\n",
    "    return Row(\n",
    "        amount_requested=float(d['Amount Requested']) if d['Amount Requested'] is not None else None,\n",
    "        application_date=d['Application Date'],\n",
    "        loan_title=d['Loan Title'],\n",
    "        risk_score=float(d['Risk_Score']) if d['Risk_Score'] is not None else None,\n",
    "        dti=float(d['Debt-To-Income Ratio']) if d['Debt-To-Income Ratio'] is not None else None,\n",
    "        zip_code=d['Zip Code'],\n",
    "        state=d['State'],\n",
    "        emp_length=int(d['Employment Length']) if d['Employment Length'] is not None else None,\n",
    "        policy_code=d['Policy Code'],\n",
    "    )\n",
    "\n",
    "rejected_row_rdd = cleaned_rejected_rdd.map(rejected_dict_to_row)\n",
    "rejected_silver_df = spark.createDataFrame(rejected_row_rdd, schema=rejected_silver_schema)\n",
    "\n",
    "rejected_silver_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(SILVER_REJECTED_PATH)\n",
    "\n",
    "print(f\"✅ Rejected Silver data saved to: {SILVER_REJECTED_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verification and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Silver Layer Verification ===\n",
      "Accepted loans: 2,231,965 rows, 30 columns\n",
      "Rejected loans: 20,116,218 rows, 9 columns\n"
     ]
    }
   ],
   "source": [
    "# Verify saved data\n",
    "verify_accepted = spark.read.parquet(SILVER_ACCEPTED_PATH)\n",
    "verify_rejected = spark.read.parquet(SILVER_REJECTED_PATH)\n",
    "\n",
    "print(\"=== Silver Layer Verification ===\")\n",
    "print(f\"Accepted loans: {verify_accepted.count():,} rows, {len(verify_accepted.columns)} columns\")\n",
    "print(f\"Rejected loans: {verify_rejected.count():,} rows, {len(verify_rejected.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70M\t../data/medallion/silver/accepted_loans\n",
      "161M\t../data/medallion/silver/rejected_loans\n"
     ]
    }
   ],
   "source": [
    "# Check storage sizes\n",
    "!du -sh {SILVER_ACCEPTED_PATH}\n",
    "!du -sh {SILVER_REJECTED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SILVER LAYER CLEANING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "MapReduce Operations Used:\n",
      "  - map(): Data cleaning and transformation\n",
      "  - filter(): Remove invalid records\n",
      "  - flatMap(): Profile null values across columns\n",
      "  - reduceByKey(): Aggregate statistics & Deduplication\n",
      "  - aggregate(): Compute min/max/sum/count in single pass\n",
      "  - glom(): Analyze partition distribution\n",
      "  - repartition(): Optimize partitioning\n",
      "\n",
      "Data Quality Improvements:\n",
      "  - Cleaned 27 key columns\n",
      "  - Standardized date formats\n",
      "  - Converted percentages and currency values\n",
      "  - Sanitized empty strings/whitespace (Null handling)\n",
      "  - Created binary target variable for ML\n",
      "  - Added derived features (fico_avg, loan_to_income)\n",
      "\n",
      "Accepted Loans (Cleaned):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Bronze Input: 2,260,701\n",
      "  - Silver Output: 2,231,965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Records removed: 28,736\n",
      "\n",
      "Rejected Loans (Cleaned & Deduplicated):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Bronze Input: 27,648,741\n",
      "  - Silver Output: 20,116,218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Records removed: 7,532,523\n",
      "\n",
      "Output Paths:\n",
      "  - ../data/medallion/silver/accepted_loans\n",
      "  - ../data/medallion/silver/rejected_loans\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Final summary statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"SILVER LAYER CLEANING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nMapReduce Operations Used:\")\n",
    "print(\"  - map(): Data cleaning and transformation\")\n",
    "print(\"  - filter(): Remove invalid records\")\n",
    "print(\"  - flatMap(): Profile null values across columns\")\n",
    "print(\"  - reduceByKey(): Aggregate statistics & Deduplication\") # Updated\n",
    "print(\"  - aggregate(): Compute min/max/sum/count in single pass\")\n",
    "print(\"  - glom(): Analyze partition distribution\") # Confirmed usage\n",
    "print(\"  - repartition(): Optimize partitioning\")\n",
    "\n",
    "print(\"\\nData Quality Improvements:\")\n",
    "print(f\"  - Cleaned {len(KEY_COLUMNS)} key columns\")\n",
    "print(f\"  - Standardized date formats\")\n",
    "print(f\"  - Converted percentages and currency values\")\n",
    "print(f\"  - Sanitized empty strings/whitespace (Null handling)\") # Added this (Critical Fix)\n",
    "print(f\"  - Created binary target variable for ML\")\n",
    "print(f\"  - Added derived features (fico_avg, loan_to_income)\")\n",
    "\n",
    "print(\"\\nAccepted Loans (Cleaned):\")\n",
    "print(f\"  - Bronze Input: {accepted_rdd.count():,}\")\n",
    "print(f\"  - Silver Output: {verify_accepted.count():,}\")\n",
    "print(f\"  - Records removed: {accepted_rdd.count() - verify_accepted.count():,}\")\n",
    "\n",
    "print(\"\\nRejected Loans (Cleaned & Deduplicated):\")\n",
    "print(f\"  - Bronze Input: {rejected_rdd.count():,}\")\n",
    "print(f\"  - Silver Output: {verify_rejected.count():,}\")\n",
    "print(f\"  - Records removed: {rejected_rdd.count() - verify_rejected.count():,}\")\n",
    "\n",
    "print(\"\\nOutput Paths:\")\n",
    "print(f\"  - {SILVER_ACCEPTED_PATH}\")\n",
    "print(f\"  - {SILVER_REJECTED_PATH}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached RDDs unpersisted.\n"
     ]
    }
   ],
   "source": [
    "# Unpersist cached RDDs to free memory\n",
    "cleaned_rdd.unpersist()\n",
    "filtered_rdd.unpersist()\n",
    "optimized_rdd.unpersist()\n",
    "cleaned_rejected_rdd.unpersist()\n",
    "\n",
    "print(\"Cached RDDs unpersisted.\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The Silver layer is complete. The data is now cleaned and ready for analytics and ML.\n",
    "\n",
    "**Continue to:** `03_gold_serving.ipynb` for data serving using DataFrames, SQL, and MLlib."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
