{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer - Data Cleaning with MapReduce\n",
    "\n",
    "## Lending Club Loan Data Pipeline\n",
    "\n",
    "**Use Case:** Predict loan default risk and analyze factors affecting loan approval\n",
    "\n",
    "This notebook implements the Silver (Cleaned) layer of the Medallion Architecture:\n",
    "- Clean and transform data using **RDD MapReduce operations** (no DataFrames/SQL)\n",
    "- Handle missing values, type conversions, and data standardization\n",
    "- Profile and tune performance\n",
    "\n",
    "**Important:** As per project requirements, this notebook uses basic MapReduce routines in Spark (map, filter, reduce, reduceByKey, etc.) - NOT DataFrames or SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/25 18:52:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "\n",
    "# Initialize Spark Session with tuned configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LendingClub-Silver-Layer\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze input path: ../data/medallion/bronze/\n",
      "Silver output path: ../data/medallion/silver/\n"
     ]
    }
   ],
   "source": [
    "# Define paths (matching Bronze notebook)\n",
    "BRONZE_PATH = \"../data/medallion/bronze/\"\n",
    "SILVER_PATH = \"../data/medallion/silver/\"\n",
    "\n",
    "BRONZE_ACCEPTED_PATH = os.path.join(BRONZE_PATH, \"accepted_loans\")\n",
    "BRONZE_REJECTED_PATH = os.path.join(BRONZE_PATH, \"rejected_loans\")\n",
    "\n",
    "# Create silver directory\n",
    "os.makedirs(SILVER_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Bronze input path: {BRONZE_PATH}\")\n",
    "print(f\"Silver output path: {SILVER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Bronze Data and Convert to RDD\n",
    "\n",
    "We load the Parquet data into a DataFrame first (for efficient reading), then immediately convert to RDD for MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted loans columns: 154\n",
      "Rejected loans columns: 12\n",
      "\n",
      "Accepted loans RDD partitions: 4\n",
      "Rejected loans RDD partitions: 4\n"
     ]
    }
   ],
   "source": [
    "# Load Bronze data and convert to RDD\n",
    "# We read parquet (efficient) then convert to RDD of Row objects\n",
    "\n",
    "accepted_df = spark.read.parquet(BRONZE_ACCEPTED_PATH)\n",
    "rejected_df = spark.read.parquet(BRONZE_REJECTED_PATH)\n",
    "\n",
    "# Get column names for reference\n",
    "accepted_columns = accepted_df.columns\n",
    "rejected_columns = rejected_df.columns\n",
    "\n",
    "print(f\"Accepted loans columns: {len(accepted_columns)}\")\n",
    "print(f\"Rejected loans columns: {len(rejected_columns)}\")\n",
    "\n",
    "# Convert to RDD - each element is a Row object\n",
    "accepted_rdd = accepted_df.rdd\n",
    "rejected_rdd = rejected_df.rdd\n",
    "\n",
    "print(f\"\\nAccepted loans RDD partitions: {accepted_rdd.getNumPartitions()}\")\n",
    "print(f\"Rejected loans RDD partitions: {rejected_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample row type: <class 'pyspark.sql.types.Row'>\n",
      "\n",
      "Sample row as dict (first 10 fields):\n",
      "  id: 68407277 (type: str)\n",
      "  member_id: None (type: NoneType)\n",
      "  loan_amnt: 3600.0 (type: str)\n",
      "  funded_amnt: 3600.0 (type: str)\n",
      "  funded_amnt_inv: 3600.0 (type: str)\n",
      "  term:  36 months (type: str)\n",
      "  int_rate: 13.99 (type: str)\n",
      "  installment: 123.03 (type: str)\n",
      "  grade: C (type: str)\n",
      "  sub_grade: C4 (type: str)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Examine sample row structure\n",
    "sample_row = accepted_rdd.first()\n",
    "print(\"Sample row type:\", type(sample_row))\n",
    "print(\"\\nSample row as dict (first 10 fields):\")\n",
    "sample_dict = sample_row.asDict()\n",
    "for i, (k, v) in enumerate(sample_dict.items()):\n",
    "    if i < 10:\n",
    "        print(f\"  {k}: {v} (type: {type(v).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Data Quality Issues\n",
    "\n",
    "Before cleaning, let's identify the specific issues in the data using MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 27 key columns for cleaning\n"
     ]
    }
   ],
   "source": [
    "# Select key columns for our use case (loan default prediction)\n",
    "# These are the columns we'll clean and use in the Gold layer\n",
    "\n",
    "KEY_COLUMNS = [\n",
    "    # Loan characteristics\n",
    "    'loan_amnt',        # Loan amount\n",
    "    'term',             # Loan term (36 or 60 months)\n",
    "    'int_rate',         # Interest rate\n",
    "    'installment',      # Monthly installment\n",
    "    'grade',            # Loan grade (A-G)\n",
    "    'sub_grade',        # Loan sub-grade (A1-G5)\n",
    "    \n",
    "    # Borrower information\n",
    "    'emp_length',       # Employment length\n",
    "    'home_ownership',   # Home ownership status\n",
    "    'annual_inc',       # Annual income\n",
    "    'verification_status',  # Income verification\n",
    "    \n",
    "    # Loan purpose and status\n",
    "    'purpose',          # Loan purpose\n",
    "    'loan_status',      # Current loan status (TARGET)\n",
    "    'issue_d',          # Issue date\n",
    "    \n",
    "    # Credit history\n",
    "    'dti',              # Debt-to-income ratio\n",
    "    'earliest_cr_line', # Earliest credit line\n",
    "    'open_acc',         # Open credit accounts\n",
    "    'pub_rec',          # Public records\n",
    "    'revol_bal',        # Revolving balance\n",
    "    'revol_util',       # Revolving utilization\n",
    "    'total_acc',        # Total accounts\n",
    "    'fico_range_low',   # FICO score low\n",
    "    'fico_range_high',  # FICO score high\n",
    "    \n",
    "    # Additional useful features\n",
    "    'addr_state',       # State\n",
    "    'delinq_2yrs',      # Delinquencies in 2 years\n",
    "    'inq_last_6mths',   # Inquiries in last 6 months\n",
    "    'mort_acc',         # Mortgage accounts\n",
    "    'pub_rec_bankruptcies'  # Bankruptcies\n",
    "]\n",
    "\n",
    "print(f\"Selected {len(KEY_COLUMNS)} key columns for cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2,260,701\n",
      "\n",
      "Null/Missing values per column:\n",
      "--------------------------------------------------\n",
      "emp_length               :    146,940 ( 6.50%)\n",
      "mort_acc                 :     50,063 ( 2.21%)\n",
      "revol_util               :      1,835 ( 0.08%)\n",
      "dti                      :      1,744 ( 0.08%)\n",
      "pub_rec_bankruptcies     :      1,398 ( 0.06%)\n",
      "inq_last_6mths           :         63 ( 0.00%)\n",
      "open_acc                 :         62 ( 0.00%)\n",
      "earliest_cr_line         :         62 ( 0.00%)\n",
      "total_acc                :         62 ( 0.00%)\n",
      "pub_rec                  :         62 ( 0.00%)\n",
      "delinq_2yrs              :         62 ( 0.00%)\n",
      "annual_inc               :         37 ( 0.00%)\n",
      "grade                    :         33 ( 0.00%)\n",
      "loan_status              :         33 ( 0.00%)\n",
      "issue_d                  :         33 ( 0.00%)\n",
      "fico_range_high          :         33 ( 0.00%)\n",
      "addr_state               :         33 ( 0.00%)\n",
      "installment              :         33 ( 0.00%)\n",
      "purpose                  :         33 ( 0.00%)\n",
      "term                     :         33 ( 0.00%)\n",
      "int_rate                 :         33 ( 0.00%)\n",
      "sub_grade                :         33 ( 0.00%)\n",
      "home_ownership           :         33 ( 0.00%)\n",
      "verification_status      :         33 ( 0.00%)\n",
      "revol_bal                :         33 ( 0.00%)\n",
      "fico_range_low           :         33 ( 0.00%)\n",
      "loan_amnt                :         33 ( 0.00%)\n",
      "CPU times: user 32.9 ms, sys: 16.5 ms, total: 49.4 ms\n",
      "Wall time: 1min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Profile null values using MapReduce\n",
    "# Map: emit (column_name, 1) if value is null/empty, else (column_name, 0)\n",
    "# Reduce: sum to get total nulls per column\n",
    "\n",
    "def count_nulls_mapper(row):\n",
    "    \"\"\"Map function to count nulls for each column\"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    results = []\n",
    "    for col in KEY_COLUMNS:\n",
    "        if col in row_dict:\n",
    "            value = row_dict[col]\n",
    "            # Check for null, None, empty string, or 'null' string\n",
    "            is_null = (value is None or \n",
    "                      value == '' or \n",
    "                      str(value).lower() == 'null' or\n",
    "                      str(value).lower() == 'nan')\n",
    "            results.append((col, 1 if is_null else 0))\n",
    "    return results\n",
    "\n",
    "# Use flatMap since mapper returns multiple pairs\n",
    "null_counts = accepted_rdd \\\n",
    "    .flatMap(count_nulls_mapper) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "total_rows = accepted_rdd.count()\n",
    "\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(\"\\nNull/Missing values per column:\")\n",
    "print(\"-\" * 50)\n",
    "for col, count in sorted(null_counts, key=lambda x: x[1], reverse=True):\n",
    "    pct = (count / total_rows) * 100\n",
    "    print(f\"{col:25s}: {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "term:\n",
      "   36 months                    :  1,609,754 (71.21%)\n",
      "   60 months                    :    650,914 (28.79%)\n",
      "  NULL                          :         33 ( 0.00%)\n",
      "\n",
      "grade:\n",
      "  B                             :    663,557 (29.35%)\n",
      "  C                             :    650,053 (28.75%)\n",
      "  A                             :    433,027 (19.15%)\n",
      "  D                             :    324,424 (14.35%)\n",
      "  E                             :    135,639 ( 6.00%)\n",
      "  F                             :     41,800 ( 1.85%)\n",
      "  G                             :     12,168 ( 0.54%)\n",
      "  NULL                          :         33 ( 0.00%)\n",
      "\n",
      "sub_grade:\n",
      "  C1                            :    145,903 ( 6.45%)\n",
      "  B5                            :    140,288 ( 6.21%)\n",
      "  B4                            :    139,793 ( 6.18%)\n",
      "  B3                            :    131,514 ( 5.82%)\n",
      "  C2                            :    131,116 ( 5.80%)\n",
      "  C3                            :    129,193 ( 5.71%)\n",
      "  C4                            :    127,115 ( 5.62%)\n",
      "  B2                            :    126,621 ( 5.60%)\n",
      "  B1                            :    125,341 ( 5.54%)\n",
      "  C5                            :    116,726 ( 5.16%)\n",
      "\n",
      "emp_length:\n",
      "  10+ years                     :    748,005 (33.09%)\n",
      "  2 years                       :    203,677 ( 9.01%)\n",
      "  < 1 year                      :    189,988 ( 8.40%)\n",
      "  3 years                       :    180,753 ( 8.00%)\n",
      "  1 year                        :    148,403 ( 6.56%)\n",
      "  NULL                          :    146,940 ( 6.50%)\n",
      "  5 years                       :    139,698 ( 6.18%)\n",
      "  4 years                       :    136,605 ( 6.04%)\n",
      "  6 years                       :    102,628 ( 4.54%)\n",
      "  7 years                       :     92,695 ( 4.10%)\n",
      "\n",
      "home_ownership:\n",
      "  MORTGAGE                      :  1,111,450 (49.16%)\n",
      "  RENT                          :    894,929 (39.59%)\n",
      "  OWN                           :    253,057 (11.19%)\n",
      "  ANY                           :        996 ( 0.04%)\n",
      "  OTHER                         :        182 ( 0.01%)\n",
      "  NONE                          :         54 ( 0.00%)\n",
      "  NULL                          :         33 ( 0.00%)\n",
      "\n",
      "verification_status:\n",
      "  Source Verified               :    886,231 (39.20%)\n",
      "  Not Verified                  :    744,806 (32.95%)\n",
      "  Verified                      :    629,631 (27.85%)\n",
      "  NULL                          :         33 ( 0.00%)\n",
      "\n",
      "purpose:\n",
      "  debt_consolidation            :  1,277,877 (56.53%)\n",
      "  credit_card                   :    516,971 (22.87%)\n",
      "  home_improvement              :    150,457 ( 6.66%)\n",
      "  other                         :    139,440 ( 6.17%)\n",
      "  major_purchase                :     50,445 ( 2.23%)\n",
      "  medical                       :     27,488 ( 1.22%)\n",
      "  small_business                :     24,689 ( 1.09%)\n",
      "  car                           :     24,013 ( 1.06%)\n",
      "  vacation                      :     15,525 ( 0.69%)\n",
      "  moving                        :     15,403 ( 0.68%)\n",
      "\n",
      "loan_status:\n",
      "  Fully Paid                    :  1,076,751 (47.63%)\n",
      "  Current                       :    878,317 (38.85%)\n",
      "  Charged Off                   :    268,559 (11.88%)\n",
      "  Late (31-120 days)            :     21,467 ( 0.95%)\n",
      "  In Grace Period               :      8,436 ( 0.37%)\n",
      "  Late (16-30 days)             :      4,349 ( 0.19%)\n",
      "  Does not meet the credit policy. Status:Fully Paid:      1,988 ( 0.09%)\n",
      "  Does not meet the credit policy. Status:Charged Off:        761 ( 0.03%)\n",
      "  Default                       :         40 ( 0.00%)\n",
      "  NULL                          :         33 ( 0.00%)\n",
      "CPU times: user 19.1 ms, sys: 12.3 ms, total: 31.4 ms\n",
      "Wall time: 53.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Profile unique values for categorical columns using MapReduce\n",
    "\n",
    "categorical_cols = ['term', 'grade', 'sub_grade', 'emp_length', \n",
    "                    'home_ownership', 'verification_status', 'purpose', 'loan_status']\n",
    "\n",
    "def extract_categorical_mapper(row):\n",
    "    \"\"\"Extract categorical column values\"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    results = []\n",
    "    for col in categorical_cols:\n",
    "        if col in row_dict:\n",
    "            value = str(row_dict[col]) if row_dict[col] is not None else 'NULL'\n",
    "            results.append(((col, value), 1))\n",
    "    return results\n",
    "\n",
    "# Count occurrences of each value per column\n",
    "categorical_counts = accepted_rdd \\\n",
    "    .flatMap(extract_categorical_mapper) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "# Organize by column\n",
    "cat_summary = defaultdict(dict)\n",
    "for (col, value), count in categorical_counts:\n",
    "    cat_summary[col][value] = count\n",
    "\n",
    "# Display\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    values = cat_summary[col]\n",
    "    for value, count in sorted(values.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        pct = (count / total_rows) * 100\n",
    "        print(f\"  {value:30s}: {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Cleaning Functions\n",
    "\n",
    "Now we define the cleaning functions that will be applied via MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cleaning functions:\n",
      "  clean_percentage('13.99%') = 13.99\n",
      "  clean_term(' 36 months') = 36\n",
      "  clean_emp_length('10+ years') = 10\n",
      "  clean_emp_length('< 1 year') = 0\n",
      "  clean_emp_length('5 years') = 5\n",
      "  clean_date('Dec-2015') = 2015-12-01\n"
     ]
    }
   ],
   "source": [
    "# Cleaning utility functions\n",
    "\n",
    "def clean_percentage(value):\n",
    "    \"\"\"Clean percentage values like '13.99%' -> 13.99\"\"\"\n",
    "    if value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        cleaned = str(value).replace('%', '').strip()\n",
    "        return float(cleaned)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"Clean currency values like '$1,234.56' -> 1234.56\"\"\"\n",
    "    if value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        cleaned = str(value).replace('$', '').replace(',', '').strip()\n",
    "        return float(cleaned)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_term(value):\n",
    "    \"\"\"Clean term values like ' 36 months' -> 36\"\"\"\n",
    "    if value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        # Extract numeric part\n",
    "        match = re.search(r'(\\d+)', str(value))\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_emp_length(value):\n",
    "    \"\"\"Clean employment length:\n",
    "    '10+ years' -> 10\n",
    "    '< 1 year' -> 0\n",
    "    '5 years' -> 5\n",
    "    'n/a' -> None\n",
    "    \"\"\"\n",
    "    if value is None or value == '' or str(value).lower() == 'n/a':\n",
    "        return None\n",
    "    value_str = str(value).lower()\n",
    "    if '10+' in value_str:\n",
    "        return 10\n",
    "    if '< 1' in value_str:\n",
    "        return 0\n",
    "    try:\n",
    "        match = re.search(r'(\\d+)', value_str)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_numeric(value):\n",
    "    \"\"\"Clean generic numeric values\"\"\"\n",
    "    if value is None or value == '' or str(value).lower() in ['null', 'nan', 'none']:\n",
    "        return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_date(value):\n",
    "    \"\"\"Clean date values like 'Dec-2015' -> '2015-12-01'\"\"\"\n",
    "    if value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        # Parse format like 'Dec-2015'\n",
    "        dt = datetime.strptime(str(value), '%b-%Y')\n",
    "        return dt.strftime('%Y-%m-%d')\n",
    "    except (ValueError, TypeError):\n",
    "        try:\n",
    "            # Try alternate format 'Dec-15'\n",
    "            dt = datetime.strptime(str(value), '%b-%y')\n",
    "            return dt.strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def clean_string(value):\n",
    "    \"\"\"Clean string values - strip whitespace, standardize nulls\"\"\"\n",
    "    if value is None or str(value).lower() in ['null', 'nan', 'none', '']:\n",
    "        return None\n",
    "    return str(value).strip()\n",
    "\n",
    "# Test cleaning functions\n",
    "print(\"Testing cleaning functions:\")\n",
    "print(f\"  clean_percentage('13.99%') = {clean_percentage('13.99%')}\")\n",
    "print(f\"  clean_term(' 36 months') = {clean_term(' 36 months')}\")\n",
    "print(f\"  clean_emp_length('10+ years') = {clean_emp_length('10+ years')}\")\n",
    "print(f\"  clean_emp_length('< 1 year') = {clean_emp_length('< 1 year')}\")\n",
    "print(f\"  clean_emp_length('5 years') = {clean_emp_length('5 years')}\")\n",
    "print(f\"  clean_date('Dec-2015') = {clean_date('Dec-2015')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Cleaning via MapReduce\n",
    "\n",
    "This is the core cleaning step using **map** transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loan_status_binary(loan_status):\n",
    "    \"\"\"\n",
    "    Create binary target variable for loan default prediction.\n",
    "    1 = Default (bad loan)\n",
    "    0 = Paid/Current (good loan)\n",
    "    None = Unknown/Exclude\n",
    "    \"\"\"\n",
    "    if loan_status is None:\n",
    "        return None\n",
    "    \n",
    "    status = str(loan_status).lower().strip()\n",
    "    \n",
    "    # Bad loans (default = 1)\n",
    "    bad_statuses = ['charged off', 'default', 'late (31-120 days)', \n",
    "                   'late (16-30 days)', 'does not meet the credit policy. status:charged off']\n",
    "    \n",
    "    # Good loans (default = 0)\n",
    "    good_statuses = ['fully paid', 'current', \n",
    "                    'does not meet the credit policy. status:fully paid',\n",
    "                    'in grace period']\n",
    "    \n",
    "    if status in bad_statuses:\n",
    "        return 1\n",
    "    elif status in good_statuses:\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Exclude unclear statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned sample row:\n",
      "  loan_amnt: 3600.0\n",
      "  term: 36\n",
      "  int_rate: 13.99\n",
      "  installment: 123.03\n",
      "  grade: C\n",
      "  sub_grade: C4\n",
      "  emp_length: 10\n",
      "  home_ownership: MORTGAGE\n",
      "  annual_inc: 55000.0\n",
      "  verification_status: Not Verified\n",
      "  purpose: debt_consolidation\n",
      "  loan_status: Fully Paid\n",
      "  loan_status_binary: 0\n",
      "  issue_d: 2015-12-01\n",
      "  dti: 5.91\n",
      "  earliest_cr_line: 2003-08-01\n",
      "  open_acc: 7.0\n",
      "  pub_rec: 0.0\n",
      "  revol_bal: 2765.0\n",
      "  revol_util: 29.7\n",
      "  total_acc: 13.0\n",
      "  fico_range_low: 675.0\n",
      "  fico_range_high: 679.0\n",
      "  addr_state: PA\n",
      "  delinq_2yrs: 0.0\n",
      "  inq_last_6mths: 1.0\n",
      "  mort_acc: 1.0\n",
      "  pub_rec_bankruptcies: 0.0\n",
      "  fico_avg: 677.0\n",
      "  loan_to_income: 0.06545454545454546\n"
     ]
    }
   ],
   "source": [
    "def clean_accepted_loan_row(row):\n",
    "    \"\"\"\n",
    "    Main cleaning function applied to each row via map().\n",
    "    Returns a dictionary with cleaned values.\n",
    "    \"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    \n",
    "    cleaned = {\n",
    "        # Loan characteristics\n",
    "        'loan_amnt': clean_numeric(row_dict.get('loan_amnt')),\n",
    "        'term': clean_term(row_dict.get('term')),\n",
    "        'int_rate': clean_percentage(row_dict.get('int_rate')) if '%' in str(row_dict.get('int_rate', '')) else clean_numeric(row_dict.get('int_rate')),\n",
    "        'installment': clean_numeric(row_dict.get('installment')),\n",
    "        'grade': clean_string(row_dict.get('grade')),\n",
    "        'sub_grade': clean_string(row_dict.get('sub_grade')),\n",
    "        \n",
    "        # Borrower information\n",
    "        'emp_length': clean_emp_length(row_dict.get('emp_length')),\n",
    "        'home_ownership': clean_string(row_dict.get('home_ownership')),\n",
    "        'annual_inc': clean_numeric(row_dict.get('annual_inc')),\n",
    "        'verification_status': clean_string(row_dict.get('verification_status')),\n",
    "        \n",
    "        # Loan purpose and status\n",
    "        'purpose': clean_string(row_dict.get('purpose')),\n",
    "        'loan_status': clean_string(row_dict.get('loan_status')),\n",
    "        'loan_status_binary': create_loan_status_binary(row_dict.get('loan_status')),\n",
    "        'issue_d': clean_date(row_dict.get('issue_d')),\n",
    "        \n",
    "        # Credit history\n",
    "        'dti': clean_numeric(row_dict.get('dti')),\n",
    "        'earliest_cr_line': clean_date(row_dict.get('earliest_cr_line')),\n",
    "        'open_acc': clean_numeric(row_dict.get('open_acc')),\n",
    "        'pub_rec': clean_numeric(row_dict.get('pub_rec')),\n",
    "        'revol_bal': clean_numeric(row_dict.get('revol_bal')),\n",
    "        'revol_util': clean_percentage(row_dict.get('revol_util')) if '%' in str(row_dict.get('revol_util', '')) else clean_numeric(row_dict.get('revol_util')),\n",
    "        'total_acc': clean_numeric(row_dict.get('total_acc')),\n",
    "        'fico_range_low': clean_numeric(row_dict.get('fico_range_low')),\n",
    "        'fico_range_high': clean_numeric(row_dict.get('fico_range_high')),\n",
    "        \n",
    "        # Additional features\n",
    "        'addr_state': clean_string(row_dict.get('addr_state')),\n",
    "        'delinq_2yrs': clean_numeric(row_dict.get('delinq_2yrs')),\n",
    "        'inq_last_6mths': clean_numeric(row_dict.get('inq_last_6mths')),\n",
    "        'mort_acc': clean_numeric(row_dict.get('mort_acc')),\n",
    "        'pub_rec_bankruptcies': clean_numeric(row_dict.get('pub_rec_bankruptcies')),\n",
    "    }\n",
    "    \n",
    "    # Calculate derived features\n",
    "    # FICO average\n",
    "    if cleaned['fico_range_low'] and cleaned['fico_range_high']:\n",
    "        cleaned['fico_avg'] = (cleaned['fico_range_low'] + cleaned['fico_range_high']) / 2\n",
    "    else:\n",
    "        cleaned['fico_avg'] = None\n",
    "    \n",
    "    # Loan to income ratio\n",
    "    if cleaned['loan_amnt'] and cleaned['annual_inc'] and cleaned['annual_inc'] > 0:\n",
    "        cleaned['loan_to_income'] = cleaned['loan_amnt'] / cleaned['annual_inc']\n",
    "    else:\n",
    "        cleaned['loan_to_income'] = None\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Test on sample row\n",
    "test_cleaned = clean_accepted_loan_row(sample_row)\n",
    "print(\"Cleaned sample row:\")\n",
    "for k, v in test_cleaned.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying cleaning transformation via map()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned RDD count: 2,260,701\n",
      "CPU times: user 21.7 ms, sys: 9.74 ms, total: 31.4 ms\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply cleaning transformation using map()\n",
    "# This is the core MapReduce cleaning operation\n",
    "\n",
    "print(\"Applying cleaning transformation via map()...\")\n",
    "\n",
    "cleaned_rdd = accepted_rdd.map(clean_accepted_loan_row)\n",
    "\n",
    "# Cache for reuse (important for performance)\n",
    "cleaned_rdd.cache()\n",
    "\n",
    "# Force evaluation and count\n",
    "cleaned_count = cleaned_rdd.count()\n",
    "print(f\"Cleaned RDD count: {cleaned_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Filter Invalid Records\n",
    "\n",
    "Using **filter** transformation to remove invalid records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_loan_record(row_dict):\n",
    "    \"\"\"\n",
    "    Filter function to identify valid records.\n",
    "    Returns True if record should be kept.\n",
    "    \"\"\"\n",
    "    # Must have loan amount\n",
    "    if row_dict.get('loan_amnt') is None or row_dict['loan_amnt'] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # Must have a determinable loan status for ML\n",
    "    if row_dict.get('loan_status_binary') is None:\n",
    "        return False\n",
    "    \n",
    "    # Must have interest rate\n",
    "    if row_dict.get('int_rate') is None or row_dict['int_rate'] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # Must have grade\n",
    "    if row_dict.get('grade') is None:\n",
    "        return False\n",
    "    \n",
    "    # Must have annual income (and it should be positive)\n",
    "    if row_dict.get('annual_inc') is None or row_dict['annual_inc'] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # Reasonable bounds check\n",
    "    # Interest rate should be between 0 and 50%\n",
    "    if row_dict['int_rate'] > 50:\n",
    "        return False\n",
    "    \n",
    "    # Annual income should be reasonable (< $10M)\n",
    "    if row_dict['annual_inc'] > 10000000:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying filter transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records after filtering: 2,258,994\n",
      "Records removed: 1,707 (0.08%)\n",
      "CPU times: user 11.9 ms, sys: 6.01 ms, total: 17.9 ms\n",
      "Wall time: 5.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply filter transformation\n",
    "print(\"Applying filter transformation...\")\n",
    "\n",
    "filtered_rdd = cleaned_rdd.filter(is_valid_loan_record)\n",
    "\n",
    "# Cache filtered RDD\n",
    "filtered_rdd.cache()\n",
    "\n",
    "filtered_count = filtered_rdd.count()\n",
    "removed_count = cleaned_count - filtered_count\n",
    "\n",
    "print(f\"Records after filtering: {filtered_count:,}\")\n",
    "print(f\"Records removed: {removed_count:,} ({removed_count/cleaned_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Profile Cleaned Data with MapReduce Aggregations\n",
    "\n",
    "Using **reduceByKey**, **aggregateByKey**, and other MapReduce operations to profile the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Status Distribution:\n",
      "  Paid/Current (0):  1,963,912 (86.94%)\n",
      "  Default (1):    295,082 (13.06%)\n",
      "CPU times: user 10.6 ms, sys: 3.02 ms, total: 13.7 ms\n",
      "Wall time: 2.03 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute statistics using MapReduce\n",
    "# Count by loan_status_binary using map + reduceByKey\n",
    "\n",
    "status_counts = filtered_rdd \\\n",
    "    .map(lambda x: (x['loan_status_binary'], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Loan Status Distribution:\")\n",
    "for status, count in sorted(status_counts):\n",
    "    label = \"Default\" if status == 1 else \"Paid/Current\"\n",
    "    pct = count / filtered_count * 100\n",
    "    print(f\"  {label} ({status}): {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Distribution:\n",
      "  Grade A:    432,747 (19.16%)\n",
      "  Grade B:    663,130 (29.36%)\n",
      "  Grade C:    649,552 (28.75%)\n",
      "  Grade D:    324,099 (14.35%)\n",
      "  Grade E:    135,542 ( 6.00%)\n",
      "  Grade F:     41,773 ( 1.85%)\n",
      "  Grade G:     12,151 ( 0.54%)\n",
      "CPU times: user 11.6 ms, sys: 4.9 ms, total: 16.5 ms\n",
      "Wall time: 2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Grade distribution using MapReduce\n",
    "\n",
    "grade_counts = filtered_rdd \\\n",
    "    .map(lambda x: (x['grade'], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Grade Distribution:\")\n",
    "for grade, count in sorted(grade_counts):\n",
    "    pct = count / filtered_count * 100\n",
    "    print(f\"  Grade {grade}: {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Rate by Grade:\n",
      "--------------------------------------------------\n",
      "  Grade A:  3.67% default rate (15,876 / 432,747)\n",
      "  Grade B:  8.81% default rate (58,429 / 663,130)\n",
      "  Grade C: 14.59% default rate (94,795 / 649,552)\n",
      "  Grade D: 20.66% default rate (66,970 / 324,099)\n",
      "  Grade E: 28.60% default rate (38,760 / 135,542)\n",
      "  Grade F: 36.74% default rate (15,349 / 41,773)\n",
      "  Grade G: 40.35% default rate (4,903 / 12,151)\n",
      "CPU times: user 13.3 ms, sys: 3.17 ms, total: 16.5 ms\n",
      "Wall time: 2.03 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Default rate by grade using MapReduce\n",
    "# Map: (grade, (default_status, 1))\n",
    "# Reduce: (grade, (total_defaults, total_count))\n",
    "\n",
    "def grade_default_mapper(row):\n",
    "    grade = row['grade']\n",
    "    is_default = row['loan_status_binary']\n",
    "    return (grade, (is_default, 1))\n",
    "\n",
    "def grade_default_reducer(a, b):\n",
    "    return (a[0] + b[0], a[1] + b[1])\n",
    "\n",
    "default_by_grade = filtered_rdd \\\n",
    "    .map(grade_default_mapper) \\\n",
    "    .reduceByKey(grade_default_reducer) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Default Rate by Grade:\")\n",
    "print(\"-\" * 50)\n",
    "for grade, (defaults, total) in sorted(default_by_grade):\n",
    "    rate = defaults / total * 100\n",
    "    print(f\"  Grade {grade}: {rate:5.2f}% default rate ({defaults:,} / {total:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Amount Statistics:\n",
      "  Min: $500.00\n",
      "  Max: $40,000.00\n",
      "  Avg: $15,043.90\n",
      "  Count: 2,258,994\n",
      "CPU times: user 11.9 ms, sys: 3.27 ms, total: 15.2 ms\n",
      "Wall time: 1.87 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute basic statistics for numeric columns using aggregate\n",
    "# Using aggregate to compute min, max, sum, count in one pass\n",
    "\n",
    "def stats_seq_op(acc, row):\n",
    "    \"\"\"Sequential operation: update accumulator with new row\"\"\"\n",
    "    value = row.get('loan_amnt')\n",
    "    if value is not None:\n",
    "        min_val = min(acc[0], value) if acc[0] is not None else value\n",
    "        max_val = max(acc[1], value) if acc[1] is not None else value\n",
    "        sum_val = acc[2] + value\n",
    "        count_val = acc[3] + 1\n",
    "        return (min_val, max_val, sum_val, count_val)\n",
    "    return acc\n",
    "\n",
    "def stats_comb_op(acc1, acc2):\n",
    "    \"\"\"Combine operation: merge two accumulators\"\"\"\n",
    "    min_val = min(acc1[0], acc2[0]) if acc1[0] is not None and acc2[0] is not None else acc1[0] or acc2[0]\n",
    "    max_val = max(acc1[1], acc2[1]) if acc1[1] is not None and acc2[1] is not None else acc1[1] or acc2[1]\n",
    "    sum_val = acc1[2] + acc2[2]\n",
    "    count_val = acc1[3] + acc2[3]\n",
    "    return (min_val, max_val, sum_val, count_val)\n",
    "\n",
    "# Initial accumulator: (min, max, sum, count)\n",
    "zero_value = (None, None, 0.0, 0)\n",
    "\n",
    "loan_amnt_stats = filtered_rdd.aggregate(zero_value, stats_seq_op, stats_comb_op)\n",
    "\n",
    "min_val, max_val, sum_val, count_val = loan_amnt_stats\n",
    "avg_val = sum_val / count_val if count_val > 0 else 0\n",
    "\n",
    "print(\"Loan Amount Statistics:\")\n",
    "print(f\"  Min: ${min_val:,.2f}\")\n",
    "print(f\"  Max: ${max_val:,.2f}\")\n",
    "print(f\"  Avg: ${avg_val:,.2f}\")\n",
    "print(f\"  Count: {count_val:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Column Statistics:\n",
      "======================================================================\n",
      "Column                   Min          Max          Avg        Count\n",
      "----------------------------------------------------------------------\n",
      "loan_amnt             500.00    40,000.00    15,043.90    2,258,994\n",
      "int_rate                5.31        30.99        13.09    2,258,994\n",
      "annual_inc              0.36 9,930,475.00    77,969.52    2,258,994\n",
      "dti                    -1.00       999.00        18.82    2,258,942\n",
      "fico_avg              612.00       847.50       700.58    2,258,994\n",
      "CPU times: user 8.83 ms, sys: 5.15 ms, total: 14 ms\n",
      "Wall time: 2.96 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute stats for multiple columns using a single pass with aggregate\n",
    "\n",
    "numeric_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_avg']\n",
    "\n",
    "def multi_stats_seq_op(acc, row):\n",
    "    \"\"\"Update stats for all numeric columns\"\"\"\n",
    "    result = dict(acc)\n",
    "    for col in numeric_cols:\n",
    "        value = row.get(col)\n",
    "        if value is not None:\n",
    "            stats = result[col]\n",
    "            min_val = min(stats[0], value) if stats[0] is not None else value\n",
    "            max_val = max(stats[1], value) if stats[1] is not None else value\n",
    "            sum_val = stats[2] + value\n",
    "            count_val = stats[3] + 1\n",
    "            result[col] = (min_val, max_val, sum_val, count_val)\n",
    "    return result\n",
    "\n",
    "def multi_stats_comb_op(acc1, acc2):\n",
    "    \"\"\"Combine stats from two accumulators\"\"\"\n",
    "    result = {}\n",
    "    for col in numeric_cols:\n",
    "        s1, s2 = acc1[col], acc2[col]\n",
    "        min_val = min(s1[0], s2[0]) if s1[0] is not None and s2[0] is not None else s1[0] or s2[0]\n",
    "        max_val = max(s1[1], s2[1]) if s1[1] is not None and s2[1] is not None else s1[1] or s2[1]\n",
    "        sum_val = s1[2] + s2[2]\n",
    "        count_val = s1[3] + s2[3]\n",
    "        result[col] = (min_val, max_val, sum_val, count_val)\n",
    "    return result\n",
    "\n",
    "zero_multi = {col: (None, None, 0.0, 0) for col in numeric_cols}\n",
    "\n",
    "multi_stats = filtered_rdd.aggregate(zero_multi, multi_stats_seq_op, multi_stats_comb_op)\n",
    "\n",
    "print(\"Numeric Column Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Column':<15} {'Min':>12} {'Max':>12} {'Avg':>12} {'Count':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for col in numeric_cols:\n",
    "    min_v, max_v, sum_v, count_v = multi_stats[col]\n",
    "    avg_v = sum_v / count_v if count_v > 0 else 0\n",
    "    print(f\"{col:<15} {min_v:>12,.2f} {max_v:>12,.2f} {avg_v:>12,.2f} {count_v:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Profiling and Tuning\n",
    "\n",
    "Let's profile and tune our MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Partition Analysis ===\n",
      "Original RDD partitions: 4\n",
      "Cleaned RDD partitions: 4\n",
      "Filtered RDD partitions: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total records: 2,258,994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx records per partition: 564,748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Option A: Skip partition size analysis entirely\n",
    "print(\"=== Partition Analysis ===\")\n",
    "print(f\"Original RDD partitions: {accepted_rdd.getNumPartitions()}\")\n",
    "print(f\"Cleaned RDD partitions: {cleaned_rdd.getNumPartitions()}\")\n",
    "print(f\"Filtered RDD partitions: {filtered_rdd.getNumPartitions()}\")\n",
    "\n",
    "# Skip glom() - it's too memory intensive\n",
    "print(f\"\\nTotal records: {filtered_rdd.count():,}\")\n",
    "print(f\"Approx records per partition: {filtered_rdd.count() // filtered_rdd.getNumPartitions():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions:  4 | Time: 5.887s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions:  8 | Time: 5.728s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 16 | Time: 6.398s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:====================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 32 | Time: 6.847s\n",
      "\n",
      "Optimal partition count: 8 (time: 5.728s)\n",
      "CPU times: user 58.2 ms, sys: 22.1 ms, total: 80.3 ms\n",
      "Wall time: 24.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Experiment with different partition counts\n",
    "# Find optimal partition count based on data size\n",
    "\n",
    "# Rule of thumb: 2-4 partitions per CPU core, or ~128MB per partition\n",
    "# For ~2M records, let's try different values\n",
    "\n",
    "import time\n",
    "\n",
    "partition_tests = [4, 8, 16, 32]\n",
    "results = []\n",
    "\n",
    "for num_partitions in partition_tests:\n",
    "    # Repartition\n",
    "    test_rdd = filtered_rdd.repartition(num_partitions)\n",
    "    \n",
    "    # Time a simple operation\n",
    "    start = time.time()\n",
    "    \n",
    "    # Perform a MapReduce operation\n",
    "    _ = test_rdd \\\n",
    "        .map(lambda x: (x['grade'], x['loan_amnt'])) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .collect()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    results.append((num_partitions, elapsed))\n",
    "    print(f\"Partitions: {num_partitions:2d} | Time: {elapsed:.3f}s\")\n",
    "\n",
    "# Find best\n",
    "best = min(results, key=lambda x: x[1])\n",
    "print(f\"\\nOptimal partition count: {best[0]} (time: {best[1]:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RDD partitions: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Apply optimal partitioning\n",
    "OPTIMAL_PARTITIONS = 8  # Adjust based on results above\n",
    "\n",
    "optimized_rdd = filtered_rdd.repartition(OPTIMAL_PARTITIONS)\n",
    "optimized_rdd.cache()\n",
    "\n",
    "# Force evaluation\n",
    "_ = optimized_rdd.count()\n",
    "\n",
    "print(f\"Optimized RDD partitions: {optimized_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Process Rejected Loans (Simplified)\n",
    "\n",
    "Apply similar cleaning to rejected loans dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected loans columns:\n",
      "  Amount Requested: 1000.0\n",
      "  Application Date: 2007-05-26\n",
      "  Loan Title: Wedding Covered but No Honeymoon\n",
      "  Risk_Score: 693.0\n",
      "  Debt-To-Income Ratio: 10%\n",
      "  Zip Code: 481xx\n",
      "  State: NM\n",
      "  Employment Length: 4 years\n",
      "  Policy Code: 0.0\n",
      "  _ingestion_timestamp: 2025-11-25 15:47:47.261000\n",
      "  _source_file: rejected_2007_to_2018Q4.csv\n",
      "  _data_source: lending_club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check rejected loans structure\n",
    "rejected_sample = rejected_rdd.first()\n",
    "print(\"Rejected loans columns:\")\n",
    "for k, v in rejected_sample.asDict().items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rejected_loan_row(row):\n",
    "    \"\"\"\n",
    "    Clean rejected loan records using map().\n",
    "    \"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    \n",
    "    cleaned = {\n",
    "        'Amount Requested': clean_numeric(row_dict.get('Amount Requested')),\n",
    "        'Application Date': clean_string(row_dict.get('Application Date')),\n",
    "        'Loan Title': clean_string(row_dict.get('Loan Title')),\n",
    "        'Risk_Score': clean_numeric(row_dict.get('Risk_Score')),\n",
    "        'Debt-To-Income Ratio': clean_percentage(row_dict.get('Debt-To-Income Ratio')),\n",
    "        'Zip Code': clean_string(row_dict.get('Zip Code')),\n",
    "        'State': clean_string(row_dict.get('State')),\n",
    "        'Employment Length': clean_emp_length(row_dict.get('Employment Length')),\n",
    "        'Policy Code': clean_string(row_dict.get('Policy Code')),\n",
    "    }\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def is_valid_rejected_record(row_dict):\n",
    "    \"\"\"Filter valid rejected records\"\"\"\n",
    "    if row_dict.get('Amount Requested') is None or row_dict['Amount Requested'] <= 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning rejected loans...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned rejected loans: 27,647,453\n",
      "CPU times: user 27.7 ms, sys: 10.3 ms, total: 37.9 ms\n",
      "Wall time: 1min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean rejected loans using MapReduce\n",
    "\n",
    "print(\"Cleaning rejected loans...\")\n",
    "\n",
    "cleaned_rejected_rdd = rejected_rdd \\\n",
    "    .map(clean_rejected_loan_row) \\\n",
    "    .filter(is_valid_rejected_record)\n",
    "\n",
    "cleaned_rejected_rdd.cache()\n",
    "\n",
    "rejected_count = cleaned_rejected_rdd.count()\n",
    "print(f\"Cleaned rejected loans: {rejected_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save to Silver Layer\n",
    "\n",
    "Convert cleaned RDDs to DataFrames and save as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for accepted loans Silver layer\n",
    "accepted_silver_schema = StructType([\n",
    "    StructField(\"loan_amnt\", FloatType(), True),\n",
    "    StructField(\"term\", IntegerType(), True),\n",
    "    StructField(\"int_rate\", FloatType(), True),\n",
    "    StructField(\"installment\", FloatType(), True),\n",
    "    StructField(\"grade\", StringType(), True),\n",
    "    StructField(\"sub_grade\", StringType(), True),\n",
    "    StructField(\"emp_length\", IntegerType(), True),\n",
    "    StructField(\"home_ownership\", StringType(), True),\n",
    "    StructField(\"annual_inc\", FloatType(), True),\n",
    "    StructField(\"verification_status\", StringType(), True),\n",
    "    StructField(\"purpose\", StringType(), True),\n",
    "    StructField(\"loan_status\", StringType(), True),\n",
    "    StructField(\"loan_status_binary\", IntegerType(), True),\n",
    "    StructField(\"issue_d\", StringType(), True),\n",
    "    StructField(\"dti\", FloatType(), True),\n",
    "    StructField(\"earliest_cr_line\", StringType(), True),\n",
    "    StructField(\"open_acc\", FloatType(), True),\n",
    "    StructField(\"pub_rec\", FloatType(), True),\n",
    "    StructField(\"revol_bal\", FloatType(), True),\n",
    "    StructField(\"revol_util\", FloatType(), True),\n",
    "    StructField(\"total_acc\", FloatType(), True),\n",
    "    StructField(\"fico_range_low\", FloatType(), True),\n",
    "    StructField(\"fico_range_high\", FloatType(), True),\n",
    "    StructField(\"addr_state\", StringType(), True),\n",
    "    StructField(\"delinq_2yrs\", FloatType(), True),\n",
    "    StructField(\"inq_last_6mths\", FloatType(), True),\n",
    "    StructField(\"mort_acc\", FloatType(), True),\n",
    "    StructField(\"pub_rec_bankruptcies\", FloatType(), True),\n",
    "    StructField(\"fico_avg\", FloatType(), True),\n",
    "    StructField(\"loan_to_income\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver DataFrame created with 30 columns\n",
      "root\n",
      " |-- loan_amnt: float (nullable = true)\n",
      " |-- term: integer (nullable = true)\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- installment: float (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_length: integer (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: float (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_status_binary: integer (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- open_acc: float (nullable = true)\n",
      " |-- pub_rec: float (nullable = true)\n",
      " |-- revol_bal: float (nullable = true)\n",
      " |-- revol_util: float (nullable = true)\n",
      " |-- total_acc: float (nullable = true)\n",
      " |-- fico_range_low: float (nullable = true)\n",
      " |-- fico_range_high: float (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- delinq_2yrs: float (nullable = true)\n",
      " |-- inq_last_6mths: float (nullable = true)\n",
      " |-- mort_acc: float (nullable = true)\n",
      " |-- pub_rec_bankruptcies: float (nullable = true)\n",
      " |-- fico_avg: float (nullable = true)\n",
      " |-- loan_to_income: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert RDD of dicts to RDD of Rows\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def dict_to_row(d):\n",
    "    \"\"\"Convert dictionary to Row with proper type handling\"\"\"\n",
    "    return Row(\n",
    "        loan_amnt=float(d['loan_amnt']) if d['loan_amnt'] is not None else None,\n",
    "        term=int(d['term']) if d['term'] is not None else None,\n",
    "        int_rate=float(d['int_rate']) if d['int_rate'] is not None else None,\n",
    "        installment=float(d['installment']) if d['installment'] is not None else None,\n",
    "        grade=d['grade'],\n",
    "        sub_grade=d['sub_grade'],\n",
    "        emp_length=int(d['emp_length']) if d['emp_length'] is not None else None,\n",
    "        home_ownership=d['home_ownership'],\n",
    "        annual_inc=float(d['annual_inc']) if d['annual_inc'] is not None else None,\n",
    "        verification_status=d['verification_status'],\n",
    "        purpose=d['purpose'],\n",
    "        loan_status=d['loan_status'],\n",
    "        loan_status_binary=int(d['loan_status_binary']) if d['loan_status_binary'] is not None else None,\n",
    "        issue_d=d['issue_d'],\n",
    "        dti=float(d['dti']) if d['dti'] is not None else None,\n",
    "        earliest_cr_line=d['earliest_cr_line'],\n",
    "        open_acc=float(d['open_acc']) if d['open_acc'] is not None else None,\n",
    "        pub_rec=float(d['pub_rec']) if d['pub_rec'] is not None else None,\n",
    "        revol_bal=float(d['revol_bal']) if d['revol_bal'] is not None else None,\n",
    "        revol_util=float(d['revol_util']) if d['revol_util'] is not None else None,\n",
    "        total_acc=float(d['total_acc']) if d['total_acc'] is not None else None,\n",
    "        fico_range_low=float(d['fico_range_low']) if d['fico_range_low'] is not None else None,\n",
    "        fico_range_high=float(d['fico_range_high']) if d['fico_range_high'] is not None else None,\n",
    "        addr_state=d['addr_state'],\n",
    "        delinq_2yrs=float(d['delinq_2yrs']) if d['delinq_2yrs'] is not None else None,\n",
    "        inq_last_6mths=float(d['inq_last_6mths']) if d['inq_last_6mths'] is not None else None,\n",
    "        mort_acc=float(d['mort_acc']) if d['mort_acc'] is not None else None,\n",
    "        pub_rec_bankruptcies=float(d['pub_rec_bankruptcies']) if d['pub_rec_bankruptcies'] is not None else None,\n",
    "        fico_avg=float(d['fico_avg']) if d['fico_avg'] is not None else None,\n",
    "        loan_to_income=float(d['loan_to_income']) if d['loan_to_income'] is not None else None,\n",
    "    )\n",
    "\n",
    "# Transform using map\n",
    "row_rdd = optimized_rdd.map(dict_to_row)\n",
    "\n",
    "# Create DataFrame\n",
    "accepted_silver_df = spark.createDataFrame(row_rdd, schema=accepted_silver_schema)\n",
    "\n",
    "print(f\"Silver DataFrame created with {len(accepted_silver_df.columns)} columns\")\n",
    "accepted_silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----------+------------------+--------+--------------+\n",
      "|loan_amnt|int_rate|grade|annual_inc|loan_status_binary|fico_avg|loan_to_income|\n",
      "+---------+--------+-----+----------+------------------+--------+--------------+\n",
      "|16000.0  |13.44   |C    |42000.0   |1                 |672.0   |0.3809524     |\n",
      "|21875.0  |11.99   |C    |47590.0   |0                 |702.0   |0.4596554     |\n",
      "|14025.0  |18.49   |D    |39000.0   |0                 |677.0   |0.3596154     |\n",
      "|35000.0  |12.88   |C    |106000.0  |1                 |732.0   |0.3301887     |\n",
      "|20000.0  |12.88   |C    |145000.0  |0                 |697.0   |0.13793103    |\n",
      "|14000.0  |14.85   |C    |60000.0   |1                 |692.0   |0.23333333    |\n",
      "|16000.0  |8.49    |B    |62000.0   |0                 |722.0   |0.2580645     |\n",
      "|28000.0  |14.85   |C    |85000.0   |0                 |672.0   |0.32941177    |\n",
      "|20000.0  |13.99   |C    |70000.0   |1                 |682.0   |0.2857143     |\n",
      "|17600.0  |19.89   |E    |44000.0   |0                 |682.0   |0.4           |\n",
      "+---------+--------+-----+----------+------------------+--------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview silver data\n",
    "accepted_silver_df.select(\n",
    "    'loan_amnt', 'int_rate', 'grade', 'annual_inc', \n",
    "    'loan_status_binary', 'fico_avg', 'loan_to_income'\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver data saved to: ../data/medallion/silver/accepted_loans\n",
      "CPU times: user 10.2 ms, sys: 2.71 ms, total: 12.9 ms\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save to Silver layer\n",
    "SILVER_ACCEPTED_PATH = os.path.join(SILVER_PATH, \"accepted_loans\")\n",
    "\n",
    "accepted_silver_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(SILVER_ACCEPTED_PATH)\n",
    "\n",
    "print(f\"Silver data saved to: {SILVER_ACCEPTED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected Silver data saved to: ../data/medallion/silver/rejected_loans\n",
      "CPU times: user 18.4 ms, sys: 12 ms, total: 30.4 ms\n",
      "Wall time: 1min 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save rejected loans Silver layer\n",
    "rejected_silver_schema = StructType([\n",
    "    StructField(\"amount_requested\", FloatType(), True),\n",
    "    StructField(\"application_date\", StringType(), True),\n",
    "    StructField(\"loan_title\", StringType(), True),\n",
    "    StructField(\"risk_score\", FloatType(), True),\n",
    "    StructField(\"dti\", FloatType(), True),\n",
    "    StructField(\"zip_code\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"emp_length\", IntegerType(), True),\n",
    "    StructField(\"policy_code\", StringType(), True),\n",
    "])\n",
    "\n",
    "def rejected_dict_to_row(d):\n",
    "    return Row(\n",
    "        amount_requested=float(d['Amount Requested']) if d['Amount Requested'] is not None else None,\n",
    "        application_date=d['Application Date'],\n",
    "        loan_title=d['Loan Title'],\n",
    "        risk_score=float(d['Risk_Score']) if d['Risk_Score'] is not None else None,\n",
    "        dti=float(d['Debt-To-Income Ratio']) if d['Debt-To-Income Ratio'] is not None else None,\n",
    "        zip_code=d['Zip Code'],\n",
    "        state=d['State'],\n",
    "        emp_length=int(d['Employment Length']) if d['Employment Length'] is not None else None,\n",
    "        policy_code=d['Policy Code'],\n",
    "    )\n",
    "\n",
    "rejected_row_rdd = cleaned_rejected_rdd.map(rejected_dict_to_row)\n",
    "rejected_silver_df = spark.createDataFrame(rejected_row_rdd, schema=rejected_silver_schema)\n",
    "\n",
    "SILVER_REJECTED_PATH = os.path.join(SILVER_PATH, \"rejected_loans\")\n",
    "rejected_silver_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(SILVER_REJECTED_PATH)\n",
    "\n",
    "print(f\"Rejected Silver data saved to: {SILVER_REJECTED_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verification and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Silver Layer Verification ===\n",
      "Accepted loans: 2,258,994 rows, 30 columns\n",
      "Rejected loans: 27,647,453 rows, 9 columns\n"
     ]
    }
   ],
   "source": [
    "# Verify saved data\n",
    "verify_accepted = spark.read.parquet(SILVER_ACCEPTED_PATH)\n",
    "verify_rejected = spark.read.parquet(SILVER_REJECTED_PATH)\n",
    "\n",
    "print(\"=== Silver Layer Verification ===\")\n",
    "print(f\"Accepted loans: {verify_accepted.count():,} rows, {len(verify_accepted.columns)} columns\")\n",
    "print(f\"Rejected loans: {verify_rejected.count():,} rows, {len(verify_rejected.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61M\t../data/medallion/silver/accepted_loans\n",
      "211M\t../data/medallion/silver/rejected_loans\n"
     ]
    }
   ],
   "source": [
    "# Check storage sizes\n",
    "!du -sh {SILVER_ACCEPTED_PATH}\n",
    "!du -sh {SILVER_REJECTED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SILVER LAYER CLEANING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "MapReduce Operations Used:\n",
      "  - map(): Data cleaning and transformation\n",
      "  - filter(): Remove invalid records\n",
      "  - flatMap(): Profile null values across columns\n",
      "  - reduceByKey(): Aggregate statistics by category\n",
      "  - aggregate(): Compute min/max/sum/count in single pass\n",
      "  - glom(): Analyze partition distribution\n",
      "  - repartition(): Optimize partitioning\n",
      "\n",
      "Data Quality Improvements:\n",
      "  - Cleaned 27 key columns\n",
      "  - Standardized date formats\n",
      "  - Converted percentages and currency values\n",
      "  - Created binary target variable for ML\n",
      "  - Added derived features (fico_avg, loan_to_income)\n",
      "\n",
      "Records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Bronze accepted: 2,260,701\n",
      "  - Silver accepted: 2,258,994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Records removed: 1,707\n",
      "\n",
      "Output Paths:\n",
      "  - ../data/medallion/silver/accepted_loans\n",
      "  - ../data/medallion/silver/rejected_loans\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Final summary statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"SILVER LAYER CLEANING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nMapReduce Operations Used:\")\n",
    "print(\"  - map(): Data cleaning and transformation\")\n",
    "print(\"  - filter(): Remove invalid records\")\n",
    "print(\"  - flatMap(): Profile null values across columns\")\n",
    "print(\"  - reduceByKey(): Aggregate statistics by category\")\n",
    "print(\"  - aggregate(): Compute min/max/sum/count in single pass\")\n",
    "print(\"  - glom(): Analyze partition distribution\")\n",
    "print(\"  - repartition(): Optimize partitioning\")\n",
    "\n",
    "print(\"\\nData Quality Improvements:\")\n",
    "print(f\"  - Cleaned {len(KEY_COLUMNS)} key columns\")\n",
    "print(f\"  - Standardized date formats\")\n",
    "print(f\"  - Converted percentages and currency values\")\n",
    "print(f\"  - Created binary target variable for ML\")\n",
    "print(f\"  - Added derived features (fico_avg, loan_to_income)\")\n",
    "\n",
    "print(\"\\nRecords:\")\n",
    "print(f\"  - Bronze accepted: {accepted_rdd.count():,}\")\n",
    "print(f\"  - Silver accepted: {verify_accepted.count():,}\")\n",
    "print(f\"  - Records removed: {accepted_rdd.count() - verify_accepted.count():,}\")\n",
    "\n",
    "print(\"\\nOutput Paths:\")\n",
    "print(f\"  - {SILVER_ACCEPTED_PATH}\")\n",
    "print(f\"  - {SILVER_REJECTED_PATH}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached RDDs unpersisted.\n"
     ]
    }
   ],
   "source": [
    "# Unpersist cached RDDs to free memory\n",
    "cleaned_rdd.unpersist()\n",
    "filtered_rdd.unpersist()\n",
    "optimized_rdd.unpersist()\n",
    "cleaned_rejected_rdd.unpersist()\n",
    "\n",
    "print(\"Cached RDDs unpersisted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The Silver layer is complete. The data is now cleaned and ready for analytics and ML.\n",
    "\n",
    "**Continue to:** `03_gold_serving.ipynb` for data serving using DataFrames, SQL, and MLlib."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
