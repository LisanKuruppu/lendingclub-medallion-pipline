{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer - Data Cleaning with MapReduce\n",
    "\n",
    "## Lending Club Loan Data Pipeline\n",
    "\n",
    "**Use Case:** Predict loan default risk and analyze factors affecting loan approval\n",
    "\n",
    "This notebook implements the Silver (Cleaned) layer of the Medallion Architecture:\n",
    "- Clean and transform data using **RDD MapReduce operations** (no DataFrames/SQL)\n",
    "- Handle missing values, type conversions, and data standardization\n",
    "- Profile and tune performance\n",
    "\n",
    "**Important:** As per project requirements, this notebook uses basic MapReduce routines in Spark (map, filter, reduce, reduceByKey, etc.) - NOT DataFrames or SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined. Ready for benchmarking.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def clean_percentage(value):\n",
    "    if value is None or value == '': return None\n",
    "    try: return float(str(value).replace('%', '').strip())\n",
    "    except: return None\n",
    "\n",
    "def clean_term(value):\n",
    "    if value is None or value == '': return None\n",
    "    try:\n",
    "        match = re.search(r'(\\d+)', str(value))\n",
    "        return int(match.group(1)) if match else None\n",
    "    except: return None\n",
    "\n",
    "def clean_emp_length(value):\n",
    "    if value is None or value == '' or str(value).lower() == 'n/a': return None\n",
    "    val = str(value).lower()\n",
    "    if '10+' in val: return 10\n",
    "    if '< 1' in val: return 0\n",
    "    try:\n",
    "        match = re.search(r'(\\d+)', val)\n",
    "        return int(match.group(1)) if match else None\n",
    "    except: return None\n",
    "\n",
    "def clean_numeric(value):\n",
    "    if value is None or str(value).lower() in ['null', 'nan', 'none', '']: return None\n",
    "    try: return float(value)\n",
    "    except: return None\n",
    "\n",
    "def clean_string(value):\n",
    "    if value is None: return None\n",
    "    cleaned = str(value).strip()\n",
    "    if not cleaned or cleaned.lower() == 'null': return None\n",
    "    return cleaned\n",
    "\n",
    "def clean_date(value):\n",
    "    if value is None or value == '': return None\n",
    "    try: return datetime.strptime(str(value), '%b-%Y').strftime('%Y-%m-%d')\n",
    "    except: return None\n",
    "\n",
    "def create_loan_status_binary(status):\n",
    "    if status is None: return None\n",
    "    s = str(status).lower().strip()\n",
    "    if s in ['charged off', 'default', 'late (31-120 days)', 'does not meet the credit policy. status:charged off']: return 1\n",
    "    if s in ['fully paid', 'current', 'in grace period']: return 0\n",
    "    return None\n",
    "\n",
    "def clean_row(row):\n",
    "    d = row.asDict()\n",
    "    return {\n",
    "        'loan_amnt': clean_numeric(d.get('loan_amnt')),\n",
    "        'term': clean_term(d.get('term')),\n",
    "        'int_rate': clean_percentage(d.get('int_rate')),\n",
    "        'grade': clean_string(d.get('grade')),\n",
    "        'emp_length': clean_emp_length(d.get('emp_length')),\n",
    "        'annual_inc': clean_numeric(d.get('annual_inc')),\n",
    "        'loan_status_binary': create_loan_status_binary(d.get('loan_status'))\n",
    "    }\n",
    "\n",
    "print(\"Functions defined. Ready for benchmarking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING CONFIGURATION BENCHMARK ===\n",
      "\n",
      "Testing: Config 1 (Standard 4x4)\n",
      "Settings: {'spark.executor.memory': '4g', 'spark.executor.cores': '4', 'spark.task.cpus': '1', 'spark.default.parallelism': '16'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 25.13 seconds\n",
      "\n",
      "Testing: Config 2 (Fat Tasks)\n",
      "Settings: {'spark.executor.memory': '4g', 'spark.executor.cores': '4', 'spark.task.cpus': '4', 'spark.default.parallelism': '4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 59.47 seconds\n",
      "\n",
      "Testing: Config 3 (Tiny Executors)\n",
      "Settings: {'spark.executor.memory': '1g', 'spark.executor.cores': '1', 'spark.task.cpus': '1', 'spark.default.parallelism': '24'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 31.22 seconds\n",
      "\n",
      "Testing: Config 4 (Balanced 2x2)\n",
      "Settings: {'spark.executor.memory': '2g', 'spark.executor.cores': '2', 'spark.task.cpus': '1', 'spark.default.parallelism': '16'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 26.49 seconds\n",
      "\n",
      "========================================\n",
      "BENCHMARK LEADERBOARD\n",
      "========================================\n",
      "1. Config 1 (Standard 4x4)   : 25.13s\n",
      "2. Config 4 (Balanced 2x2)   : 26.49s\n",
      "3. Config 3 (Tiny Executors) : 31.22s\n",
      "4. Config 2 (Fat Tasks)      : 59.47s\n",
      "\n",
      "Initializing session with winner: Config 1 (Standard 4x4)\n",
      "Ready to proceed with notebook!\n"
     ]
    }
   ],
   "source": [
    "# Define the configurations to test\n",
    "configs = {\n",
    "    \"Config 1 (Standard 4x4)\": {\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"4\",\n",
    "        \"spark.task.cpus\": \"1\",\n",
    "        \"spark.default.parallelism\": \"16\" \n",
    "    },\n",
    "    \"Config 2 (Fat Tasks)\": {\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"4\",\n",
    "        \"spark.task.cpus\": \"4\", # One task takes whole executor (no parallelism)\n",
    "        \"spark.default.parallelism\": \"4\"\n",
    "    },\n",
    "    \"Config 3 (Tiny Executors)\": {\n",
    "        \"spark.executor.memory\": \"1g\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.task.cpus\": \"1\",\n",
    "        \"spark.default.parallelism\": \"24\" # High parallelism\n",
    "    },\n",
    "    \"Config 4 (Balanced 2x2)\": {\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"2\",\n",
    "        \"spark.task.cpus\": \"1\",\n",
    "        \"spark.default.parallelism\": \"16\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define paths\n",
    "BRONZE_PATH = \"../data/medallion/bronze/accepted_loans\"\n",
    "results = {}\n",
    "\n",
    "print(\"=== STARTING CONFIGURATION BENCHMARK ===\")\n",
    "\n",
    "for name, conf in configs.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    print(f\"Settings: {conf}\")\n",
    "    \n",
    "    # 1. Stop existing Spark session\n",
    "    if 'spark' in globals():\n",
    "        spark.stop()\n",
    "        time.sleep(5) # Give cluster time to release resources\n",
    "    \n",
    "    # 2. Start new Spark session with specific config\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(f\"Silver-Benchmark-{name}\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    \n",
    "    for k, v in conf.items():\n",
    "        builder.config(k, v)\n",
    "        \n",
    "    spark = builder.getOrCreate()\n",
    "    \n",
    "    # 3. Run Workload (Read -> Map -> Filter -> Count)\n",
    "    # We include a Shuffle (reduceByKey) to stress test the network config\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load\n",
    "        df = spark.read.parquet(BRONZE_PATH)\n",
    "        rdd = df.rdd\n",
    "        \n",
    "        # Heavy Processing\n",
    "        res = rdd.map(clean_row) \\\n",
    "                 .filter(lambda x: x['loan_amnt'] is not None and x['loan_amnt'] > 1000) \\\n",
    "                 .map(lambda x: (x['grade'], 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b) \\\n",
    "                 .collect()\n",
    "                 \n",
    "        duration = time.time() - start_time\n",
    "        results[name] = duration\n",
    "        print(f\"Result: {duration:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {e}\")\n",
    "        results[name] = 9999.0\n",
    "\n",
    "# 4. Print Final Leaderboard\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"BENCHMARK LEADERBOARD\")\n",
    "print(\"=\"*40)\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "\n",
    "for rank, (name, time_taken) in enumerate(sorted_results, 1):\n",
    "    print(f\"{rank}. {name:<25} : {time_taken:.2f}s\")\n",
    "\n",
    "# 5. Initialize the winner for the rest of the notebook\n",
    "winner_name = sorted_results[0][0]\n",
    "winner_conf = configs[winner_name]\n",
    "print(f\"\\nInitializing session with winner: {winner_name}\")\n",
    "spark.stop()\n",
    "time.sleep(2)\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"LendingClub-Silver-Final\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "\n",
    "for k, v in winner_conf.items():\n",
    "    builder.config(k, v)\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(\"Ready to proceed with notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Bronze Data and Convert to RDD\n",
    "\n",
    "We load the Parquet data into a DataFrame first (for efficient reading), then immediately convert to RDD for MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze input path: ../data/medallion/bronze/\n",
      "Silver output path: ../data/medallion/silver/\n"
     ]
    }
   ],
   "source": [
    "# Define paths (matching Bronze notebook)\n",
    "BRONZE_PATH = \"../data/medallion/bronze/\"\n",
    "SILVER_PATH = \"../data/medallion/silver/\"\n",
    "\n",
    "BRONZE_ACCEPTED_PATH = os.path.join(BRONZE_PATH, \"accepted_loans\")\n",
    "BRONZE_REJECTED_PATH = os.path.join(BRONZE_PATH, \"rejected_loans\")\n",
    "\n",
    "# Create silver directory\n",
    "os.makedirs(SILVER_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Bronze input path: {BRONZE_PATH}\")\n",
    "print(f\"Silver output path: {SILVER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted loans columns: 155\n",
      "Rejected loans columns: 13\n",
      "\n",
      "Accepted loans RDD partitions: 18\n",
      "Rejected loans RDD partitions: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load Bronze data and convert to RDD\n",
    "# We read parquet (efficient) then convert to RDD of Row objects\n",
    "\n",
    "accepted_df = spark.read.parquet(BRONZE_ACCEPTED_PATH)\n",
    "rejected_df = spark.read.parquet(BRONZE_REJECTED_PATH)\n",
    "\n",
    "# Get column names for reference\n",
    "accepted_columns = accepted_df.columns\n",
    "rejected_columns = rejected_df.columns\n",
    "\n",
    "print(f\"Accepted loans columns: {len(accepted_columns)}\")\n",
    "print(f\"Rejected loans columns: {len(rejected_columns)}\")\n",
    "\n",
    "# Convert to RDD - each element is a Row object\n",
    "accepted_rdd = accepted_df.rdd\n",
    "rejected_rdd = rejected_df.rdd\n",
    "\n",
    "print(f\"\\nAccepted loans RDD partitions: {accepted_rdd.getNumPartitions()}\")\n",
    "print(f\"Rejected loans RDD partitions: {rejected_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample row type: <class 'pyspark.sql.types.Row'>\n",
      "\n",
      "Sample row as dict (first 10 fields):\n",
      "  _data_source: lending_club (type: str)\n",
      "  _ingestion_timestamp: 1764368685.8879948 (type: float)\n",
      "  _source_file: accepted_2007_to_2018Q4.csv (type: str)\n",
      "  _status: valid (type: str)\n",
      "  acc_now_delinq: 0.0 (type: str)\n",
      "  acc_open_past_24mths: 4.0 (type: str)\n",
      "  addr_state: NV (type: str)\n",
      "  all_util: 70.0 (type: str)\n",
      "  annual_inc: 84000.0 (type: str)\n",
      "  annual_inc_joint:  (type: str)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Examine sample row structure\n",
    "sample_row = accepted_rdd.first()\n",
    "print(\"Sample row type:\", type(sample_row))\n",
    "print(\"\\nSample row as dict (first 10 fields):\")\n",
    "sample_dict = sample_row.asDict()\n",
    "for i, (k, v) in enumerate(sample_dict.items()):\n",
    "    if i < 10:\n",
    "        print(f\"  {k}: {v} (type: {type(v).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Data Quality Issues\n",
    "\n",
    "Before cleaning, let's identify the specific issues in the data using MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 27 key columns for cleaning\n"
     ]
    }
   ],
   "source": [
    "# Select key columns for our use case (loan default prediction)\n",
    "# These are the columns we'll clean and use in the Gold layer\n",
    "\n",
    "KEY_COLUMNS = [\n",
    "    # Loan characteristics\n",
    "    'loan_amnt',        # Loan amount\n",
    "    'term',             # Loan term (36 or 60 months)\n",
    "    'int_rate',         # Interest rate\n",
    "    'installment',      # Monthly installment\n",
    "    'grade',            # Loan grade (A-G)\n",
    "    'sub_grade',        # Loan sub-grade (A1-G5)\n",
    "    \n",
    "    # Borrower information\n",
    "    'emp_length',       # Employment length\n",
    "    'home_ownership',   # Home ownership status\n",
    "    'annual_inc',       # Annual income\n",
    "    'verification_status',  # Income verification\n",
    "    \n",
    "    # Loan purpose and status\n",
    "    'purpose',          # Loan purpose\n",
    "    'loan_status',      # Current loan status (TARGET)\n",
    "    'issue_d',          # Issue date\n",
    "    \n",
    "    # Credit history\n",
    "    'dti',              # Debt-to-income ratio\n",
    "    'earliest_cr_line', # Earliest credit line\n",
    "    'open_acc',         # Open credit accounts\n",
    "    'pub_rec',          # Public records\n",
    "    'revol_bal',        # Revolving balance\n",
    "    'revol_util',       # Revolving utilization\n",
    "    'total_acc',        # Total accounts\n",
    "    'fico_range_low',   # FICO score low\n",
    "    'fico_range_high',  # FICO score high\n",
    "    \n",
    "    # Additional useful features\n",
    "    'addr_state',       # State\n",
    "    'delinq_2yrs',      # Delinquencies in 2 years\n",
    "    'inq_last_6mths',   # Inquiries in last 6 months\n",
    "    'mort_acc',         # Mortgage accounts\n",
    "    'pub_rec_bankruptcies'  # Bankruptcies\n",
    "]\n",
    "\n",
    "print(f\"Selected {len(KEY_COLUMNS)} key columns for cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 16) / 18]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2,260,701\n",
      "\n",
      "Null/Missing values per column:\n",
      "--------------------------------------------------\n",
      "emp_length               :    146,938 ( 6.50%)\n",
      "mort_acc                 :     53,470 ( 2.37%)\n",
      "open_acc                 :     44,417 ( 1.96%)\n",
      "pub_rec_bankruptcies     :     21,252 ( 0.94%)\n",
      "purpose                  :     21,010 ( 0.93%)\n",
      "pub_rec                  :     12,308 ( 0.54%)\n",
      "revol_bal                :      5,805 ( 0.26%)\n",
      "revol_util               :      4,931 ( 0.22%)\n",
      "total_acc                :      1,927 ( 0.09%)\n",
      "dti                      :      1,754 ( 0.08%)\n",
      "earliest_cr_line         :         57 ( 0.00%)\n",
      "delinq_2yrs              :         54 ( 0.00%)\n",
      "inq_last_6mths           :         53 ( 0.00%)\n",
      "addr_state               :         48 ( 0.00%)\n",
      "fico_range_low           :         48 ( 0.00%)\n",
      "home_ownership           :         42 ( 0.00%)\n",
      "fico_range_high          :         40 ( 0.00%)\n",
      "annual_inc               :         38 ( 0.00%)\n",
      "installment              :         33 ( 0.00%)\n",
      "verification_status      :         33 ( 0.00%)\n",
      "grade                    :         33 ( 0.00%)\n",
      "issue_d                  :         33 ( 0.00%)\n",
      "sub_grade                :         33 ( 0.00%)\n",
      "loan_status              :         33 ( 0.00%)\n",
      "loan_amnt                :         33 ( 0.00%)\n",
      "term                     :         33 ( 0.00%)\n",
      "int_rate                 :         33 ( 0.00%)\n",
      "CPU times: user 43.2 ms, sys: 16.2 ms, total: 59.4 ms\n",
      "Wall time: 42.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Profile null values using MapReduce\n",
    "# Map: emit (column_name, 1) if value is null/empty, else (column_name, 0)\n",
    "# Reduce: sum to get total nulls per column\n",
    "\n",
    "def count_nulls_mapper(row):\n",
    "    \"\"\"Map function to count nulls for each column\"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    results = []\n",
    "    for col in KEY_COLUMNS:\n",
    "        if col in row_dict:\n",
    "            value = row_dict[col]\n",
    "            # Check for null, None, empty string, or 'null' string\n",
    "            is_null = (value is None or \n",
    "                      value == '' or \n",
    "                      str(value).lower() == 'null' or\n",
    "                      str(value).lower() == 'nan')\n",
    "            results.append((col, 1 if is_null else 0))\n",
    "    return results\n",
    "\n",
    "# Use flatMap since mapper returns multiple pairs\n",
    "null_counts = accepted_rdd \\\n",
    "    .flatMap(count_nulls_mapper) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "total_rows = accepted_rdd.count()\n",
    "\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(\"\\nNull/Missing values per column:\")\n",
    "print(\"-\" * 50)\n",
    "for col, count in sorted(null_counts, key=lambda x: x[1], reverse=True):\n",
    "    pct = (count / total_rows) * 100\n",
    "    print(f\"{col:25s}: {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "term:\n",
      "   36 months                    :  1,609,754 (71.21%)\n",
      "   60 months                    :    650,914 (28.79%)\n",
      "                                :         33 ( 0.00%)\n",
      "\n",
      "grade:\n",
      "  B                             :    663,557 (29.35%)\n",
      "  C                             :    650,053 (28.75%)\n",
      "  A                             :    433,027 (19.15%)\n",
      "  D                             :    324,424 (14.35%)\n",
      "  E                             :    135,639 ( 6.00%)\n",
      "  F                             :     41,800 ( 1.85%)\n",
      "  G                             :     12,168 ( 0.54%)\n",
      "                                :         33 ( 0.00%)\n",
      "\n",
      "sub_grade:\n",
      "  C1                            :    145,903 ( 6.45%)\n",
      "  B5                            :    140,288 ( 6.21%)\n",
      "  B4                            :    139,793 ( 6.18%)\n",
      "  B3                            :    131,514 ( 5.82%)\n",
      "  C2                            :    131,116 ( 5.80%)\n",
      "  C3                            :    129,193 ( 5.71%)\n",
      "  C4                            :    127,115 ( 5.62%)\n",
      "  B2                            :    126,621 ( 5.60%)\n",
      "  B1                            :    125,341 ( 5.54%)\n",
      "  C5                            :    116,726 ( 5.16%)\n",
      "\n",
      "emp_length:\n",
      "  10+ years                     :    739,268 (32.70%)\n",
      "  2 years                       :    200,964 ( 8.89%)\n",
      "  < 1 year                      :    187,525 ( 8.29%)\n",
      "  3 years                       :    178,318 ( 7.89%)\n",
      "                                :    146,938 ( 6.50%)\n",
      "  1 year                        :    146,496 ( 6.48%)\n",
      "  5 years                       :    137,900 ( 6.10%)\n",
      "  4 years                       :    134,757 ( 5.96%)\n",
      "  6 years                       :    101,178 ( 4.48%)\n",
      "  7 years                       :     91,330 ( 4.04%)\n",
      "\n",
      "home_ownership:\n",
      "  MORTGAGE                      :  1,096,525 (48.50%)\n",
      "  RENT                          :    885,030 (39.15%)\n",
      "  OWN                           :    250,867 (11.10%)\n",
      "  10+ years                     :      8,419 ( 0.37%)\n",
      "  2 years                       :      2,623 ( 0.12%)\n",
      "  < 1 year                      :      2,396 ( 0.11%)\n",
      "  3 years                       :      2,345 ( 0.10%)\n",
      "  1 year                        :      1,855 ( 0.08%)\n",
      "  4 years                       :      1,786 ( 0.08%)\n",
      "  5 years                       :      1,739 ( 0.08%)\n",
      "\n",
      "verification_status:\n",
      "  Source Verified               :    876,650 (38.78%)\n",
      "  Not Verified                  :    736,142 (32.56%)\n",
      "  Verified                      :    620,847 (27.46%)\n",
      "  60000.0                       :        589 ( 0.03%)\n",
      "  120000.0                      :        553 ( 0.02%)\n",
      "  80000.0                       :        549 ( 0.02%)\n",
      "  100000.0                      :        546 ( 0.02%)\n",
      "  150000.0                      :        545 ( 0.02%)\n",
      "  90000.0                       :        492 ( 0.02%)\n",
      "  110000.0                      :        480 ( 0.02%)\n",
      "\n",
      "purpose:\n",
      "  debt_consolidation            :  1,242,803 (54.97%)\n",
      "  credit_card                   :    503,424 (22.27%)\n",
      "  home_improvement              :    145,928 ( 6.45%)\n",
      "  other                         :    135,833 ( 6.01%)\n",
      "  major_purchase                :     48,808 ( 2.16%)\n",
      "  medical                       :     26,861 ( 1.19%)\n",
      "  car                           :     22,983 ( 1.02%)\n",
      "  small_business                :     22,906 ( 1.01%)\n",
      "                                :     21,010 ( 0.93%)\n",
      "  vacation                      :     15,228 ( 0.67%)\n",
      "\n",
      "loan_status:\n",
      "  Fully Paid                    :  1,059,885 (46.88%)\n",
      "  Current                       :    871,037 (38.53%)\n",
      "  Charged Off                   :    266,014 (11.77%)\n",
      "  Late (31-120 days)            :     21,349 ( 0.94%)\n",
      "  In Grace Period               :      8,378 ( 0.37%)\n",
      "  Late (16-30 days)             :      4,328 ( 0.19%)\n",
      "  Does not meet the credit policy. Status:Fully Paid:      1,877 ( 0.08%)\n",
      "  Does not meet the credit policy. Status:Charged Off:        732 ( 0.03%)\n",
      "  Sep-2013                      :        566 ( 0.03%)\n",
      "  Aug-2013                      :        549 ( 0.02%)\n",
      "CPU times: user 78.3 ms, sys: 21.6 ms, total: 99.8 ms\n",
      "Wall time: 20.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Profile unique values for categorical columns using MapReduce\n",
    "\n",
    "categorical_cols = ['term', 'grade', 'sub_grade', 'emp_length', \n",
    "                    'home_ownership', 'verification_status', 'purpose', 'loan_status']\n",
    "\n",
    "def extract_categorical_mapper(row):\n",
    "    \"\"\"Extract categorical column values\"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    results = []\n",
    "    for col in categorical_cols:\n",
    "        if col in row_dict:\n",
    "            value = str(row_dict[col]) if row_dict[col] is not None else 'NULL'\n",
    "            results.append(((col, value), 1))\n",
    "    return results\n",
    "\n",
    "# Count occurrences of each value per column\n",
    "categorical_counts = accepted_rdd \\\n",
    "    .flatMap(extract_categorical_mapper) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "# Organize by column\n",
    "cat_summary = defaultdict(dict)\n",
    "for (col, value), count in categorical_counts:\n",
    "    cat_summary[col][value] = count\n",
    "\n",
    "# Display\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    values = cat_summary[col]\n",
    "    for value, count in sorted(values.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        pct = (count / total_rows) * 100\n",
    "        print(f\"  {value:30s}: {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Cleaning via MapReduce\n",
    "\n",
    "This is the core cleaning step using **map** transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loan_status_binary(loan_status):\n",
    "    \"\"\"\n",
    "    Create binary target variable for loan default prediction.\n",
    "    1 = Default (bad loan)\n",
    "    0 = Paid/Current (good loan)\n",
    "    None = Unknown/Exclude\n",
    "    \"\"\"\n",
    "    if loan_status is None:\n",
    "        return None\n",
    "    \n",
    "    status = str(loan_status).lower().strip()\n",
    "    \n",
    "    # Bad loans (default = 1)\n",
    "    bad_statuses = ['charged off', 'default', 'late (31-120 days)', \n",
    "                   'late (16-30 days)', 'does not meet the credit policy. status:charged off']\n",
    "    \n",
    "    # Good loans (default = 0)\n",
    "    good_statuses = ['fully paid', 'current', \n",
    "                    'does not meet the credit policy. status:fully paid',\n",
    "                    'in grace period']\n",
    "    \n",
    "    if status in bad_statuses:\n",
    "        return 1\n",
    "    elif status in good_statuses:\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Exclude unclear statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned sample row:\n",
      "  loan_amnt: 11200.0\n",
      "  term: 60\n",
      "  int_rate: 15.04\n",
      "  installment: 266.69\n",
      "  grade: C\n",
      "  sub_grade: C4\n",
      "  emp_length: 5\n",
      "  home_ownership: MORTGAGE\n",
      "  annual_inc: 84000.0\n",
      "  verification_status: Not Verified\n",
      "  purpose: debt_consolidation\n",
      "  loan_status: Current\n",
      "  loan_status_binary: 0\n",
      "  issue_d: 2018-04-01\n",
      "  dti: 22.1\n",
      "  earliest_cr_line: 2002-02-01\n",
      "  open_acc: 13.0\n",
      "  pub_rec: 0.0\n",
      "  revol_bal: 33431.0\n",
      "  revol_util: 62.1\n",
      "  total_acc: 16.0\n",
      "  fico_range_low: 725.0\n",
      "  fico_range_high: 729.0\n",
      "  addr_state: NV\n",
      "  delinq_2yrs: 0.0\n",
      "  inq_last_6mths: 0.0\n",
      "  mort_acc: 1.0\n",
      "  pub_rec_bankruptcies: 0.0\n",
      "  fico_avg: 727.0\n",
      "  loan_to_income: 0.13333333333333333\n"
     ]
    }
   ],
   "source": [
    "def clean_accepted_loan_row(row):\n",
    "    \"\"\"\n",
    "    Main cleaning function applied to each row via map().\n",
    "    Returns a dictionary with cleaned values.\n",
    "    \"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    \n",
    "    cleaned = {\n",
    "        # Loan characteristics\n",
    "        'loan_amnt': clean_numeric(row_dict.get('loan_amnt')),\n",
    "        'term': clean_term(row_dict.get('term')),\n",
    "        'int_rate': clean_percentage(row_dict.get('int_rate')) if '%' in str(row_dict.get('int_rate', '')) else clean_numeric(row_dict.get('int_rate')),\n",
    "        'installment': clean_numeric(row_dict.get('installment')),\n",
    "        'grade': clean_string(row_dict.get('grade')),\n",
    "        'sub_grade': clean_string(row_dict.get('sub_grade')),\n",
    "        \n",
    "        # Borrower information\n",
    "        'emp_length': clean_emp_length(row_dict.get('emp_length')),\n",
    "        'home_ownership': clean_string(row_dict.get('home_ownership')),\n",
    "        'annual_inc': clean_numeric(row_dict.get('annual_inc')),\n",
    "        'verification_status': clean_string(row_dict.get('verification_status')),\n",
    "        \n",
    "        # Loan purpose and status\n",
    "        'purpose': clean_string(row_dict.get('purpose')),\n",
    "        'loan_status': clean_string(row_dict.get('loan_status')),\n",
    "        'loan_status_binary': create_loan_status_binary(row_dict.get('loan_status')),\n",
    "        'issue_d': clean_date(row_dict.get('issue_d')),\n",
    "        \n",
    "        # Credit history\n",
    "        'dti': clean_numeric(row_dict.get('dti')),\n",
    "        'earliest_cr_line': clean_date(row_dict.get('earliest_cr_line')),\n",
    "        'open_acc': clean_numeric(row_dict.get('open_acc')),\n",
    "        'pub_rec': clean_numeric(row_dict.get('pub_rec')),\n",
    "        'revol_bal': clean_numeric(row_dict.get('revol_bal')),\n",
    "        'revol_util': clean_percentage(row_dict.get('revol_util')) if '%' in str(row_dict.get('revol_util', '')) else clean_numeric(row_dict.get('revol_util')),\n",
    "        'total_acc': clean_numeric(row_dict.get('total_acc')),\n",
    "        'fico_range_low': clean_numeric(row_dict.get('fico_range_low')),\n",
    "        'fico_range_high': clean_numeric(row_dict.get('fico_range_high')),\n",
    "        \n",
    "        # Additional features\n",
    "        'addr_state': clean_string(row_dict.get('addr_state')),\n",
    "        'delinq_2yrs': clean_numeric(row_dict.get('delinq_2yrs')),\n",
    "        'inq_last_6mths': clean_numeric(row_dict.get('inq_last_6mths')),\n",
    "        'mort_acc': clean_numeric(row_dict.get('mort_acc')),\n",
    "        'pub_rec_bankruptcies': clean_numeric(row_dict.get('pub_rec_bankruptcies')),\n",
    "    }\n",
    "    \n",
    "    # Calculate derived features\n",
    "    # FICO average\n",
    "    if cleaned['fico_range_low'] and cleaned['fico_range_high']:\n",
    "        cleaned['fico_avg'] = (cleaned['fico_range_low'] + cleaned['fico_range_high']) / 2\n",
    "    else:\n",
    "        cleaned['fico_avg'] = None\n",
    "    \n",
    "    # Loan to income ratio\n",
    "    if cleaned['loan_amnt'] and cleaned['annual_inc'] and cleaned['annual_inc'] > 0:\n",
    "        cleaned['loan_to_income'] = cleaned['loan_amnt'] / cleaned['annual_inc']\n",
    "    else:\n",
    "        cleaned['loan_to_income'] = None\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Test on sample row\n",
    "test_cleaned = clean_accepted_loan_row(sample_row)\n",
    "print(\"Cleaned sample row:\")\n",
    "for k, v in test_cleaned.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying cleaning transformation via map()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned RDD count: 2,260,701\n",
      "CPU times: user 21.4 ms, sys: 8.5 ms, total: 29.9 ms\n",
      "Wall time: 21.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply cleaning transformation using map()\n",
    "# This is the core MapReduce cleaning operation\n",
    "\n",
    "print(\"Applying cleaning transformation via map()...\")\n",
    "\n",
    "cleaned_rdd = accepted_rdd.map(clean_accepted_loan_row)\n",
    "\n",
    "# Cache for reuse (important for performance)\n",
    "cleaned_rdd.cache()\n",
    "\n",
    "# Force evaluation and count\n",
    "cleaned_count = cleaned_rdd.count()\n",
    "print(f\"Cleaned RDD count: {cleaned_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter Invalid Records\n",
    "\n",
    "Using **filter** transformation to remove invalid records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_loan_record(row_dict):\n",
    "    \"\"\"\n",
    "    Filter function to identify valid records.\n",
    "    Returns True if record should be kept.\n",
    "    \"\"\"\n",
    "    # Must have loan amount\n",
    "    if row_dict.get('loan_amnt') is None or row_dict['loan_amnt'] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # Must have a determinable loan status for ML\n",
    "    if row_dict.get('loan_status_binary') is None:\n",
    "        return False\n",
    "    \n",
    "    # Must have interest rate\n",
    "    if row_dict.get('int_rate') is None or row_dict['int_rate'] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # Must have grade\n",
    "    if row_dict.get('grade') is None:\n",
    "        return False\n",
    "    \n",
    "    # Must have annual income (and it should be positive)\n",
    "    if row_dict.get('annual_inc') is None or row_dict['annual_inc'] <= 0:\n",
    "        return False\n",
    "    \n",
    "    # Reasonable bounds check\n",
    "    # Interest rate should be between 0 and 50%\n",
    "    if row_dict['int_rate'] > 50:\n",
    "        return False\n",
    "    \n",
    "    # Annual income should be reasonable (< $10M)\n",
    "    if row_dict['annual_inc'] > 10000000:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying filter transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records after filtering: 2,231,965\n",
      "Records removed: 28,736 (1.27%)\n",
      "CPU times: user 13.7 ms, sys: 3.64 ms, total: 17.3 ms\n",
      "Wall time: 1.77 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply filter transformation\n",
    "print(\"Applying filter transformation...\")\n",
    "\n",
    "filtered_rdd = cleaned_rdd.filter(is_valid_loan_record)\n",
    "\n",
    "# Cache filtered RDD\n",
    "filtered_rdd.cache()\n",
    "\n",
    "filtered_count = filtered_rdd.count()\n",
    "removed_count = cleaned_count - filtered_count\n",
    "\n",
    "print(f\"Records after filtering: {filtered_count:,}\")\n",
    "print(f\"Records removed: {removed_count:,} ({removed_count/cleaned_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Profile Cleaned Data with MapReduce Aggregations\n",
    "\n",
    "Using **reduceByKey**, **aggregateByKey**, and other MapReduce operations to profile the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Status Distribution:\n",
      "  Paid/Current (0):  1,939,597 (86.90%)\n",
      "  Default (1):    292,368 (13.10%)\n",
      "CPU times: user 13.3 ms, sys: 2.53 ms, total: 15.9 ms\n",
      "Wall time: 844 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute statistics using MapReduce\n",
    "# Count by loan_status_binary using map + reduceByKey\n",
    "\n",
    "status_counts = filtered_rdd \\\n",
    "    .map(lambda x: (x['loan_status_binary'], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Loan Status Distribution:\")\n",
    "for status, count in sorted(status_counts):\n",
    "    label = \"Default\" if status == 1 else \"Paid/Current\"\n",
    "    pct = count / filtered_count * 100\n",
    "    print(f\"  {label} ({status}): {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===========================================>            (14 + 4) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Distribution:\n",
      "  Grade A:    426,139 (19.09%)\n",
      "  Grade B:    654,756 (29.34%)\n",
      "  Grade C:    642,765 (28.80%)\n",
      "  Grade D:    320,918 (14.38%)\n",
      "  Grade E:    134,102 ( 6.01%)\n",
      "  Grade F:     41,272 ( 1.85%)\n",
      "  Grade G:     12,013 ( 0.54%)\n",
      "CPU times: user 10.5 ms, sys: 6.34 ms, total: 16.8 ms\n",
      "Wall time: 923 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Grade distribution using MapReduce\n",
    "\n",
    "grade_counts = filtered_rdd \\\n",
    "    .map(lambda x: (x['grade'], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Grade Distribution:\")\n",
    "for grade, count in sorted(grade_counts):\n",
    "    pct = count / filtered_count * 100\n",
    "    print(f\"  Grade {grade}: {count:>10,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:==================================>                     (11 + 7) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Rate by Grade:\n",
      "--------------------------------------------------\n",
      "  Grade A:  3.68% default rate (15,691 / 426,139)\n",
      "  Grade B:  8.83% default rate (57,843 / 654,756)\n",
      "  Grade C: 14.62% default rate (93,971 / 642,765)\n",
      "  Grade D: 20.69% default rate (66,389 / 320,918)\n",
      "  Grade E: 28.65% default rate (38,417 / 134,102)\n",
      "  Grade F: 36.83% default rate (15,200 / 41,272)\n",
      "  Grade G: 40.43% default rate (4,857 / 12,013)\n",
      "CPU times: user 12.3 ms, sys: 2.85 ms, total: 15.1 ms\n",
      "Wall time: 970 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Default rate by grade using MapReduce\n",
    "# Map: (grade, (default_status, 1))\n",
    "# Reduce: (grade, (total_defaults, total_count))\n",
    "\n",
    "def grade_default_mapper(row):\n",
    "    grade = row['grade']\n",
    "    is_default = row['loan_status_binary']\n",
    "    return (grade, (is_default, 1))\n",
    "\n",
    "def grade_default_reducer(a, b):\n",
    "    return (a[0] + b[0], a[1] + b[1])\n",
    "\n",
    "default_by_grade = filtered_rdd \\\n",
    "    .map(grade_default_mapper) \\\n",
    "    .reduceByKey(grade_default_reducer) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Default Rate by Grade:\")\n",
    "print(\"-\" * 50)\n",
    "for grade, (defaults, total) in sorted(default_by_grade):\n",
    "    rate = defaults / total * 100\n",
    "    print(f\"  Grade {grade}: {rate:5.2f}% default rate ({defaults:,} / {total:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Amount Statistics:\n",
      "  Min: $500.00\n",
      "  Max: $40,000.00\n",
      "  Avg: $15,010.02\n",
      "  Count: 2,231,965\n",
      "CPU times: user 5.42 ms, sys: 4.9 ms, total: 10.3 ms\n",
      "Wall time: 689 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute basic statistics for numeric columns using aggregate\n",
    "# Using aggregate to compute min, max, sum, count in one pass\n",
    "\n",
    "def stats_seq_op(acc, row):\n",
    "    \"\"\"Sequential operation: update accumulator with new row\"\"\"\n",
    "    value = row.get('loan_amnt')\n",
    "    if value is not None:\n",
    "        min_val = min(acc[0], value) if acc[0] is not None else value\n",
    "        max_val = max(acc[1], value) if acc[1] is not None else value\n",
    "        sum_val = acc[2] + value\n",
    "        count_val = acc[3] + 1\n",
    "        return (min_val, max_val, sum_val, count_val)\n",
    "    return acc\n",
    "\n",
    "def stats_comb_op(acc1, acc2):\n",
    "    \"\"\"Combine operation: merge two accumulators\"\"\"\n",
    "    min_val = min(acc1[0], acc2[0]) if acc1[0] is not None and acc2[0] is not None else acc1[0] or acc2[0]\n",
    "    max_val = max(acc1[1], acc2[1]) if acc1[1] is not None and acc2[1] is not None else acc1[1] or acc2[1]\n",
    "    sum_val = acc1[2] + acc2[2]\n",
    "    count_val = acc1[3] + acc2[3]\n",
    "    return (min_val, max_val, sum_val, count_val)\n",
    "\n",
    "# Initial accumulator: (min, max, sum, count)\n",
    "zero_value = (None, None, 0.0, 0)\n",
    "\n",
    "loan_amnt_stats = filtered_rdd.aggregate(zero_value, stats_seq_op, stats_comb_op)\n",
    "\n",
    "min_val, max_val, sum_val, count_val = loan_amnt_stats\n",
    "avg_val = sum_val / count_val if count_val > 0 else 0\n",
    "\n",
    "print(\"Loan Amount Statistics:\")\n",
    "print(f\"  Min: ${min_val:,.2f}\")\n",
    "print(f\"  Max: ${max_val:,.2f}\")\n",
    "print(f\"  Avg: ${avg_val:,.2f}\")\n",
    "print(f\"  Count: {count_val:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Column Statistics:\n",
      "======================================================================\n",
      "Column                   Min          Max          Avg        Count\n",
      "----------------------------------------------------------------------\n",
      "loan_amnt             500.00    40,000.00    15,010.02    2,231,965\n",
      "int_rate                5.31        30.99        13.10    2,231,965\n",
      "annual_inc              0.36 9,930,475.00    77,601.46    2,231,965\n",
      "dti                    -1.00     2,800.00        18.91    2,194,254\n",
      "fico_avg                0.72     2,271.12       700.27    2,194,763\n",
      "CPU times: user 6.62 ms, sys: 4.06 ms, total: 10.7 ms\n",
      "Wall time: 962 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute stats for multiple columns using a single pass with aggregate\n",
    "\n",
    "numeric_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_avg']\n",
    "\n",
    "def multi_stats_seq_op(acc, row):\n",
    "    \"\"\"Update stats for all numeric columns\"\"\"\n",
    "    result = dict(acc)\n",
    "    for col in numeric_cols:\n",
    "        value = row.get(col)\n",
    "        if value is not None:\n",
    "            stats = result[col]\n",
    "            min_val = min(stats[0], value) if stats[0] is not None else value\n",
    "            max_val = max(stats[1], value) if stats[1] is not None else value\n",
    "            sum_val = stats[2] + value\n",
    "            count_val = stats[3] + 1\n",
    "            result[col] = (min_val, max_val, sum_val, count_val)\n",
    "    return result\n",
    "\n",
    "def multi_stats_comb_op(acc1, acc2):\n",
    "    \"\"\"Combine stats from two accumulators\"\"\"\n",
    "    result = {}\n",
    "    for col in numeric_cols:\n",
    "        s1, s2 = acc1[col], acc2[col]\n",
    "        min_val = min(s1[0], s2[0]) if s1[0] is not None and s2[0] is not None else s1[0] or s2[0]\n",
    "        max_val = max(s1[1], s2[1]) if s1[1] is not None and s2[1] is not None else s1[1] or s2[1]\n",
    "        sum_val = s1[2] + s2[2]\n",
    "        count_val = s1[3] + s2[3]\n",
    "        result[col] = (min_val, max_val, sum_val, count_val)\n",
    "    return result\n",
    "\n",
    "zero_multi = {col: (None, None, 0.0, 0) for col in numeric_cols}\n",
    "\n",
    "multi_stats = filtered_rdd.aggregate(zero_multi, multi_stats_seq_op, multi_stats_comb_op)\n",
    "\n",
    "print(\"Numeric Column Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Column':<15} {'Min':>12} {'Max':>12} {'Avg':>12} {'Count':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for col in numeric_cols:\n",
    "    min_v, max_v, sum_v, count_v = multi_stats[col]\n",
    "    avg_v = sum_v / count_v if count_v > 0 else 0\n",
    "    print(f\"{col:<15} {min_v:>12,.2f} {max_v:>12,.2f} {avg_v:>12,.2f} {count_v:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Profiling and Tuning\n",
    "\n",
    "Let's profile and tune our MapReduce operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Partition Analysis ===\n",
      "Original RDD partitions: 18\n",
      "Cleaned RDD partitions: 18\n",
      "Filtered RDD partitions: 18\n",
      "\n",
      "--- Detailed Partition Analysis (using glom) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:========================================>               (13 + 5) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 18\n",
      "Total records: 2,231,965\n",
      "Min partition size: 43,842\n",
      "Max partition size: 143,010\n",
      "Avg partition size: 123,998\n",
      "Skew Ratio (Max/Min): 3.26x\n",
      "Data is skewed! Some partitions are much larger than others.\n",
      "\n",
      "Raw partition sizes: [81597, 82794, 131248, 132285, 132125, 133400, 133577, 132672, 133310, 131546, 136336, 134565, 135174, 142867, 143010, 139136, 132481, 43842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"=== Partition Analysis ===\")\n",
    "print(f\"Original RDD partitions: {accepted_rdd.getNumPartitions()}\")\n",
    "print(f\"Cleaned RDD partitions: {cleaned_rdd.getNumPartitions()}\")\n",
    "print(f\"Filtered RDD partitions: {filtered_rdd.getNumPartitions()}\")\n",
    "\n",
    "print(\"\\n--- Detailed Partition Analysis (using glom) ---\")\n",
    "\n",
    "# Use glom() to get the list of rows per partition\n",
    "# Then map(len) to just calculate the size of that list\n",
    "# This runs entirely on the workers, sending only 18 integers back to the driver\n",
    "partition_sizes = filtered_rdd.glom().map(len).collect()\n",
    "\n",
    "print(f\"Total partitions: {len(partition_sizes)}\")\n",
    "print(f\"Total records: {sum(partition_sizes):,}\")\n",
    "print(f\"Min partition size: {min(partition_sizes):,}\")\n",
    "print(f\"Max partition size: {max(partition_sizes):,}\")\n",
    "print(f\"Avg partition size: {sum(partition_sizes)/len(partition_sizes):,.0f}\")\n",
    "\n",
    "# Check for Skew\n",
    "skew_ratio = max(partition_sizes) / min(partition_sizes) if min(partition_sizes) > 0 else 0\n",
    "print(f\"Skew Ratio (Max/Min): {skew_ratio:.2f}x\")\n",
    "\n",
    "if skew_ratio > 1.5:\n",
    "    print(\"Data is skewed! Some partitions are much larger than others.\")\n",
    "else:\n",
    "    print(\"Data is well balanced across partitions.\")\n",
    "\n",
    "print(f\"\\nRaw partition sizes: {partition_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions:  2 | Time: 4.024s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions:  4 | Time: 2.829s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions:  8 | Time: 2.309s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 16 | Time: 2.083s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 32 | Time: 2.290s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:======================================>                (45 + 16) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 64 | Time: 2.418s\n",
      "\n",
      "Optimal partition count: 16 (time: 2.083s)\n",
      "CPU times: user 83.2 ms, sys: 40 ms, total: 123 ms\n",
      "Wall time: 16 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Experiment with different partition counts\n",
    "# Find optimal partition count based on data size\n",
    "# For ~30M records, let's try different values\n",
    "\n",
    "import time\n",
    "\n",
    "partition_tests = [2, 4, 8, 16, 32, 64]\n",
    "results = []\n",
    "\n",
    "for num_partitions in partition_tests:\n",
    "    # Repartition\n",
    "    test_rdd = filtered_rdd.repartition(num_partitions)\n",
    "    \n",
    "    # Time a simple operation\n",
    "    start = time.time()\n",
    "    \n",
    "    # Perform a MapReduce operation\n",
    "    _ = test_rdd \\\n",
    "        .map(lambda x: (x['grade'], x['loan_amnt'])) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .collect()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    results.append((num_partitions, elapsed))\n",
    "    print(f\"Partitions: {num_partitions:2d} | Time: {elapsed:.3f}s\")\n",
    "\n",
    "# Find best\n",
    "best = min(results, key=lambda x: x[1])\n",
    "print(f\"\\nOptimal partition count: {best[0]} (time: {best[1]:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RDD partitions: 16\n"
     ]
    }
   ],
   "source": [
    "# Apply optimal partitioning\n",
    "OPTIMAL_PARTITIONS = 16  # Adjust based on results above\n",
    "\n",
    "optimized_rdd = filtered_rdd.repartition(OPTIMAL_PARTITIONS)\n",
    "optimized_rdd.cache()\n",
    "\n",
    "# Force evaluation\n",
    "_ = optimized_rdd.count()\n",
    "\n",
    "print(f\"Optimized RDD partitions: {optimized_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Process Rejected Loans\n",
    "\n",
    "Apply similar cleaning to rejected loans dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected loans columns:\n",
      "  Amount Requested: 1000.0\n",
      "  Application Date: 2007-05-26\n",
      "  Debt-To-Income Ratio: 10%\n",
      "  Employment Length: 4 years\n",
      "  Loan Title: Wedding Covered but No Honeymoon\n",
      "  Policy Code: 0.0\n",
      "  Risk_Score: 693.0\n",
      "  State: NM\n",
      "  Zip Code: 481xx\n",
      "  _data_source: lending_club\n",
      "  _ingestion_timestamp: 1764368694.9139888\n",
      "  _source_file: rejected_2007_to_2018Q4.csv\n",
      "  _status: valid\n"
     ]
    }
   ],
   "source": [
    "# Check rejected loans structure\n",
    "rejected_sample = rejected_rdd.first()\n",
    "print(\"Rejected loans columns:\")\n",
    "for k, v in rejected_sample.asDict().items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rejected_loan_row(row):\n",
    "    \"\"\"\n",
    "    Clean rejected loan records using map().\n",
    "    \"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    \n",
    "    cleaned = {\n",
    "        'Amount Requested': clean_numeric(row_dict.get('Amount Requested')),\n",
    "        'Application Date': clean_string(row_dict.get('Application Date')),\n",
    "        'Loan Title': clean_string(row_dict.get('Loan Title')),\n",
    "        'Risk_Score': clean_numeric(row_dict.get('Risk_Score')),\n",
    "        'Debt-To-Income Ratio': clean_percentage(row_dict.get('Debt-To-Income Ratio')),\n",
    "        'Zip Code': clean_string(row_dict.get('Zip Code')),\n",
    "        'State': clean_string(row_dict.get('State')),\n",
    "        'Employment Length': clean_emp_length(row_dict.get('Employment Length')),\n",
    "        'Policy Code': clean_string(row_dict.get('Policy Code')),\n",
    "    }\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def is_valid_rejected_record(row_dict):\n",
    "    \"\"\"\n",
    "    Filter valid rejected records with comprehensive data quality checks.\n",
    "    Same criteria as Project.ipynb:\n",
    "    - loan_amount: not null, > 0, <= 100,000\n",
    "    - debt_to_income_ratio: not null, >= 0, <= 100\n",
    "    - risk_score: null OR (>= 300 AND <= 850)\n",
    "    \"\"\"\n",
    "    # Check loan amount\n",
    "    loan_amt = row_dict.get('Amount Requested')\n",
    "    if loan_amt is None or loan_amt <= 0 or loan_amt > 100000:\n",
    "        return False\n",
    "    \n",
    "    # Check debt-to-income ratio (must not be null and within range)\n",
    "    dti = row_dict.get('Debt-To-Income Ratio')\n",
    "    if dti is None or dti < 0 or dti > 100:\n",
    "        return False\n",
    "    \n",
    "    # Check risk score (can be null, but if present must be in valid range)\n",
    "    risk_score = row_dict.get('Risk_Score')\n",
    "    if risk_score is not None and (risk_score < 300 or risk_score > 850):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning rejected loans...\n",
      "Applying data quality filters:\n",
      "  1. Basic cleaning (percentages, strings, employment length)\n",
      "  2. Unrealistic value filtering (amount, DTI, risk_score)\n",
      "  3. Date validation (2007-2018)\n",
      "  4. Deduplication\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial records: 27,648,741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After validation filters: 25,528,260 (removed 2,120,481)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After date validation: 25,528,260 (removed 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/28 22:40:02 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 129923 ms exceeds timeout 120000 ms\n",
      "25/11/28 22:40:02 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 125104 ms exceeds timeout 120000 ms\n",
      "25/11/28 22:40:02 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.18.110: Executor heartbeat timed out after 129923 ms\n",
      "25/11/28 22:40:02 WARN TaskSetManager: Lost task 8.0 in stage 43.0 (TID 708) (192.168.18.110 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 129923 ms\n",
      "25/11/28 22:40:02 WARN TaskSetManager: Lost task 4.0 in stage 43.0 (TID 704) (192.168.18.110 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 129923 ms\n",
      "25/11/28 22:40:02 WARN TaskSetManager: Lost task 0.0 in stage 43.0 (TID 700) (192.168.18.110 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 129923 ms\n",
      "25/11/28 22:40:02 WARN TaskSetManager: Lost task 12.0 in stage 43.0 (TID 712) (192.168.18.110 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 129923 ms\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_27_6 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_2 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_10 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_113_3 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_14 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_113_7 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_27_14 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_27_10 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_113_15 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_113_11 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_6 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_27_2 !\n",
      "25/11/28 22:40:02 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.18.110: Executor heartbeat timed out after 125104 ms\n",
      "25/11/28 22:40:02 WARN TaskSetManager: Lost task 11.0 in stage 43.0 (TID 711) (192.168.18.110 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 125104 ms\n",
      "25/11/28 22:40:02 WARN TaskSetManager: Lost task 7.0 in stage 43.0 (TID 707) (192.168.18.110 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 125104 ms\n",
      "25/11/28 22:40:02 WARN TaskSetManager: Lost task 15.0 in stage 43.0 (TID 715) (192.168.18.110 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 125104 ms\n",
      "25/11/28 22:40:02 WARN TaskSetManager: Lost task 3.0 in stage 43.0 (TID 703) (192.168.18.110 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 125104 ms\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_27_1 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_113_13 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_113_5 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_13 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_27_9 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_27_5 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_27_17 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_17 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_113_9 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_27_13 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_1 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_5 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_113_1 !\n",
      "25/11/28 22:40:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_9 !\n",
      "25/11/28 22:40:02 WARN TransportChannelHandler: Exception in connection from /192.168.18.110:51066\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "[Stage 44:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: 20,116,218 (removed 5,412,042)\n",
      "\n",
      " Total removed: 7,532,523 (27.24%)\n",
      "CPU times: user 447 ms, sys: 1min 7s, total: 1min 7s\n",
      "Wall time: 9min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean rejected loans using MapReduce with comprehensive data quality checks\n",
    "\n",
    "print(\"Cleaning rejected loans...\")\n",
    "print(\"Applying data quality filters:\")\n",
    "print(\"  1. Basic cleaning (percentages, strings, employment length)\")\n",
    "print(\"  2. Unrealistic value filtering (amount, DTI, risk_score)\")\n",
    "print(\"  3. Date validation (2007-2018)\")\n",
    "print(\"  4. Deduplication\")\n",
    "\n",
    "# Step 1: Clean using map()\n",
    "cleaned_rejected_rdd = rejected_rdd.map(clean_rejected_loan_row)\n",
    "\n",
    "# Step 2: Filter unrealistic values using filter()\n",
    "initial_count = cleaned_rejected_rdd.count()\n",
    "print(f\"\\nInitial records: {initial_count:,}\")\n",
    "\n",
    "valid_rejected_rdd = cleaned_rejected_rdd.filter(is_valid_rejected_record)\n",
    "after_validation_count = valid_rejected_rdd.count()\n",
    "removed_validation = initial_count - after_validation_count\n",
    "print(f\"After validation filters: {after_validation_count:,} (removed {removed_validation:,})\")\n",
    "\n",
    "# Step 3: Date validation using filter()\n",
    "def is_valid_date_range(row_dict):\n",
    "    \"\"\"Check if application date is within 2007-2018\"\"\"\n",
    "    app_date = row_dict.get('Application Date')\n",
    "    if app_date is None:\n",
    "        return True  # Allow null dates\n",
    "    \n",
    "    try:\n",
    "        # Parse date format: \"yyyy-MM-dd\"\n",
    "        from datetime import datetime\n",
    "        date_obj = datetime.strptime(app_date, '%Y-%m-%d')\n",
    "        year = date_obj.year\n",
    "        return 2007 <= year <= 2018\n",
    "    except:\n",
    "        return False  # Invalid date format\n",
    "\n",
    "date_filtered_rdd = valid_rejected_rdd.filter(is_valid_date_range)\n",
    "after_date_count = date_filtered_rdd.count()\n",
    "removed_dates = after_validation_count - after_date_count\n",
    "print(f\"After date validation: {after_date_count:,} (removed {removed_dates:,})\")\n",
    "\n",
    "# Step 4: Deduplication using MapReduce pattern\n",
    "# Create composite key from key fields, then use reduceByKey to keep first occurrence\n",
    "def create_dedup_key(row_dict):\n",
    "    \"\"\"Create composite key for deduplication\"\"\"\n",
    "    key = (\n",
    "        row_dict.get('Amount Requested'),\n",
    "        row_dict.get('Loan Title'),\n",
    "        row_dict.get('Application Date'),\n",
    "        row_dict.get('State'),\n",
    "        row_dict.get('Zip Code')\n",
    "    )\n",
    "    return (key, row_dict)\n",
    "\n",
    "# Map to (key, row), reduceByKey to keep first, then extract row\n",
    "deduped_rdd = date_filtered_rdd \\\n",
    "    .map(create_dedup_key) \\\n",
    "    .reduceByKey(lambda a, b: a) \\\n",
    "    .map(lambda x: x[1])\n",
    "\n",
    "deduped_rdd.cache()\n",
    "\n",
    "final_rejected_count = deduped_rdd.count()\n",
    "removed_duplicates = after_date_count - final_rejected_count\n",
    "print(f\"After deduplication: {final_rejected_count:,} (removed {removed_duplicates:,})\")\n",
    "\n",
    "print(f\"\\n Total removed: {initial_count - final_rejected_count:,} ({100*(initial_count - final_rejected_count)/initial_count:.2f}%)\")\n",
    "\n",
    "# Update reference to use deduplicated RDD\n",
    "cleaned_rejected_rdd = deduped_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save to Silver Layer\n",
    "\n",
    "Convert cleaned RDDs to DataFrames and save as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for accepted loans Silver layer\n",
    "accepted_silver_schema = StructType([\n",
    "    StructField(\"loan_amnt\", FloatType(), True),\n",
    "    StructField(\"term\", IntegerType(), True),\n",
    "    StructField(\"int_rate\", FloatType(), True),\n",
    "    StructField(\"installment\", FloatType(), True),\n",
    "    StructField(\"grade\", StringType(), True),\n",
    "    StructField(\"sub_grade\", StringType(), True),\n",
    "    StructField(\"emp_length\", IntegerType(), True),\n",
    "    StructField(\"home_ownership\", StringType(), True),\n",
    "    StructField(\"annual_inc\", FloatType(), True),\n",
    "    StructField(\"verification_status\", StringType(), True),\n",
    "    StructField(\"purpose\", StringType(), True),\n",
    "    StructField(\"loan_status\", StringType(), True),\n",
    "    StructField(\"loan_status_binary\", IntegerType(), True),\n",
    "    StructField(\"issue_d\", StringType(), True),\n",
    "    StructField(\"dti\", FloatType(), True),\n",
    "    StructField(\"earliest_cr_line\", StringType(), True),\n",
    "    StructField(\"open_acc\", FloatType(), True),\n",
    "    StructField(\"pub_rec\", FloatType(), True),\n",
    "    StructField(\"revol_bal\", FloatType(), True),\n",
    "    StructField(\"revol_util\", FloatType(), True),\n",
    "    StructField(\"total_acc\", FloatType(), True),\n",
    "    StructField(\"fico_range_low\", FloatType(), True),\n",
    "    StructField(\"fico_range_high\", FloatType(), True),\n",
    "    StructField(\"addr_state\", StringType(), True),\n",
    "    StructField(\"delinq_2yrs\", FloatType(), True),\n",
    "    StructField(\"inq_last_6mths\", FloatType(), True),\n",
    "    StructField(\"mort_acc\", FloatType(), True),\n",
    "    StructField(\"pub_rec_bankruptcies\", FloatType(), True),\n",
    "    StructField(\"fico_avg\", FloatType(), True),\n",
    "    StructField(\"loan_to_income\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver DataFrame created with 30 columns\n",
      "root\n",
      " |-- loan_amnt: float (nullable = true)\n",
      " |-- term: integer (nullable = true)\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- installment: float (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_length: integer (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: float (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_status_binary: integer (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- open_acc: float (nullable = true)\n",
      " |-- pub_rec: float (nullable = true)\n",
      " |-- revol_bal: float (nullable = true)\n",
      " |-- revol_util: float (nullable = true)\n",
      " |-- total_acc: float (nullable = true)\n",
      " |-- fico_range_low: float (nullable = true)\n",
      " |-- fico_range_high: float (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- delinq_2yrs: float (nullable = true)\n",
      " |-- inq_last_6mths: float (nullable = true)\n",
      " |-- mort_acc: float (nullable = true)\n",
      " |-- pub_rec_bankruptcies: float (nullable = true)\n",
      " |-- fico_avg: float (nullable = true)\n",
      " |-- loan_to_income: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert RDD of dicts to RDD of Rows\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def dict_to_row(d):\n",
    "    \"\"\"Convert dictionary to Row with proper type handling\"\"\"\n",
    "    return Row(\n",
    "        loan_amnt=float(d['loan_amnt']) if d['loan_amnt'] is not None else None,\n",
    "        term=int(d['term']) if d['term'] is not None else None,\n",
    "        int_rate=float(d['int_rate']) if d['int_rate'] is not None else None,\n",
    "        installment=float(d['installment']) if d['installment'] is not None else None,\n",
    "        grade=d['grade'],\n",
    "        sub_grade=d['sub_grade'],\n",
    "        emp_length=int(d['emp_length']) if d['emp_length'] is not None else None,\n",
    "        home_ownership=d['home_ownership'],\n",
    "        annual_inc=float(d['annual_inc']) if d['annual_inc'] is not None else None,\n",
    "        verification_status=d['verification_status'],\n",
    "        purpose=d['purpose'],\n",
    "        loan_status=d['loan_status'],\n",
    "        loan_status_binary=int(d['loan_status_binary']) if d['loan_status_binary'] is not None else None,\n",
    "        issue_d=d['issue_d'],\n",
    "        dti=float(d['dti']) if d['dti'] is not None else None,\n",
    "        earliest_cr_line=d['earliest_cr_line'],\n",
    "        open_acc=float(d['open_acc']) if d['open_acc'] is not None else None,\n",
    "        pub_rec=float(d['pub_rec']) if d['pub_rec'] is not None else None,\n",
    "        revol_bal=float(d['revol_bal']) if d['revol_bal'] is not None else None,\n",
    "        revol_util=float(d['revol_util']) if d['revol_util'] is not None else None,\n",
    "        total_acc=float(d['total_acc']) if d['total_acc'] is not None else None,\n",
    "        fico_range_low=float(d['fico_range_low']) if d['fico_range_low'] is not None else None,\n",
    "        fico_range_high=float(d['fico_range_high']) if d['fico_range_high'] is not None else None,\n",
    "        addr_state=d['addr_state'],\n",
    "        delinq_2yrs=float(d['delinq_2yrs']) if d['delinq_2yrs'] is not None else None,\n",
    "        inq_last_6mths=float(d['inq_last_6mths']) if d['inq_last_6mths'] is not None else None,\n",
    "        mort_acc=float(d['mort_acc']) if d['mort_acc'] is not None else None,\n",
    "        pub_rec_bankruptcies=float(d['pub_rec_bankruptcies']) if d['pub_rec_bankruptcies'] is not None else None,\n",
    "        fico_avg=float(d['fico_avg']) if d['fico_avg'] is not None else None,\n",
    "        loan_to_income=float(d['loan_to_income']) if d['loan_to_income'] is not None else None,\n",
    "    )\n",
    "\n",
    "# Transform using map\n",
    "row_rdd = optimized_rdd.map(dict_to_row)\n",
    "\n",
    "# Create DataFrame\n",
    "accepted_silver_df = spark.createDataFrame(row_rdd, schema=accepted_silver_schema)\n",
    "\n",
    "print(f\"Silver DataFrame created with {len(accepted_silver_df.columns)} columns\")\n",
    "accepted_silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----------+------------------+--------+--------------+\n",
      "|loan_amnt|int_rate|grade|annual_inc|loan_status_binary|fico_avg|loan_to_income|\n",
      "+---------+--------+-----+----------+------------------+--------+--------------+\n",
      "|10000.0  |5.32    |A    |170000.0  |0                 |732.0   |0.05882353    |\n",
      "|11200.0  |11.49   |B    |34000.0   |1                 |712.0   |0.32941177    |\n",
      "|20000.0  |12.74   |C    |80000.0   |0                 |662.0   |0.25          |\n",
      "|1500.0   |10.49   |B    |20000.0   |0                 |732.0   |0.075         |\n",
      "|4800.0   |8.24    |B    |62000.0   |0                 |777.0   |0.077419356   |\n",
      "|17500.0  |10.49   |B    |80000.0   |0                 |697.0   |0.21875       |\n",
      "|29175.0  |15.99   |C    |150000.0  |0                 |692.0   |0.1945        |\n",
      "|10000.0  |13.99   |C    |52000.0   |0                 |677.0   |0.1923077     |\n",
      "|19200.0  |14.99   |C    |68000.0   |0                 |662.0   |0.28235295    |\n",
      "|3500.0   |13.49   |C    |40000.0   |0                 |682.0   |0.0875        |\n",
      "+---------+--------+-----+----------+------------------+--------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preview silver data\n",
    "accepted_silver_df.select(\n",
    "    'loan_amnt', 'int_rate', 'grade', 'annual_inc', \n",
    "    'loan_status_binary', 'fico_avg', 'loan_to_income'\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Saving Accepted Loans to Silver ===\n",
      "Removing existing directory: ../data/medallion/silver/accepted_loans\n",
      "Directory cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                        (0 + 8) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:============================>                            (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver data saved to: ../data/medallion/silver/accepted_loans\n",
      "CPU times: user 6.24 ms, sys: 17.2 ms, total: 23.5 ms\n",
      "Wall time: 8.49 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import shutil\n",
    "\n",
    "# Helper function to clean directory before saving\n",
    "def clean_output_directory(path):\n",
    "    \"\"\"Remove existing directory to prevent duplicate files.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Removing existing directory: {path}\")\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Directory cleaned.\")\n",
    "\n",
    "# Save to Silver layer\n",
    "SILVER_ACCEPTED_PATH = os.path.join(SILVER_PATH, \"accepted_loans\")\n",
    "\n",
    "print(\"=== Saving Accepted Loans to Silver ===\")\n",
    "clean_output_directory(SILVER_ACCEPTED_PATH)\n",
    "\n",
    "accepted_silver_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(SILVER_ACCEPTED_PATH)\n",
    "\n",
    "print(f\"Silver data saved to: {SILVER_ACCEPTED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saving Rejected Loans to Silver ===\n",
      "Removing existing directory: ../data/medallion/silver/rejected_loans\n",
      "Directory cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected Silver data saved to: ../data/medallion/silver/rejected_loans\n",
      "CPU times: user 10.5 ms, sys: 14.2 ms, total: 24.7 ms\n",
      "Wall time: 11.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save rejected loans Silver layer\n",
    "SILVER_REJECTED_PATH = os.path.join(SILVER_PATH, \"rejected_loans\")\n",
    "\n",
    "print(\"\\n=== Saving Rejected Loans to Silver ===\")\n",
    "clean_output_directory(SILVER_REJECTED_PATH)\n",
    "\n",
    "rejected_silver_schema = StructType([\n",
    "    StructField(\"amount_requested\", FloatType(), True),\n",
    "    StructField(\"application_date\", StringType(), True),\n",
    "    StructField(\"loan_title\", StringType(), True),\n",
    "    StructField(\"risk_score\", FloatType(), True),\n",
    "    StructField(\"dti\", FloatType(), True),\n",
    "    StructField(\"zip_code\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"emp_length\", IntegerType(), True),\n",
    "    StructField(\"policy_code\", StringType(), True),\n",
    "])\n",
    "\n",
    "def rejected_dict_to_row(d):\n",
    "    return Row(\n",
    "        amount_requested=float(d['Amount Requested']) if d['Amount Requested'] is not None else None,\n",
    "        application_date=d['Application Date'],\n",
    "        loan_title=d['Loan Title'],\n",
    "        risk_score=float(d['Risk_Score']) if d['Risk_Score'] is not None else None,\n",
    "        dti=float(d['Debt-To-Income Ratio']) if d['Debt-To-Income Ratio'] is not None else None,\n",
    "        zip_code=d['Zip Code'],\n",
    "        state=d['State'],\n",
    "        emp_length=int(d['Employment Length']) if d['Employment Length'] is not None else None,\n",
    "        policy_code=d['Policy Code'],\n",
    "    )\n",
    "\n",
    "rejected_row_rdd = cleaned_rejected_rdd.map(rejected_dict_to_row)\n",
    "rejected_silver_df = spark.createDataFrame(rejected_row_rdd, schema=rejected_silver_schema)\n",
    "\n",
    "rejected_silver_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(SILVER_REJECTED_PATH)\n",
    "\n",
    "print(f\"Rejected Silver data saved to: {SILVER_REJECTED_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verification and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Silver Layer Verification ===\n",
      "Accepted loans: 2,231,965 rows, 30 columns\n",
      "Rejected loans: 20,116,218 rows, 9 columns\n"
     ]
    }
   ],
   "source": [
    "# Verify saved data\n",
    "verify_accepted = spark.read.parquet(SILVER_ACCEPTED_PATH)\n",
    "verify_rejected = spark.read.parquet(SILVER_REJECTED_PATH)\n",
    "\n",
    "print(\"=== Silver Layer Verification ===\")\n",
    "print(f\"Accepted loans: {verify_accepted.count():,} rows, {len(verify_accepted.columns)} columns\")\n",
    "print(f\"Rejected loans: {verify_rejected.count():,} rows, {len(verify_rejected.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70M\t../data/medallion/silver/accepted_loans\n",
      "161M\t../data/medallion/silver/rejected_loans\n"
     ]
    }
   ],
   "source": [
    "# Check storage sizes\n",
    "!du -sh {SILVER_ACCEPTED_PATH}\n",
    "!du -sh {SILVER_REJECTED_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SILVER LAYER CLEANING SUMMARY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MapReduce Operations Used:\n",
      "  - map(): Data cleaning and transformation\n",
      "  - filter(): Remove invalid records\n",
      "  - flatMap(): Profile null values across columns\n",
      "  - reduceByKey(): Aggregate statistics & Deduplication\n",
      "  - aggregate(): Compute min/max/sum/count in single pass\n",
      "  - glom(): Analyze partition distribution\n",
      "  - repartition(): Optimize partitioning\n",
      "\n",
      "Data Quality Improvements:\n",
      "  - Cleaned 27 key columns\n",
      "  - Standardized date formats\n",
      "  - Converted percentages and currency values\n",
      "  - Sanitized empty strings/whitespace (Null handling)\n",
      "  - Created binary target variable for ML\n",
      "  - Added derived features (fico_avg, loan_to_income)\n",
      "\n",
      "Accepted Loans (Cleaned):\n",
      "  - Bronze Input: 2,260,701\n",
      "  - Silver Output: 2,231,965\n",
      "  - Records removed: 28,736 (1.27%)\n",
      "\n",
      "Rejected Loans (Cleaned & Deduplicated):\n",
      "  - Bronze Input: 27,648,741\n",
      "  - Silver Output: 20,116,218\n",
      "  - Records removed: 7,532,523 (27.24%)\n",
      "\n",
      "Partitioning:\n",
      "  - Optimized RDD partitions: 16\n",
      "  - Partition sizes (min/max/avg): 43,842/143,010/123998\n",
      "\n",
      "Output Paths:\n",
      "  - ../data/medallion/silver/accepted_loans\n",
      "  - ../data/medallion/silver/rejected_loans\n",
      "\n",
      "Summary generated at: 2025-11-28T22:42:47.972185\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Final summary statistics (replace existing summary block)\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SILVER LAYER CLEANING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute counts once to avoid repeated evaluation\n",
    "accepted_input = accepted_rdd.count()\n",
    "accepted_output = verify_accepted.count()\n",
    "accepted_removed = accepted_input - accepted_output\n",
    "\n",
    "rejected_input = rejected_rdd.count()\n",
    "rejected_output = verify_rejected.count()\n",
    "rejected_removed = rejected_input - rejected_output\n",
    "\n",
    "print(\"\\nMapReduce Operations Used:\")\n",
    "ops = [\n",
    "    \"map(): Data cleaning and transformation\",\n",
    "    \"filter(): Remove invalid records\",\n",
    "    \"flatMap(): Profile null values across columns\",\n",
    "    \"reduceByKey(): Aggregate statistics & Deduplication\",\n",
    "    \"aggregate(): Compute min/max/sum/count in single pass\",\n",
    "    \"glom(): Analyze partition distribution\",\n",
    "    \"repartition(): Optimize partitioning\",\n",
    "]\n",
    "for o in ops:\n",
    "    print(\"  -\", o)\n",
    "\n",
    "print(\"\\nData Quality Improvements:\")\n",
    "improvements = [\n",
    "    f\"Cleaned {len(KEY_COLUMNS)} key columns\",\n",
    "    \"Standardized date formats\",\n",
    "    \"Converted percentages and currency values\",\n",
    "    \"Sanitized empty strings/whitespace (Null handling)\",\n",
    "    \"Created binary target variable for ML\",\n",
    "    \"Added derived features (fico_avg, loan_to_income)\",\n",
    "]\n",
    "for i in improvements:\n",
    "    print(\"  -\", i)\n",
    "\n",
    "print(\"\\nAccepted Loans (Cleaned):\")\n",
    "print(f\"  - Bronze Input: {accepted_input:,}\")\n",
    "print(f\"  - Silver Output: {accepted_output:,}\")\n",
    "print(f\"  - Records removed: {accepted_removed:,} ({accepted_removed/accepted_input*100 if accepted_input>0 else 0:.2f}%)\")\n",
    "\n",
    "print(\"\\nRejected Loans (Cleaned & Deduplicated):\")\n",
    "print(f\"  - Bronze Input: {rejected_input:,}\")\n",
    "print(f\"  - Silver Output: {rejected_output:,}\")\n",
    "print(f\"  - Records removed: {rejected_removed:,} ({rejected_removed/rejected_input*100 if rejected_input>0 else 0:.2f}%)\")\n",
    "\n",
    "# Partitioning / performance summary (if available)\n",
    "try:\n",
    "    parts = optimized_rdd.getNumPartitions()\n",
    "    print(\"\\nPartitioning:\")\n",
    "    print(f\"  - Optimized RDD partitions: {parts}\")\n",
    "    if 'partition_sizes' in globals() and partition_sizes:\n",
    "        print(f\"  - Partition sizes (min/max/avg): {min(partition_sizes):,}/{max(partition_sizes):,}/{sum(partition_sizes)/len(partition_sizes):.0f}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"\\nOutput Paths:\")\n",
    "print(f\"  - {SILVER_ACCEPTED_PATH}\")\n",
    "print(f\"  - {SILVER_REJECTED_PATH}\")\n",
    "\n",
    "print(\"\\nSummary generated at:\", datetime.utcnow().isoformat())\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached RDDs unpersisted.\n"
     ]
    }
   ],
   "source": [
    "# Unpersist cached RDDs to free memory\n",
    "cleaned_rdd.unpersist()\n",
    "filtered_rdd.unpersist()\n",
    "optimized_rdd.unpersist()\n",
    "cleaned_rejected_rdd.unpersist()\n",
    "\n",
    "print(\"Cached RDDs unpersisted.\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The Silver layer is complete. The data is now cleaned and ready for analytics and ML.\n",
    "\n",
    "**Continue to:** `03_gold_serving.ipynb` for data serving using DataFrames, SQL, and MLlib."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
