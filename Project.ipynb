{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36917e24",
   "metadata": {},
   "source": [
    "# Loan Prediction - Medallion Architecture Pipeline\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a Bronze → Silver → Gold data pipeline for loan prediction analysis.\n",
    "\n",
    "### Medallion Architecture Layers:\n",
    "- **Bronze Layer**: Raw data ingestion with error handling\n",
    "- **Silver Layer**: Cleaned and typed data\n",
    "- **Gold Layer**: Business metrics and analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2d938",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cdf7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from typing import List, Tuple, Any\n",
    "import builtins\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# For Spark (will install if needed)\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import *\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql.types import *\n",
    "    pyspark_available = True\n",
    "except ImportError:\n",
    "    print(\"PySpark not available. Install with: pip install pyspark\")\n",
    "    pyspark_available = False\n",
    "\n",
    "print(\"Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57dc3ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/25 14:39:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully!\n",
      "Spark version: 3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 45900)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if pyspark_available:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"LoanPrediction\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    #spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")  # Only show errors, not warnings\n",
    "    print(\"Spark session initialized successfully!\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "else:\n",
    "    print(\"Skipping Spark tasks - PySpark not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1ee2c",
   "metadata": {},
   "source": [
    "## Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "The Bronze layer ingests raw CSV data with error handling and metadata enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d437a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bronze Layer: Ingesting Accepted and Rejected Loans ===\n",
      "\n",
      "Processing accepted loans from: data/lendingclub/accepted_2007_to_2018Q4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rejected loans from: data/lendingclub/rejected_2007_to_2018Q4.csv\n",
      "\n",
      "Converting RDDs to DataFrames...\n",
      "Combining accepted and rejected loan DataFrames...\n",
      "\n",
      "=== Bronze Layer Data Sample ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------+--------+----------+----------------+----------------+-----------+--------------+-------+------------------------+-----------------------+--------------------------+--------------------+-------------------------+-------------+-----------+-----------+----+-------------------+-----+---------+----------------+----------+--------------------+---------------+--------------+-----------+---------------+-----+---------------+------------+-----------------+-------------+----------------------------+---------------+--------------------+------------------------------+---------------+-------------------+---------------+-------------+--------------+--------+-------+--------------------+-------------------+------+------------+--------------+-----------+--------+--------+------------------+--------------------+-------------------+---------------+------------+---------+-----------+---------+----------+---------+------------------+--------------------+---------------------+--------------+--------+----------------------+---------------------------+----------------------+------------------+--------------------+------------------------+---------------------+------------------------------+------------+---------------------+--------------+---------------+-----------+---------+---------+-------------+-------------+-------------------+--------+----------------+------------+------------------+------------------+--------+-----------+-----------+-----------+-----------+-----------+-----------+------------------------------------------+---------+-------------+-----------------------+--------------+----------------+-----------+-------+--------------------+------------------+----------+----------+---------+---------------+----------+--------------------------------+----------------------------------+------------------------+-----------------------+----------------------+----------------------+----------------+-----------------------------------+---------------------+----------------+-------------------+------------------+-----------------+---------------+---------------------+-----------------+---------------+--------------------+------+---------+---------+----------+--------------------+------------+-----------+---------------+---------+-----------------+------------+--------------+-----------+--------------------------+------------------+---------------+-------------+------------------+---------------+----------------+--------------------+-------------------+-------------------------+--------+----------------+----------------+--------------------+-----------------+----------+-----------+----------+-----+--------+\n",
      "|acc_now_delinq|acc_open_past_24mths|addr_state|all_util|annual_inc|annual_inc_joint|application_type|avg_cur_bal|bc_open_to_buy|bc_util|chargeoff_within_12_mths|collection_recovery_fee|collections_12_mths_ex_med|debt_settlement_flag|debt_settlement_flag_date|deferral_term|delinq_2yrs|delinq_amnt|desc|disbursement_method|  dti|dti_joint|earliest_cr_line|emp_length|           emp_title|fico_range_high|fico_range_low|funded_amnt|funded_amnt_inv|grade|hardship_amount|hardship_dpd|hardship_end_date|hardship_flag|hardship_last_payment_amount|hardship_length|hardship_loan_status|hardship_payoff_balance_amount|hardship_reason|hardship_start_date|hardship_status|hardship_type|home_ownership|      id|il_util|      ingestion_time|initial_list_status|inq_fi|inq_last_12m|inq_last_6mths|installment|int_rate| issue_d|last_credit_pull_d|last_fico_range_high|last_fico_range_low|last_pymnt_amnt|last_pymnt_d|loan_amnt|loan_status|loan_type|max_bal_bc|member_id|mo_sin_old_il_acct|mo_sin_old_rev_tl_op|mo_sin_rcnt_rev_tl_op|mo_sin_rcnt_tl|mort_acc|mths_since_last_delinq|mths_since_last_major_derog|mths_since_last_record|mths_since_rcnt_il|mths_since_recent_bc|mths_since_recent_bc_dlq|mths_since_recent_inq|mths_since_recent_revol_delinq|next_pymnt_d|num_accts_ever_120_pd|num_actv_bc_tl|num_actv_rev_tl|num_bc_sats|num_bc_tl|num_il_tl|num_op_rev_tl|num_rev_accts|num_rev_tl_bal_gt_0|num_sats|num_tl_120dpd_2m|num_tl_30dpd|num_tl_90g_dpd_24m|num_tl_op_past_12m|open_acc|open_acc_6m|open_act_il|open_il_12m|open_il_24m|open_rv_12m|open_rv_24m|orig_projected_additional_accrued_interest|out_prncp|out_prncp_inv|payment_plan_start_date|pct_tl_nvr_dlq|percent_bc_gt_75|policy_code|pub_rec|pub_rec_bankruptcies|           purpose|pymnt_plan|recoveries|revol_bal|revol_bal_joint|revol_util|sec_app_chargeoff_within_12_mths|sec_app_collections_12_mths_ex_med|sec_app_earliest_cr_line|sec_app_fico_range_high|sec_app_fico_range_low|sec_app_inq_last_6mths|sec_app_mort_acc|sec_app_mths_since_last_major_derog|sec_app_num_rev_accts|sec_app_open_acc|sec_app_open_act_il|sec_app_revol_util|settlement_amount|settlement_date|settlement_percentage|settlement_status|settlement_term|              source|status|sub_grade|tax_liens|      term|               title|tot_coll_amt|tot_cur_bal|tot_hi_cred_lim|total_acc|total_bal_ex_mort|total_bal_il|total_bc_limit|total_cu_tl|total_il_high_credit_limit|       total_pymnt|total_pymnt_inv|total_rec_int|total_rec_late_fee|total_rec_prncp|total_rev_hi_lim|                 url|verification_status|verification_status_joint|zip_code|Amount Requested|Application Date|Debt-To-Income Ratio|Employment Length|Loan Title|Policy Code|Risk_Score|State|Zip Code|\n",
      "+--------------+--------------------+----------+--------+----------+----------------+----------------+-----------+--------------+-------+------------------------+-----------------------+--------------------------+--------------------+-------------------------+-------------+-----------+-----------+----+-------------------+-----+---------+----------------+----------+--------------------+---------------+--------------+-----------+---------------+-----+---------------+------------+-----------------+-------------+----------------------------+---------------+--------------------+------------------------------+---------------+-------------------+---------------+-------------+--------------+--------+-------+--------------------+-------------------+------+------------+--------------+-----------+--------+--------+------------------+--------------------+-------------------+---------------+------------+---------+-----------+---------+----------+---------+------------------+--------------------+---------------------+--------------+--------+----------------------+---------------------------+----------------------+------------------+--------------------+------------------------+---------------------+------------------------------+------------+---------------------+--------------+---------------+-----------+---------+---------+-------------+-------------+-------------------+--------+----------------+------------+------------------+------------------+--------+-----------+-----------+-----------+-----------+-----------+-----------+------------------------------------------+---------+-------------+-----------------------+--------------+----------------+-----------+-------+--------------------+------------------+----------+----------+---------+---------------+----------+--------------------------------+----------------------------------+------------------------+-----------------------+----------------------+----------------------+----------------+-----------------------------------+---------------------+----------------+-------------------+------------------+-----------------+---------------+---------------------+-----------------+---------------+--------------------+------+---------+---------+----------+--------------------+------------+-----------+---------------+---------+-----------------+------------+--------------+-----------+--------------------------+------------------+---------------+-------------+------------------+---------------+----------------+--------------------+-------------------+-------------------------+--------+----------------+----------------+--------------------+-----------------+----------+-----------+----------+-----+--------+\n",
      "|           0.0|                 4.0|        PA|    34.0|   55000.0|                |      Individual|    20701.0|        1506.0|   37.2|                     0.0|                    0.0|                       0.0|                   N|                         |             |        0.0|        0.0|    |               Cash| 5.91|         |        Aug-2003| 10+ years|             leadman|          679.0|         675.0|     3600.0|         3600.0|    C|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |      MORTGAGE|68407277|   36.0| 1.764081588185779E9|                  w|   3.0|         4.0|           1.0|     123.03|   13.99|Dec-2015|          Mar-2019|               564.0|              560.0|         122.67|    Jan-2019|   3600.0| Fully Paid| accepted|     722.0|         |             148.0|               128.0|                  3.0|           3.0|     1.0|                  30.0|                       30.0|                      |              21.0|                 4.0|                    69.0|                  4.0|                          69.0|            |                  2.0|           2.0|            4.0|        2.0|      5.0|      3.0|          4.0|          9.0|                4.0|     7.0|             0.0|         0.0|               0.0|               3.0|     7.0|        2.0|        2.0|        0.0|        1.0|        3.0|        3.0|                                          |      0.0|          0.0|                       |          76.9|             0.0|        1.0|    0.0|                 0.0|debt_consolidation|         n|       0.0|   2765.0|               |      29.7|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       C4|      0.0| 36 months|  Debt consolidation|       722.0|   144904.0|       178050.0|     13.0|           7746.0|      4981.0|        2400.0|        1.0|                   13734.0| 4421.723916800001|        4421.72|       821.72|               0.0|         3600.0|          9300.0|https://lendingcl...|       Not Verified|                         |   190xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "|           0.0|                 4.0|        SD|    29.0|   65000.0|                |      Individual|     9733.0|       57830.0|   27.1|                     0.0|                    0.0|                       0.0|                   N|                         |             |        1.0|        0.0|    |               Cash|16.06|         |        Dec-1999| 10+ years|            Engineer|          719.0|         715.0|    24700.0|        24700.0|    C|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |      MORTGAGE|68355089|   73.0|1.7640815881858995E9|                  w|   0.0|         6.0|           4.0|     820.28|   11.99|Dec-2015|          Mar-2019|               699.0|              695.0|         926.35|    Jun-2016|  24700.0| Fully Paid| accepted|    6472.0|         |             113.0|               192.0|                  2.0|           2.0|     4.0|                   6.0|                           |                      |              19.0|                 2.0|                        |                  0.0|                           6.0|            |                  0.0|           5.0|            5.0|       13.0|     17.0|      6.0|         20.0|         27.0|                5.0|    22.0|             0.0|         0.0|               0.0|               2.0|    22.0|        1.0|        1.0|        0.0|        1.0|        2.0|        3.0|                                          |      0.0|          0.0|                       |          97.4|             7.7|        1.0|    0.0|                 0.0|    small_business|         n|       0.0|  21470.0|               |      19.2|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       C1|      0.0| 36 months|            Business|         0.0|   204396.0|       314017.0|     38.0|          39475.0|     18005.0|       79300.0|        0.0|                   24667.0|          25679.66|       25679.66|       979.66|               0.0|        24700.0|        111800.0|https://lendingcl...|       Not Verified|                         |   577xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "|           0.0|                 6.0|        IL|    65.0|   63000.0|         71000.0|       Joint App|    31617.0|        2737.0|   55.9|                     0.0|                    0.0|                       0.0|                   N|                         |             |        0.0|        0.0|    |               Cash|10.78|    13.85|        Aug-2000| 10+ years|        truck driver|          699.0|         695.0|    20000.0|        20000.0|    B|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |      MORTGAGE|68341763|   73.0| 1.764081588185927E9|                  w|   2.0|         1.0|           0.0|     432.66|   10.78|Dec-2015|          Mar-2019|               704.0|              700.0|        15813.3|    Jun-2017|  20000.0| Fully Paid| accepted|    2081.0|         |             125.0|               184.0|                 14.0|          14.0|     5.0|                      |                           |                      |              19.0|               101.0|                        |                 10.0|                              |            |                  0.0|           2.0|            3.0|        2.0|      4.0|      6.0|          4.0|          7.0|                3.0|     6.0|             0.0|         0.0|               0.0|               0.0|     6.0|        0.0|        1.0|        0.0|        4.0|        0.0|        2.0|                                          |      0.0|          0.0|                       |         100.0|            50.0|        1.0|    0.0|                 0.0|  home_improvement|         n|       0.0|   7869.0|               |      56.2|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       B4|      0.0| 60 months|                    |         0.0|   189699.0|       218418.0|     18.0|          18696.0|     10827.0|        6200.0|        5.0|                   14877.0|22705.924293878397|       22705.92|      2705.92|               0.0|        20000.0|         14000.0|https://lendingcl...|       Not Verified|             Not Verified|   605xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "|           0.0|                 2.0|        NJ|    45.0|  110000.0|                |      Individual|    23192.0|       54962.0|   12.1|                     0.0|                    0.0|                       0.0|                   N|                         |             |        0.0|        0.0|    |               Cash|17.06|         |        Sep-2008| 10+ years|Information Syste...|          789.0|         785.0|    35000.0|        35000.0|    C|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |      MORTGAGE|66310712|   70.0|1.7640815881859806E9|                  w|   0.0|         0.0|           0.0|      829.9|   14.85|Dec-2015|          Mar-2019|               679.0|              675.0|          829.9|    Feb-2019|  35000.0|    Current| accepted|    6987.0|         |              36.0|                87.0|                  2.0|           2.0|     1.0|                      |                           |                      |              23.0|                 2.0|                        |                     |                              |    Apr-2019|                  0.0|           4.0|            5.0|        8.0|     10.0|      2.0|         10.0|         13.0|                5.0|    13.0|             0.0|         0.0|               0.0|               1.0|    13.0|        1.0|        1.0|        0.0|        1.0|        1.0|        1.0|                                          | 15897.65|     15897.65|                       |         100.0|             0.0|        1.0|    0.0|                 0.0|debt_consolidation|         n|       0.0|   7802.0|               |      11.6|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       C5|      0.0| 60 months|  Debt consolidation|         0.0|   301500.0|       381215.0|     17.0|          52226.0|     12609.0|       62500.0|        1.0|                   18000.0|          31464.01|       31464.01|     12361.66|               0.0|       19102.35|         67300.0|https://lendingcl...|    Source Verified|                         |   076xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "|           0.0|                10.0|        PA|    78.0|  104433.0|                |      Individual|    27644.0|        4567.0|   77.5|                     0.0|                    0.0|                       0.0|                   N|                         |             |        1.0|        0.0|    |               Cash|25.37|         |        Jun-1998|   3 years| Contract Specialist|          699.0|         695.0|    10400.0|        10400.0|    F|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |      MORTGAGE|68476807|   84.0|1.7640815881860042E9|                  w|   2.0|         3.0|           3.0|     289.91|   22.45|Dec-2015|          Mar-2018|               704.0|              700.0|       10128.96|    Jul-2016|  10400.0| Fully Paid| accepted|    9702.0|         |             128.0|               210.0|                  4.0|           4.0|     6.0|                  12.0|                           |                      |              14.0|                 4.0|                    12.0|                  1.0|                          12.0|            |                  0.0|           4.0|            6.0|        5.0|      9.0|     10.0|          7.0|         19.0|                6.0|    12.0|             0.0|         0.0|               0.0|               4.0|    12.0|        1.0|        3.0|        0.0|        3.0|        4.0|        7.0|                                          |      0.0|          0.0|                       |          96.6|            60.0|        1.0|    0.0|                 0.0|    major_purchase|         n|       0.0|  21929.0|               |      64.5|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       F1|      0.0| 60 months|      Major purchase|         0.0|   331730.0|       439570.0|     35.0|          95768.0|     73839.0|       20300.0|        1.0|                   88097.0|           11740.5|        11740.5|       1340.5|               0.0|        10400.0|         34000.0|https://lendingcl...|    Source Verified|                         |   174xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "|           0.0|                 0.0|        GA|    76.0|   34000.0|                |      Individual|     2560.0|         844.0|   91.0|                     0.0|                    0.0|                       0.0|                   N|                         |             |        0.0|        0.0|    |               Cash| 10.2|         |        Oct-1987|   4 years|Veterinary Tecnician|          694.0|         690.0|    11950.0|        11950.0|    C|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |          RENT|68426831|   99.0|1.7640815881860256E9|                  w|   0.0|         0.0|           0.0|     405.18|   13.44|Dec-2015|          May-2017|               759.0|              755.0|        7653.56|    May-2017|  11950.0| Fully Paid| accepted|    4522.0|         |             338.0|                54.0|                 32.0|          32.0|     0.0|                      |                           |                      |             338.0|                36.0|                        |                     |                              |            |                  0.0|           2.0|            3.0|        2.0|      2.0|      2.0|          4.0|          4.0|                3.0|     5.0|             0.0|         0.0|               0.0|               0.0|     5.0|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|                                          |      0.0|          0.0|                       |         100.0|           100.0|        1.0|    0.0|                 0.0|debt_consolidation|         n|       0.0|   8822.0|               |      68.4|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       C3|      0.0| 36 months|  Debt consolidation|         0.0|    12798.0|        16900.0|      6.0|          12798.0|      3976.0|        9400.0|        0.0|                    4000.0|  13708.9485297572|       13708.95|      1758.95|               0.0|        11950.0|         12900.0|https://lendingcl...|    Source Verified|                         |   300xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "|           0.0|                 6.0|        MN|    74.0|  180000.0|                |      Individual|    30030.0|           0.0|  102.9|                     0.0|                    0.0|                       0.0|                   N|                         |             |        0.0|        0.0|    |               Cash|14.67|         |        Jun-1990| 10+ years|Vice President of...|          684.0|         680.0|    20000.0|        20000.0|    B|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |      MORTGAGE|68476668|   63.0| 1.764081588186047E9|                  f|   1.0|         1.0|           0.0|     637.58|    9.17|Dec-2015|          Mar-2019|               654.0|              650.0|       15681.05|    Nov-2016|  20000.0| Fully Paid| accepted|   13048.0|         |             142.0|               306.0|                 10.0|          10.0|     4.0|                  49.0|                           |                      |              18.0|                12.0|                        |                 10.0|                              |            |                  0.0|           4.0|            6.0|        4.0|      5.0|      7.0|          9.0|         16.0|                6.0|    12.0|             0.0|         0.0|               0.0|               2.0|    12.0|        0.0|        2.0|        0.0|        2.0|        2.0|        3.0|                                          |      0.0|          0.0|                       |          96.3|           100.0|        1.0|    0.0|                 0.0|debt_consolidation|         n|       0.0|  87329.0|               |      84.5|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       B2|      0.0| 36 months|  Debt consolidation|         0.0|   360358.0|       388852.0|     27.0|         116762.0|     29433.0|       31500.0|        0.0|                   46452.0|   21393.800000011|        21393.8|       1393.8|               0.0|        20000.0|         94200.0|https://lendingcl...|       Not Verified|                         |   550xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "|           0.0|                 4.0|        SC|    55.0|   85000.0|                |      Individual|    17700.0|       13674.0|    5.7|                     0.0|                    0.0|                       0.0|                   N|                         |             |        1.0|        0.0|    |               Cash|17.61|         |        Feb-1999| 10+ years|         road driver|          709.0|         705.0|    20000.0|        20000.0|    B|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |      MORTGAGE|67275481|   75.0|1.7640815881861403E9|                  w|   1.0|         2.0|           0.0|     631.26|    8.49|Dec-2015|          Mar-2019|               674.0|              670.0|       14618.23|    Jan-2017|  20000.0| Fully Paid| accepted|     640.0|         |             149.0|                55.0|                 32.0|          13.0|     3.0|                   3.0|                        3.0|                      |              13.0|                32.0|                        |                  8.0|                              |            |                  1.0|           2.0|            2.0|        3.0|      3.0|      9.0|          3.0|          3.0|                2.0|     8.0|             0.0|         0.0|               1.0|               0.0|     8.0|        0.0|        3.0|        0.0|        4.0|        0.0|        0.0|                                          |      0.0|          0.0|                       |          93.3|             0.0|        1.0|    0.0|                 0.0|    major_purchase|         n|       0.0|    826.0|               |       5.7|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       B1|      0.0| 36 months|      Major purchase|         0.0|   141601.0|       193390.0|     15.0|          27937.0|     27111.0|       14500.0|        0.0|                   36144.0|   21538.508976797|       21538.51|      1538.51|               0.0|        20000.0|         14500.0|https://lendingcl...|       Not Verified|                         |   293xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "|           0.0|                 7.0|        PA|    46.0|   85000.0|                |      Individual|     1997.0|        8182.0|   50.1|                     0.0|                    0.0|                       0.0|                   N|                         |             |        0.0|        0.0|    |               Cash|13.07|         |        Apr-2002|   6 years|     SERVICE MANAGER|          689.0|         685.0|    10000.0|        10000.0|    A|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |          RENT|68466926|   57.0|1.7640815881861813E9|                  w|   2.0|         1.0|           1.0|     306.45|    6.49|Dec-2015|          Mar-2019|               719.0|              715.0|        1814.48|    Aug-2018|  10000.0| Fully Paid| accepted|    2524.0|         |             164.0|               129.0|                  1.0|           1.0|     1.0|                      |                           |                 106.0|              35.0|                 4.0|                        |                  1.0|                              |            |                  0.0|           6.0|            9.0|        7.0|     10.0|      3.0|         13.0|         19.0|                9.0|    14.0|             0.0|         0.0|               0.0|               2.0|    14.0|        2.0|        1.0|        0.0|        0.0|        2.0|        7.0|                                          |      0.0|          0.0|                       |          95.7|            28.6|        1.0|    1.0|                 1.0|       credit_card|         n|       0.0|  10464.0|               |      34.5|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       A2|      0.0| 36 months|Credit card refin...|      8341.0|    27957.0|        61099.0|     23.0|          27957.0|     17493.0|       16400.0|        0.0|                   30799.0|  10998.9715749644|       10998.97|       998.97|               0.0|        10000.0|         30300.0|https://lendingcl...|       Not Verified|                         |   160xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "|           0.0|                 5.0|        RI|    49.0|   42000.0|                |      Individual|    28528.0|        9966.0|   41.4|                     0.0|                    0.0|                       0.0|                   N|                         |             |        0.0|        0.0|    |               Cash| 34.8|         |        Nov-1994| 10+ years|      Vendor liaison|          704.0|         700.0|     8000.0|         8000.0|    B|               |            |                 |            N|                            |               |                    |                              |               |                   |               |             |      MORTGAGE|68616873|   72.0|1.7640815881862047E9|                  w|   0.0|         1.0|           0.0|     263.74|   11.48|Dec-2015|          Nov-2018|               679.0|              675.0|        4996.24|    Apr-2017|   8000.0| Fully Paid| accepted|    4725.0|         |             155.0|               253.0|                 15.0|          10.0|     1.0|                  75.0|                       75.0|                      |              10.0|                50.0|                        |                 10.0|                              |            |                  1.0|           3.0|            3.0|        3.0|      6.0|      5.0|          5.0|         11.0|                3.0|     8.0|             0.0|         0.0|               0.0|               2.0|     8.0|        0.0|        2.0|        2.0|        3.0|        0.0|        2.0|                                          |      0.0|          0.0|                       |          94.4|            33.3|        1.0|    0.0|                 0.0|       credit_card|         n|       0.0|   7034.0|               |      39.1|                                |                                  |                        |                       |                      |                      |                |                                   |                     |                |                   |                  |                 |               |                     |                 |               |accepted_2007_to_...| valid|       B5|      0.0| 36 months|Credit card refin...|         0.0|   199696.0|       256513.0|     18.0|         113782.0|    106748.0|       17000.0|        0.0|                  135513.0|   8939.5805031401|        8939.58|       939.58|               0.0|         8000.0|         18000.0|https://lendingcl...|       Not Verified|                         |   029xx|            NULL|            NULL|                NULL|             NULL|      NULL|       NULL|      NULL| NULL|    NULL|\n",
      "+--------------+--------------------+----------+--------+----------+----------------+----------------+-----------+--------------+-------+------------------------+-----------------------+--------------------------+--------------------+-------------------------+-------------+-----------+-----------+----+-------------------+-----+---------+----------------+----------+--------------------+---------------+--------------+-----------+---------------+-----+---------------+------------+-----------------+-------------+----------------------------+---------------+--------------------+------------------------------+---------------+-------------------+---------------+-------------+--------------+--------+-------+--------------------+-------------------+------+------------+--------------+-----------+--------+--------+------------------+--------------------+-------------------+---------------+------------+---------+-----------+---------+----------+---------+------------------+--------------------+---------------------+--------------+--------+----------------------+---------------------------+----------------------+------------------+--------------------+------------------------+---------------------+------------------------------+------------+---------------------+--------------+---------------+-----------+---------+---------+-------------+-------------+-------------------+--------+----------------+------------+------------------+------------------+--------+-----------+-----------+-----------+-----------+-----------+-----------+------------------------------------------+---------+-------------+-----------------------+--------------+----------------+-----------+-------+--------------------+------------------+----------+----------+---------+---------------+----------+--------------------------------+----------------------------------+------------------------+-----------------------+----------------------+----------------------+----------------+-----------------------------------+---------------------+----------------+-------------------+------------------+-----------------+---------------+---------------------+-----------------+---------------+--------------------+------+---------+---------+----------+--------------------+------------+-----------+---------------+---------+-----------------+------------+--------------+-----------+--------------------------+------------------+---------------+-------------+------------------+---------------+----------------+--------------------+-------------------+-------------------------+--------+----------------+----------------+--------------------+-----------------+----------+-----------+----------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "=== Data Quality Metrics (Approximate) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size (10%): 2,991,157\n",
      "  - Accepted loans (sample): 226,766 (7.6%)\n",
      "  - Rejected loans (sample): 2,764,391 (92.4%)\n",
      "\n",
      "Estimated total records: ~29,911,570\n",
      "\n",
      "Saving to Bronze layer (this may take several minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Bronze data saved to data/medallion/bronze (partitioned by loan_type)\n",
      "\n",
      "=== Verification: Reading from Bronze Layer ===\n",
      "Total records saved: 29,909,442\n",
      "  - Accepted loans: 2,260,701 (7.6%)\n",
      "  - Rejected loans: 27,648,741 (92.4%)\n",
      "\n",
      "=== Available Columns ===\n",
      "Total columns: 164\n",
      "First 20 columns: ['acc_now_delinq', 'acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc', 'annual_inc_joint', 'application_type', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'collection_recovery_fee', 'collections_12_mths_ex_med', 'debt_settlement_flag', 'debt_settlement_flag_date', 'deferral_term', 'delinq_2yrs', 'delinq_amnt', 'desc', 'disbursement_method']\n"
     ]
    }
   ],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Bronze Layer: Ingesting Accepted and Rejected Loans ===\\n\")\n",
    "\n",
    "    # Define file paths\n",
    "    accepted_path = \"data/lendingclub/accepted_2007_to_2018Q4.csv\"\n",
    "    rejected_path = \"data/lendingclub/rejected_2007_to_2018Q4.csv\"\n",
    "    \n",
    "    def process_loan_file(file_path, loan_type):\n",
    "        \"\"\"Process a loan CSV file and return parsed RDD with metadata\"\"\"\n",
    "        print(f\"Processing {loan_type} loans from: {file_path}\")\n",
    "        \n",
    "        # Read the file\n",
    "        raw_rdd = spark.sparkContext.textFile(file_path)\n",
    "        header = raw_rdd.first()\n",
    "        header_cols = header.split(\",\")\n",
    "        \n",
    "        # Filter out header and parse rows\n",
    "        data_rdd = raw_rdd.filter(lambda row: row != header)\n",
    "        \n",
    "        def parse_safe(row):\n",
    "            try:\n",
    "                parts = row.split(\",\")\n",
    "                # Create a dictionary with all columns\n",
    "                record = {}\n",
    "                for i, col_name in enumerate(header_cols):\n",
    "                    record[col_name] = parts[i] if i < len(parts) else None\n",
    "                \n",
    "                # Add metadata\n",
    "                record[\"loan_type\"] = loan_type\n",
    "                record[\"ingestion_time\"] = time.time()\n",
    "                record[\"source\"] = file_path.split(\"/\")[-1]\n",
    "                record[\"status\"] = \"valid\"\n",
    "                \n",
    "                return record\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"raw_data\": row,\n",
    "                    \"loan_type\": loan_type,\n",
    "                    \"ingestion_time\": time.time(),\n",
    "                    \"source\": file_path.split(\"/\")[-1],\n",
    "                    \"status\": \"parse_error\",\n",
    "                    \"error_message\": str(e)\n",
    "                }\n",
    "        \n",
    "        parsed_rdd = data_rdd.map(parse_safe)\n",
    "        return parsed_rdd\n",
    "    \n",
    "    # Process both files using RDDs\n",
    "    accepted_rdd = process_loan_file(accepted_path, \"accepted\")\n",
    "    rejected_rdd = process_loan_file(rejected_path, \"rejected\")\n",
    "    \n",
    "    # Convert each RDD to DataFrame SEPARATELY to preserve their schemas\n",
    "    print(\"\\nConverting RDDs to DataFrames...\")\n",
    "    accepted_df = spark.createDataFrame(accepted_rdd)\n",
    "    rejected_df = spark.createDataFrame(rejected_rdd)\n",
    "    \n",
    "    # Union DataFrames (this properly merges schemas with all columns)\n",
    "    print(\"Combining accepted and rejected loan DataFrames...\")\n",
    "    bronze_df = accepted_df.unionByName(rejected_df, allowMissingColumns=True)\n",
    "    \n",
    "    print(\"\\n=== Bronze Layer Data Sample ===\")\n",
    "    # Sample first to avoid processing entire dataset\n",
    "    bronze_df.show(10, truncate=True)\n",
    "    \n",
    "    # Data quality metrics - use sampling for efficiency\n",
    "    print(\"\\n=== Data Quality Metrics (Approximate) ===\")\n",
    "    \n",
    "    # Sample 10% of data for quick metrics\n",
    "    sample_df = bronze_df.sample(fraction=0.1, seed=42)\n",
    "    sample_count = sample_df.count()\n",
    "    \n",
    "    accepted_sample = sample_df.filter(col(\"loan_type\") == \"accepted\").count()\n",
    "    rejected_sample = sample_df.filter(col(\"loan_type\") == \"rejected\").count()\n",
    "    \n",
    "    print(f\"Sample size (10%): {sample_count:,}\")\n",
    "    print(f\"  - Accepted loans (sample): {accepted_sample:,} ({accepted_sample/sample_count*100:.1f}%)\")\n",
    "    print(f\"  - Rejected loans (sample): {rejected_sample:,} ({rejected_sample/sample_count*100:.1f}%)\")\n",
    "    print(f\"\\nEstimated total records: ~{sample_count * 10:,}\")\n",
    "    \n",
    "    # Save to Bronze layer with partitioning by loan_type\n",
    "    # This is the main operation - write directly without intermediate steps\n",
    "    print(\"\\nSaving to Bronze layer (this may take several minutes)...\")\n",
    "    bronze_df.write.mode(\"overwrite\").partitionBy(\"loan_type\").parquet(\"data/medallion/bronze\")\n",
    "    print(\"\\n✅ Bronze data saved to data/medallion/bronze (partitioned by loan_type)\")\n",
    "    \n",
    "    # Now read back from parquet for verification (more efficient)\n",
    "    print(\"\\n=== Verification: Reading from Bronze Layer ===\")\n",
    "    bronze_saved = spark.read.parquet(\"data/medallion/bronze\")\n",
    "    \n",
    "    total_records = bronze_saved.count()\n",
    "    accepted_total = bronze_saved.filter(col(\"loan_type\") == \"accepted\").count()\n",
    "    rejected_total = bronze_saved.filter(col(\"loan_type\") == \"rejected\").count()\n",
    "    \n",
    "    print(f\"Total records saved: {total_records:,}\")\n",
    "    print(f\"  - Accepted loans: {accepted_total:,} ({accepted_total/total_records*100:.1f}%)\")\n",
    "    print(f\"  - Rejected loans: {rejected_total:,} ({rejected_total/total_records*100:.1f}%)\")\n",
    "    \n",
    "    # Show available columns\n",
    "    print(\"\\n=== Available Columns ===\")\n",
    "    all_columns = bronze_saved.columns\n",
    "    print(f\"Total columns: {len(all_columns)}\")\n",
    "    print(\"First 20 columns:\", all_columns[:20])\n",
    "    \n",
    "    # Update bronze_df to point to saved data for downstream use\n",
    "    bronze_df = bronze_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb588e",
   "metadata": {},
   "source": [
    "## Silver Layer - Data Cleaning and Standardization\n",
    "\n",
    "The Silver layer cleanses data, enforces types, and validates quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48238a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Silver Layer: Cleaned and Standardized Data ===\n",
      "\n",
      "Processing accepted and rejected loans separately due to different schemas...\n",
      "\n",
      "=== Processing Accepted Loans ===\n",
      "Total accepted loans before cleaning: 2,260,701\n",
      "\n",
      "=== DATA QUALITY CHECKS ===\n",
      "\n",
      "1. Checking for duplicate loan IDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total records: 2,260,701\n",
      "   Unique loan IDs: 2,260,701\n",
      "   Duplicates: 0\n",
      "\n",
      "2. Filtering unrealistic values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Removed 69,023 records with unrealistic values\n",
      "   Criteria: loan_amount (0-100K), annual_income (0-10M), DTI (0-100%), interest_rate (0-35%)\n",
      "\n",
      "3. Validating dates (issue_date 2007-2018)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Removed 0 records with out-of-range dates\n",
      "\n",
      "4. Removing duplicate loan IDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Removed 0 duplicate records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Accepted loans after all cleaning: 2,191,678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total removed: 69,023 (3.05%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-----------+-------------+-----------------------+-------------+-----------+-----+---------+----------+-----------+------------------+------------------+----------------+-------------+-------------------+-----------------+-----------------+--------------+-------------------+-------------------------+-----+--------+--------------------+--------------------------+--------------+---------------+-------------------+--------------------+--------------------+------------------+------------------+-----------------------+-----------------------------+----------------------------+------------------------+--------------------+------------------+--------------------------------+-------------+--------------+---------------------------+-----------------+-----------------------+---------------------+----------------------+---------------------------+---------------------------------+---------------------+----------------------------+----------------------------------+--------------------+--------------------+------------------------+-------------------------+-----------------------+--------------+--------------------------+---------+-----------------------------+-----------------+-------------------------+---------------------+-----------------------+--------------------+--------------------------+---------------------------------+-----------------------+---------------+--------------------+----------------------+-------------------------------+----------+-----------------+-----------------+---------------------+----------------+-----------------------+------------------------+-----------------------+-----------------------+-------------------+---------------------+-------------------------------+----------+-----------------------+-------------+-------------+---------------+--------------------+--------------------+-----------------+-----------------+---------------------+----------------------------+-----------------------------+----------------------------------+----------------------------------+-------------------------------+---------------------------+-----------------------------------+------------------------------------+---------------------+\n",
      "|  loan_id|member_id|loan_type|loan_amount|funded_amount|funded_amount_investors|interest_rate|installment|grade|sub_grade|      term|loan_status|           purpose|        loan_title|application_type|annual_income|annual_income_joint|employment_length| employment_title|home_ownership|verification_status|verification_status_joint|state|zip_code|debt_to_income_ratio|debt_to_income_ratio_joint|fico_range_low|fico_range_high|last_fico_range_low|last_fico_range_high|earliest_credit_line|delinquencies_2yrs|delinquency_amount|accounts_now_delinquent|months_since_last_delinquency|months_since_last_derogatory|months_since_last_record|inquiries_last_6mths|inquiries_last_12m|inquiries_financial_institutions|open_accounts|total_accounts|accounts_opened_past_24mths|revolving_balance|revolving_balance_joint|revolving_utilization|num_revolving_accounts|num_active_revolving_trades|num_revolving_trades_balance_gt_0|num_bankcard_accounts|num_active_bankcard_accounts|num_satisfactory_bankcard_accounts|bankcard_open_to_buy|bankcard_utilization|num_installment_accounts|total_balance_installment|installment_utilization|public_records|public_record_bankruptcies|tax_liens|collections_12mths_ex_medical|mortgage_accounts|total_balance_ex_mortgage|total_current_balance|total_high_credit_limit|total_bankcard_limit|total_il_high_credit_limit|total_revolving_high_credit_limit|average_current_balance|all_utilization|max_balance_bankcard|percent_bankcard_gt_75|percent_trades_never_delinquent|issue_date|last_payment_date|next_payment_date|last_credit_pull_date|   total_payment|total_payment_investors|total_principal_received|total_interest_received|total_late_fee_received|last_payment_amount|outstanding_principal|outstanding_principal_investors|recoveries|collection_recovery_fee|hardship_flag|hardship_type|hardship_status|hardship_loan_status|debt_settlement_flag|settlement_status|settlement_amount|settlement_percentage|secondary_app_fico_range_low|secondary_app_fico_range_high|secondary_app_earliest_credit_line|secondary_app_inquiries_last_6mths|secondary_app_mortgage_accounts|secondary_app_open_accounts|secondary_app_revolving_utilization|secondary_app_num_revolving_accounts|silver_processed_time|\n",
      "+---------+---------+---------+-----------+-------------+-----------------------+-------------+-----------+-----+---------+----------+-----------+------------------+------------------+----------------+-------------+-------------------+-----------------+-----------------+--------------+-------------------+-------------------------+-----+--------+--------------------+--------------------------+--------------+---------------+-------------------+--------------------+--------------------+------------------+------------------+-----------------------+-----------------------------+----------------------------+------------------------+--------------------+------------------+--------------------------------+-------------+--------------+---------------------------+-----------------+-----------------------+---------------------+----------------------+---------------------------+---------------------------------+---------------------+----------------------------+----------------------------------+--------------------+--------------------+------------------------+-------------------------+-----------------------+--------------+--------------------------+---------+-----------------------------+-----------------+-------------------------+---------------------+-----------------------+--------------------+--------------------------+---------------------------------+-----------------------+---------------+--------------------+----------------------+-------------------------------+----------+-----------------+-----------------+---------------------+----------------+-----------------------+------------------------+-----------------------+-----------------------+-------------------+---------------------+-------------------------------+----------+-----------------------+-------------+-------------+---------------+--------------------+--------------------+-----------------+-----------------+---------------------+----------------------------+-----------------------------+----------------------------------+----------------------------------+-------------------------------+---------------------------+-----------------------------------+------------------------------------+---------------------+\n",
      "|100001671|         | accepted|     3000.0|       3000.0|                 3000.0|         6.99|      92.62|    A|       A2| 36 months|    Current|  home_improvement|  Home improvement|      Individual|      35000.0|               NULL|        10+ years|          Cleaner|      MORTGAGE|       Not Verified|                         |   MN|   553xx|               15.05|                      NULL|           725|            729|                780|                 784|            Oct-2000|                 0|               0.0|                      0|                         NULL|                        NULL|                     106|                   0|                 0|                               0|            7|            18|                          3|           3501.0|                   NULL|                 22.6|                     9|                          4|                                4|                    5|                           3|                                 3|              2881.0|                43.5|                       9|                   4482.0|                   50.0|             1|                         1|        0|                            0|                0|                   7983.0|               7983.0|                24500.0|              5100.0|                    9000.0|                          15500.0|                 1140.0|           33.0|              1496.0|                   0.0|                          100.0|  Mar-2017|         Mar-2019|         Apr-2019|             Mar-2019|         2220.55|                2220.55|                 1929.61|                 290.94|                    0.0|              92.62|              1070.39|                        1070.39|       0.0|                    0.0|            N|             |               |                    |                   N|                 |             NULL|                 NULL|                        NULL|                         NULL|                                  |                              NULL|                           NULL|                       NULL|                               NULL|                                NULL| 2025-11-25 14:48:...|\n",
      "|100002366|         | accepted|    25000.0|      25000.0|                25000.0|        13.49|     848.27|    C|       C2| 36 months| Fully Paid|debt_consolidation|Debt consolidation|      Individual|      60000.0|               NULL|          2 years|   Loan Processor|      MORTGAGE|       Not Verified|                         |   NV|   891xx|               17.36|                      NULL|           710|            714|                680|                 684|            Apr-1997|                 0|               0.0|                      0|                         NULL|                        NULL|                    NULL|                   0|                 1|                               0|           25|            40|                          6|          21487.0|                   NULL|                 15.9|                    37|                         12|                               12|                   21|                           6|                                15|             94295.0|                13.3|                       1|                      0.0|                   NULL|             0|                         0|        0|                            0|                2|                  21487.0|             265853.0|               453000.0|            108800.0|                       0.0|                         135000.0|                10634.0|           16.0|              5721.0|                   0.0|                          100.0|  Mar-2017|         Apr-2017|                 |             Mar-2018|25215.4675000012|               25215.47|                 25000.0|                 215.47|                    0.0|           25252.94|                  0.0|                            0.0|       0.0|                    0.0|            N|             |               |                    |                   N|                 |             NULL|                 NULL|                        NULL|                         NULL|                                  |                              NULL|                           NULL|                       NULL|                               NULL|                                NULL| 2025-11-25 14:48:...|\n",
      "|100002529|         | accepted|    35000.0|      35000.0|                35000.0|        15.99|     850.95|    C|       C5| 60 months|    Current|debt_consolidation|Debt consolidation|      Individual|     120000.0|               NULL|          5 years|          manager|      MORTGAGE|    Source Verified|                         |   NM|   878xx|               11.32|                      NULL|           660|            664|                  0|                 499|            Sep-2005|                 0|               0.0|                      0|                           62|                          62|                    NULL|                   0|                 0|                               0|            5|            11|                          4|           3156.0|                   NULL|                 17.3|                     5|                          3|                                3|                    4|                           3|                                 3|             15044.0|                17.3|                       4|                  27214.0|                   91.0|             0|                         0|        0|                            0|                2|                  30370.0|             163156.0|               185000.0|             18200.0|                   30000.0|                          18200.0|                32631.0|           63.0|              1492.0|                   0.0|                           81.8|  Mar-2017|         Mar-2019|         Apr-2019|             Mar-2019|        20403.17|               20403.17|                10784.41|                9576.21|                  42.55|             850.95|             24215.59|                       24215.59|       0.0|                    0.0|            N|             |               |                    |                   N|                 |             NULL|                 NULL|                        NULL|                         NULL|                                  |                              NULL|                           NULL|                       NULL|                               NULL|                                NULL| 2025-11-25 14:48:...|\n",
      "|100002680|         | accepted|    35000.0|      35000.0|                35000.0|        23.99|    1006.68|    E|       E2| 60 months|    Current|debt_consolidation|Debt consolidation|      Individual|      79000.0|               NULL|         < 1 year|Interior Designer|           OWN|           Verified|                         |   MA|   010xx|               27.94|                      NULL|           670|            674|                690|                 694|            Jul-2008|                 1|               0.0|                      0|                           19|                          19|                    NULL|                   0|                 5|                               1|           11|            29|                          1|           7511.0|                   NULL|                 70.2|                    10|                          3|                                3|                    5|                           2|                                 2|               450.0|                92.9|                      19|                 121747.0|                  104.0|             0|                         0|        0|                            0|                0|                 129258.0|             129258.0|               132304.0|              6300.0|                  121604.0|                          10700.0|                11751.0|          100.0|              3542.0|                 100.0|                           92.9|  Mar-2017|         Mar-2019|         Apr-2019|             Mar-2019|        24113.67|               24113.67|                  9337.7|               14775.97|                    0.0|            1006.68|              25662.3|                        25662.3|       0.0|                    0.0|            N|             |               |                    |                   N|                 |             NULL|                 NULL|                        NULL|                         NULL|                                  |                              NULL|                           NULL|                       NULL|                               NULL|                                NULL| 2025-11-25 14:48:...|\n",
      "|100002911|         | accepted|     6500.0|       6500.0|                 6500.0|        11.49|     214.32|    B|       B5| 36 months|    Current|debt_consolidation|Debt consolidation|      Individual|      34600.0|               NULL|          3 years|           editor|      MORTGAGE|    Source Verified|                         |   AL|   367xx|               22.34|                      NULL|           665|            669|                695|                 699|            Dec-1996|                 1|               0.0|                      0|                            6|                          41|                      63|                   0|                 3|                               1|           10|            25|                          8|           4840.0|                   NULL|                 69.1|                     6|                          6|                                6|                    3|                           3|                                 3|               411.0|                82.9|                      17|                  75013.0|                   90.0|             2|                         0|        2|                            0|                2|                  79853.0|              84146.0|                92272.0|              2400.0|                   57187.0|                           7000.0|                 8415.0|           86.0|               932.0|                  66.7|                           47.8|  Mar-2017|         Mar-2019|         Apr-2019|             Mar-2019|         5139.53|                5139.53|                 4081.58|                1057.95|                    0.0|             214.32|              2418.42|                        2418.42|       0.0|                    0.0|            N|             |               |                    |                   N|                 |             NULL|                 NULL|                        NULL|                         NULL|                                  |                              NULL|                           NULL|                       NULL|                               NULL|                                NULL| 2025-11-25 14:48:...|\n",
      "+---------+---------+---------+-----------+-------------+-----------------------+-------------+-----------+-----+---------+----------+-----------+------------------+------------------+----------------+-------------+-------------------+-----------------+-----------------+--------------+-------------------+-------------------------+-----+--------+--------------------+--------------------------+--------------+---------------+-------------------+--------------------+--------------------+------------------+------------------+-----------------------+-----------------------------+----------------------------+------------------------+--------------------+------------------+--------------------------------+-------------+--------------+---------------------------+-----------------+-----------------------+---------------------+----------------------+---------------------------+---------------------------------+---------------------+----------------------------+----------------------------------+--------------------+--------------------+------------------------+-------------------------+-----------------------+--------------+--------------------------+---------+-----------------------------+-----------------+-------------------------+---------------------+-----------------------+--------------------+--------------------------+---------------------------------+-----------------------+---------------+--------------------+----------------------+-------------------------------+----------+-----------------+-----------------+---------------------+----------------+-----------------------+------------------------+-----------------------+-----------------------+-------------------+---------------------+-------------------------------+----------+-----------------------+-------------+-------------+---------------+--------------------+--------------------+-----------------+-----------------+---------------------+----------------------------+-----------------------------+----------------------------------+----------------------------------+-------------------------------+---------------------------+-----------------------------------+------------------------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Processing Rejected Loans ===\n",
      "Analyzing rejected loan data structure...\n",
      "  Amount Requested: 1000.0\n",
      "  Application Date: 2007-05-26\n",
      "  Debt-To-Income Ratio: 10%\n",
      "  Employment Length: 4 years\n",
      "  Loan Title: Wedding Covered but No Honeymoon\n",
      "  Policy Code: 0.0\n",
      "  Risk_Score: 693.0\n",
      "  State: NM\n",
      "  Zip Code: 481xx\n",
      "\n",
      "Total columns with data: 9\n",
      "\n",
      "=== Column Mapping Results ===\n",
      "Amount Requested -> Amount Requested\n",
      "Application Date -> Application Date\n",
      "Loan Title -> Loan Title\n",
      "Risk_Score -> Risk_Score\n",
      "Debt-To-Income Ratio -> Debt-To-Income Ratio\n",
      "Zip Code -> Zip Code\n",
      "State -> State\n",
      "Employment Length -> Employment Length\n",
      "Policy Code -> Policy Code\n",
      "\n",
      "=== DATA QUALITY CHECKS FOR REJECTED LOANS ===\n",
      "\n",
      "1. Checking for duplicate applications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total records: 27,648,741\n",
      "   Unique applications: 10,723,228\n",
      "   Potential duplicates: 16,925,513\n",
      "\n",
      "2. Filtering unrealistic values and null DTI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Removed 2,120,481 records with unrealistic values or null DTI\n",
      "   Criteria: loan_amount (0-100K), DTI not null and (0-100%), risk_score (300-850 or null)\n",
      "\n",
      "3. Validating application dates (2007-2018)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Removed 0 records with out-of-range dates\n",
      "\n",
      "4. Removing exact duplicate applications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Removed 5,412,025 duplicate records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Rejected loans after all cleaning: 20,116,235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total removed: 7,532,506 (27.24%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+--------------------+----------+--------------------+--------+-----+-----------------+----------------+-----------+-------------+-------------+-----------+-----+---------+----+-----------+-------------+--------------+-------------------+---------------------+\n",
      "|loan_id|loan_type|loan_amount|          loan_title|risk_score|debt_to_income_ratio|zip_code|state|employment_length|application_date|policy_code|funded_amount|interest_rate|installment|grade|sub_grade|term|loan_status|annual_income|home_ownership|verification_status|silver_processed_time|\n",
      "+-------+---------+-----------+--------------------+----------+--------------------+--------+-----+-----------------+----------------+-----------+-------------+-------------+-----------+-----+---------+----+-----------+-------------+--------------+-------------------+---------------------+\n",
      "|  74390| rejected|    15000.0|  \"Paying Back Bills|      NULL|                 0.0|      0%|483xx|               MI|      2009-03-06|    8 years|         NULL|         NULL|       NULL| NULL|     NULL|NULL|   Rejected|         NULL|          NULL|               NULL| 2025-11-25 14:54:...|\n",
      "|  58444| rejected|     5000.0| \"Need $5000 for car|      NULL|                 0.0|      0%|633xx|               MO|      2008-12-16|    2 years|         NULL|         NULL|       NULL| NULL|     NULL|NULL|   Rejected|         NULL|          NULL|               NULL| 2025-11-25 14:54:...|\n",
      "|  22752| rejected|     2000.0|\"I am approxamatl...|      NULL|                 0.0|      0%|843xx|               UT|      2008-03-17|    3 years|         NULL|         NULL|       NULL| NULL|     NULL|NULL|   Rejected|         NULL|          NULL|               NULL| 2025-11-25 14:54:...|\n",
      "| 118204| rejected|     5500.0|    \"To pay off debt|      NULL|                 0.0|      0%|981xx|               WA|      2009-06-23|   < 1 year|         NULL|         NULL|       NULL| NULL|     NULL|NULL|   Rejected|         NULL|          NULL|               NULL| 2025-11-25 14:54:...|\n",
      "|4398444| rejected|     1000.0|          Green loan|      NULL|               100.0|   000xx|   AK|         < 1 year|      2017-05-21|        0.0|         NULL|         NULL|       NULL| NULL|     NULL|NULL|   Rejected|         NULL|          NULL|               NULL| 2025-11-25 14:54:...|\n",
      "+-------+---------+-----------+--------------------+----------+--------------------+--------+-----+-----------------+----------------+-----------+-------------+-------------+-----------+-----+---------+----+-----------+-------------+--------------+-------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Saving to Silver Layer ===\n",
      "Saving accepted loans to data/medallion/silver/accepted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving rejected loans to data/medallion/silver/rejected...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Silver layer saved successfully!\n",
      "  - Accepted loans: data/medallion/silver/accepted\n",
      "  - Rejected loans: data/medallion/silver/rejected\n",
      "\n",
      "=== Silver Layer Data Quality Summary ===\n",
      "\n",
      "Accepted Loans: 2,191,678 records\n",
      "Key columns null counts:\n",
      "+-----------------+-------------------+-------------------+--------------------------+\n",
      "|loan_amount_nulls|interest_rate_nulls|annual_income_nulls|debt_to_income_ratio_nulls|\n",
      "+-----------------+-------------------+-------------------+--------------------------+\n",
      "|                0|                  0|                  0|                         0|\n",
      "+-----------------+-------------------+-------------------+--------------------------+\n",
      "\n",
      "\n",
      "Rejected Loans: 20,116,235 records\n",
      "Key columns null counts:\n",
      "+-----------------+----------------+--------------------------+\n",
      "|loan_amount_nulls|risk_score_nulls|debt_to_income_ratio_nulls|\n",
      "+-----------------+----------------+--------------------------+\n",
      "|                0|        12837284|                         0|\n",
      "+-----------------+----------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Silver Layer: Cleaned and Standardized Data ===\\n\")\n",
    "    \n",
    "    # Read from Bronze layer\n",
    "    bronze_df = spark.read.parquet(\"data/medallion/bronze\")\n",
    "    \n",
    "    # Filter only valid records\n",
    "    valid_bronze_df = bronze_df.filter(col(\"status\") == \"valid\")\n",
    "    \n",
    "    print(\"Processing accepted and rejected loans separately due to different schemas...\\n\")\n",
    "    \n",
    "    # ===== ACCEPTED LOANS PROCESSING =====\n",
    "    print(\"=== Processing Accepted Loans ===\")\n",
    "    accepted_df = valid_bronze_df.filter(col(\"loan_type\") == \"accepted\")\n",
    "    \n",
    "    print(f\"Total accepted loans before cleaning: {accepted_df.count():,}\")\n",
    "    \n",
    "    # Clean and standardize accepted loans with expanded columns\n",
    "    accepted_silver = accepted_df.select(\n",
    "        # Core loan identifiers\n",
    "        col(\"id\").cast(StringType()).alias(\"loan_id\"),\n",
    "        col(\"member_id\").cast(StringType()),\n",
    "        col(\"loan_type\"),\n",
    "        \n",
    "        # Loan amounts and rates\n",
    "        col(\"loan_amnt\").cast(DoubleType()).alias(\"loan_amount\"),\n",
    "        col(\"funded_amnt\").cast(DoubleType()).alias(\"funded_amount\"),\n",
    "        col(\"funded_amnt_inv\").cast(DoubleType()).alias(\"funded_amount_investors\"),\n",
    "        regexp_replace(col(\"int_rate\"), \"%\", \"\").cast(DoubleType()).alias(\"interest_rate\"),\n",
    "        col(\"installment\").cast(DoubleType()),\n",
    "        \n",
    "        # Loan classification\n",
    "        col(\"grade\").cast(StringType()),\n",
    "        col(\"sub_grade\").cast(StringType()),\n",
    "        col(\"term\").cast(StringType()),\n",
    "        col(\"loan_status\").cast(StringType()),\n",
    "        col(\"purpose\").cast(StringType()),\n",
    "        col(\"title\").cast(StringType()).alias(\"loan_title\"),\n",
    "        col(\"application_type\").cast(StringType()),\n",
    "        \n",
    "        # Borrower information\n",
    "        col(\"annual_inc\").cast(DoubleType()).alias(\"annual_income\"),\n",
    "        col(\"annual_inc_joint\").cast(DoubleType()).alias(\"annual_income_joint\"),\n",
    "        col(\"emp_length\").cast(StringType()).alias(\"employment_length\"),\n",
    "        col(\"emp_title\").cast(StringType()).alias(\"employment_title\"),\n",
    "        col(\"home_ownership\").cast(StringType()),\n",
    "        col(\"verification_status\").cast(StringType()),\n",
    "        col(\"verification_status_joint\").cast(StringType()),\n",
    "        col(\"addr_state\").cast(StringType()).alias(\"state\"),\n",
    "        col(\"zip_code\").cast(StringType()),\n",
    "        \n",
    "        # Credit information - Basic\n",
    "        col(\"dti\").cast(DoubleType()).alias(\"debt_to_income_ratio\"),\n",
    "        col(\"dti_joint\").cast(DoubleType()).alias(\"debt_to_income_ratio_joint\"),\n",
    "        col(\"fico_range_low\").cast(IntegerType()),\n",
    "        col(\"fico_range_high\").cast(IntegerType()),\n",
    "        col(\"last_fico_range_low\").cast(IntegerType()),\n",
    "        col(\"last_fico_range_high\").cast(IntegerType()),\n",
    "        col(\"earliest_cr_line\").cast(StringType()).alias(\"earliest_credit_line\"),\n",
    "        \n",
    "        # Credit information - Delinquencies\n",
    "        col(\"delinq_2yrs\").cast(IntegerType()).alias(\"delinquencies_2yrs\"),\n",
    "        col(\"delinq_amnt\").cast(DoubleType()).alias(\"delinquency_amount\"),\n",
    "        col(\"acc_now_delinq\").cast(IntegerType()).alias(\"accounts_now_delinquent\"),\n",
    "        col(\"mths_since_last_delinq\").cast(IntegerType()).alias(\"months_since_last_delinquency\"),\n",
    "        col(\"mths_since_last_major_derog\").cast(IntegerType()).alias(\"months_since_last_derogatory\"),\n",
    "        col(\"mths_since_last_record\").cast(IntegerType()).alias(\"months_since_last_record\"),\n",
    "        \n",
    "        # Credit information - Inquiries and Accounts\n",
    "        col(\"inq_last_6mths\").cast(IntegerType()).alias(\"inquiries_last_6mths\"),\n",
    "        col(\"inq_last_12m\").cast(IntegerType()).alias(\"inquiries_last_12m\"),\n",
    "        col(\"inq_fi\").cast(IntegerType()).alias(\"inquiries_financial_institutions\"),\n",
    "        col(\"open_acc\").cast(IntegerType()).alias(\"open_accounts\"),\n",
    "        col(\"total_acc\").cast(IntegerType()).alias(\"total_accounts\"),\n",
    "        col(\"acc_open_past_24mths\").cast(IntegerType()).alias(\"accounts_opened_past_24mths\"),\n",
    "        \n",
    "        # Credit information - Revolving Credit\n",
    "        col(\"revol_bal\").cast(DoubleType()).alias(\"revolving_balance\"),\n",
    "        col(\"revol_bal_joint\").cast(DoubleType()).alias(\"revolving_balance_joint\"),\n",
    "        regexp_replace(col(\"revol_util\"), \"%\", \"\").cast(DoubleType()).alias(\"revolving_utilization\"),\n",
    "        col(\"num_rev_accts\").cast(IntegerType()).alias(\"num_revolving_accounts\"),\n",
    "        col(\"num_actv_rev_tl\").cast(IntegerType()).alias(\"num_active_revolving_trades\"),\n",
    "        col(\"num_rev_tl_bal_gt_0\").cast(IntegerType()).alias(\"num_revolving_trades_balance_gt_0\"),\n",
    "        \n",
    "        # Credit information - Bankcard and Installment\n",
    "        col(\"num_bc_tl\").cast(IntegerType()).alias(\"num_bankcard_accounts\"),\n",
    "        col(\"num_actv_bc_tl\").cast(IntegerType()).alias(\"num_active_bankcard_accounts\"),\n",
    "        col(\"num_bc_sats\").cast(IntegerType()).alias(\"num_satisfactory_bankcard_accounts\"),\n",
    "        col(\"bc_open_to_buy\").cast(DoubleType()).alias(\"bankcard_open_to_buy\"),\n",
    "        regexp_replace(col(\"bc_util\"), \"%\", \"\").cast(DoubleType()).alias(\"bankcard_utilization\"),\n",
    "        col(\"num_il_tl\").cast(IntegerType()).alias(\"num_installment_accounts\"),\n",
    "        col(\"total_bal_il\").cast(DoubleType()).alias(\"total_balance_installment\"),\n",
    "        col(\"il_util\").cast(DoubleType()).alias(\"installment_utilization\"),\n",
    "        \n",
    "        # Credit information - Public Records\n",
    "        col(\"pub_rec\").cast(IntegerType()).alias(\"public_records\"),\n",
    "        col(\"pub_rec_bankruptcies\").cast(IntegerType()).alias(\"public_record_bankruptcies\"),\n",
    "        col(\"tax_liens\").cast(IntegerType()),\n",
    "        col(\"collections_12_mths_ex_med\").cast(IntegerType()).alias(\"collections_12mths_ex_medical\"),\n",
    "        \n",
    "        # Credit information - Mortgage\n",
    "        col(\"mort_acc\").cast(IntegerType()).alias(\"mortgage_accounts\"),\n",
    "        col(\"total_bal_ex_mort\").cast(DoubleType()).alias(\"total_balance_ex_mortgage\"),\n",
    "        \n",
    "        # Credit information - Advanced Metrics\n",
    "        col(\"tot_cur_bal\").cast(DoubleType()).alias(\"total_current_balance\"),\n",
    "        col(\"tot_hi_cred_lim\").cast(DoubleType()).alias(\"total_high_credit_limit\"),\n",
    "        col(\"total_bc_limit\").cast(DoubleType()).alias(\"total_bankcard_limit\"),\n",
    "        col(\"total_il_high_credit_limit\").cast(DoubleType()),\n",
    "        col(\"total_rev_hi_lim\").cast(DoubleType()).alias(\"total_revolving_high_credit_limit\"),\n",
    "        col(\"avg_cur_bal\").cast(DoubleType()).alias(\"average_current_balance\"),\n",
    "        col(\"all_util\").cast(DoubleType()).alias(\"all_utilization\"),\n",
    "        col(\"max_bal_bc\").cast(DoubleType()).alias(\"max_balance_bankcard\"),\n",
    "        col(\"percent_bc_gt_75\").cast(DoubleType()).alias(\"percent_bankcard_gt_75\"),\n",
    "        col(\"pct_tl_nvr_dlq\").cast(DoubleType()).alias(\"percent_trades_never_delinquent\"),\n",
    "        \n",
    "        # Payment information - Dates\n",
    "        col(\"issue_d\").cast(StringType()).alias(\"issue_date\"),\n",
    "        col(\"last_pymnt_d\").cast(StringType()).alias(\"last_payment_date\"),\n",
    "        col(\"next_pymnt_d\").cast(StringType()).alias(\"next_payment_date\"),\n",
    "        col(\"last_credit_pull_d\").cast(StringType()).alias(\"last_credit_pull_date\"),\n",
    "        \n",
    "        # Payment information - Amounts\n",
    "        col(\"total_pymnt\").cast(DoubleType()).alias(\"total_payment\"),\n",
    "        col(\"total_pymnt_inv\").cast(DoubleType()).alias(\"total_payment_investors\"),\n",
    "        col(\"total_rec_prncp\").cast(DoubleType()).alias(\"total_principal_received\"),\n",
    "        col(\"total_rec_int\").cast(DoubleType()).alias(\"total_interest_received\"),\n",
    "        col(\"total_rec_late_fee\").cast(DoubleType()).alias(\"total_late_fee_received\"),\n",
    "        col(\"last_pymnt_amnt\").cast(DoubleType()).alias(\"last_payment_amount\"),\n",
    "        col(\"out_prncp\").cast(DoubleType()).alias(\"outstanding_principal\"),\n",
    "        col(\"out_prncp_inv\").cast(DoubleType()).alias(\"outstanding_principal_investors\"),\n",
    "        \n",
    "        # Collections and Recovery\n",
    "        col(\"recoveries\").cast(DoubleType()),\n",
    "        col(\"collection_recovery_fee\").cast(DoubleType()),\n",
    "        \n",
    "        # Hardship and Settlement\n",
    "        col(\"hardship_flag\").cast(StringType()),\n",
    "        col(\"hardship_type\").cast(StringType()),\n",
    "        col(\"hardship_status\").cast(StringType()),\n",
    "        col(\"hardship_loan_status\").cast(StringType()),\n",
    "        col(\"debt_settlement_flag\").cast(StringType()),\n",
    "        col(\"settlement_status\").cast(StringType()),\n",
    "        col(\"settlement_amount\").cast(DoubleType()),\n",
    "        col(\"settlement_percentage\").cast(DoubleType()),\n",
    "        \n",
    "        # Secondary Applicant (for joint loans)\n",
    "        col(\"sec_app_fico_range_low\").cast(IntegerType()).alias(\"secondary_app_fico_range_low\"),\n",
    "        col(\"sec_app_fico_range_high\").cast(IntegerType()).alias(\"secondary_app_fico_range_high\"),\n",
    "        col(\"sec_app_earliest_cr_line\").cast(StringType()).alias(\"secondary_app_earliest_credit_line\"),\n",
    "        col(\"sec_app_inq_last_6mths\").cast(IntegerType()).alias(\"secondary_app_inquiries_last_6mths\"),\n",
    "        col(\"sec_app_mort_acc\").cast(IntegerType()).alias(\"secondary_app_mortgage_accounts\"),\n",
    "        col(\"sec_app_open_acc\").cast(IntegerType()).alias(\"secondary_app_open_accounts\"),\n",
    "        col(\"sec_app_revol_util\").cast(DoubleType()).alias(\"secondary_app_revolving_utilization\"),\n",
    "        col(\"sec_app_num_rev_accts\").cast(IntegerType()).alias(\"secondary_app_num_revolving_accounts\"),\n",
    "        \n",
    "        # Add processing metadata\n",
    "        lit(current_timestamp()).alias(\"silver_processed_time\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== DATA QUALITY CHECKS ===\")\n",
    "    \n",
    "    # 1. Check for duplicates\n",
    "    print(\"\\n1. Checking for duplicate loan IDs...\")\n",
    "    total_loans = accepted_silver.count()\n",
    "    unique_loans = accepted_silver.select(\"loan_id\").distinct().count()\n",
    "    duplicates = total_loans - unique_loans\n",
    "    print(f\"   Total records: {total_loans:,}\")\n",
    "    print(f\"   Unique loan IDs: {unique_loans:,}\")\n",
    "    print(f\"   Duplicates: {duplicates:,}\")\n",
    "    \n",
    "    # 2. Filter unrealistic values\n",
    "    print(\"\\n2. Filtering unrealistic values...\")\n",
    "    accepted_clean = accepted_silver \\\n",
    "        .filter(col(\"loan_amount\").isNotNull()) \\\n",
    "        .filter(col(\"loan_amount\") > 0) \\\n",
    "        .filter(col(\"loan_amount\") <= 100000) \\\n",
    "        .filter(col(\"annual_income\").isNotNull()) \\\n",
    "        .filter(col(\"annual_income\") > 0) \\\n",
    "        .filter(col(\"annual_income\") <= 10000000) \\\n",
    "        .filter(col(\"debt_to_income_ratio\").isNotNull()) \\\n",
    "        .filter(col(\"debt_to_income_ratio\") >= 0) \\\n",
    "        .filter(col(\"debt_to_income_ratio\") <= 100) \\\n",
    "        .filter((col(\"interest_rate\").isNull()) | ((col(\"interest_rate\") >= 0) & (col(\"interest_rate\") <= 35)))\n",
    "    \n",
    "    unrealistic_count = total_loans - accepted_clean.count()\n",
    "    print(f\"   Removed {unrealistic_count:,} records with unrealistic values\")\n",
    "    print(f\"   Criteria: loan_amount (0-100K), annual_income (0-10M), DTI (0-100%), interest_rate (0-35%)\")\n",
    "    \n",
    "    # 3. Date validation (issue_date should be 2007-2018)\n",
    "    print(\"\\n3. Validating dates (issue_date 2007-2018)...\")\n",
    "    from pyspark.sql.functions import to_date, year\n",
    "    accepted_with_dates = accepted_clean.withColumn(\"issue_year\", year(to_date(col(\"issue_date\"), \"MMM-yyyy\")))\n",
    "    \n",
    "    accepted_final = accepted_with_dates \\\n",
    "        .filter((col(\"issue_year\").isNull()) | ((col(\"issue_year\") >= 2007) & (col(\"issue_year\") <= 2018))) \\\n",
    "        .drop(\"issue_year\")\n",
    "    \n",
    "    date_invalid_count = accepted_clean.count() - accepted_final.count()\n",
    "    print(f\"   Removed {date_invalid_count:,} records with out-of-range dates\")\n",
    "    \n",
    "    # 4. Remove duplicates (keep first occurrence)\n",
    "    print(\"\\n4. Removing duplicate loan IDs...\")\n",
    "    from pyspark.sql import Window\n",
    "    window_spec = Window.partitionBy(\"loan_id\").orderBy(col(\"silver_processed_time\"))\n",
    "    accepted_deduped = accepted_final \\\n",
    "        .withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "        .filter(col(\"row_num\") == 1) \\\n",
    "        .drop(\"row_num\")\n",
    "    \n",
    "    dedup_removed = accepted_final.count() - accepted_deduped.count()\n",
    "    print(f\"   Removed {dedup_removed:,} duplicate records\")\n",
    "    \n",
    "    # Final dataset\n",
    "    accepted_silver = accepted_deduped\n",
    "    \n",
    "    print(f\"\\n✅ Accepted loans after all cleaning: {accepted_silver.count():,}\")\n",
    "    print(f\"   Total removed: {total_loans - accepted_silver.count():,} ({100*(total_loans - accepted_silver.count())/total_loans:.2f}%)\")\n",
    "    \n",
    "    accepted_silver.show(5, truncate=True)\n",
    "    \n",
    "    # ===== REJECTED LOANS PROCESSING =====\n",
    "    print(\"\\n=== Processing Rejected Loans ===\")\n",
    "    rejected_df = valid_bronze_df.filter(col(\"loan_type\") == \"rejected\")\n",
    "    \n",
    "    # DEBUGGING: Find which columns actually contain data\n",
    "    print(\"Analyzing rejected loan data structure...\")\n",
    "    sample_row = rejected_df.first()\n",
    "    \n",
    "    if sample_row is None:\n",
    "        print(\"ERROR: No rejected loans found in Bronze layer!\")\n",
    "        raise ValueError(\"No rejected loan records found\")\n",
    "    \n",
    "    # Check all columns for non-null/non-empty values\n",
    "    non_null_cols = []\n",
    "    for col_name in rejected_df.columns:\n",
    "        val = sample_row[col_name]\n",
    "        if val is not None and str(val).strip() != '' and col_name not in ['ingestion_time', 'status', 'source', 'loan_type']:\n",
    "            non_null_cols.append(col_name)\n",
    "            print(f\"  {col_name}: {val}\")\n",
    "    \n",
    "    print(f\"\\nTotal columns with data: {len(non_null_cols)}\")\n",
    "    \n",
    "    # The rejected CSV has these columns:\n",
    "    # \"Amount Requested,Application Date,Loan Title,Risk_Score,Debt-To-Income Ratio,Zip Code,State,Employment Length,Policy Code\"\n",
    "    # Let's try to map them by searching for similar column names\n",
    "    \n",
    "    # Expected rejected columns from CSV - map to actual parquet column names\n",
    "    from typing import Dict, Optional\n",
    "    rejected_col_mapping: Dict[str, Optional[str]] = {\n",
    "        \"Amount Requested\": None,\n",
    "        \"Application Date\": None,\n",
    "        \"Loan Title\": None,\n",
    "        \"Risk_Score\": None,\n",
    "        \"Debt-To-Income Ratio\": None,\n",
    "        \"Zip Code\": None,\n",
    "        \"State\": None,\n",
    "        \"Employment Length\": None,\n",
    "        \"Policy Code\": None\n",
    "    }\n",
    "    \n",
    "    # Search for matches (case-insensitive, handle spaces/underscores)\n",
    "    for csv_col in rejected_col_mapping.keys():\n",
    "        # Try exact match first\n",
    "        if csv_col in rejected_df.columns:\n",
    "            rejected_col_mapping[csv_col] = csv_col\n",
    "            continue\n",
    "        \n",
    "        # Try with underscores instead of spaces\n",
    "        col_with_underscores = csv_col.replace(\" \", \"_\")\n",
    "        if col_with_underscores in rejected_df.columns:\n",
    "            rejected_col_mapping[csv_col] = col_with_underscores\n",
    "            continue\n",
    "        \n",
    "        # Try lowercase with underscores\n",
    "        col_lower = csv_col.lower().replace(\" \", \"_\")\n",
    "        if col_lower in rejected_df.columns:\n",
    "            rejected_col_mapping[csv_col] = col_lower\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n=== Column Mapping Results ===\")\n",
    "    for csv_col, parquet_col in rejected_col_mapping.items():\n",
    "        print(f\"{csv_col} -> {parquet_col}\")\n",
    "    \n",
    "    # Clean and standardize rejected loans using the mapped columns\n",
    "    rejected_silver = rejected_df.select(\n",
    "        # Generate a unique ID for rejected loans\n",
    "        monotonically_increasing_id().cast(StringType()).alias(\"loan_id\"),\n",
    "        col(\"loan_type\"),\n",
    "        \n",
    "        # Map rejected loan columns\n",
    "        col(rejected_col_mapping[\"Amount Requested\"]).cast(DoubleType()).alias(\"loan_amount\") if rejected_col_mapping[\"Amount Requested\"] else lit(None).cast(DoubleType()).alias(\"loan_amount\"),\n",
    "        col(rejected_col_mapping[\"Loan Title\"]).cast(StringType()).alias(\"loan_title\") if rejected_col_mapping[\"Loan Title\"] else lit(None).cast(StringType()).alias(\"loan_title\"),\n",
    "        col(rejected_col_mapping[\"Risk_Score\"]).cast(DoubleType()).alias(\"risk_score\") if rejected_col_mapping[\"Risk_Score\"] else lit(None).cast(DoubleType()).alias(\"risk_score\"),\n",
    "        regexp_replace(col(rejected_col_mapping[\"Debt-To-Income Ratio\"]), \"%\", \"\").cast(DoubleType()).alias(\"debt_to_income_ratio\") if rejected_col_mapping[\"Debt-To-Income Ratio\"] else lit(None).cast(DoubleType()).alias(\"debt_to_income_ratio\"),\n",
    "        col(rejected_col_mapping[\"Zip Code\"]).cast(StringType()).alias(\"zip_code\") if rejected_col_mapping[\"Zip Code\"] else lit(None).cast(StringType()).alias(\"zip_code\"),\n",
    "        col(rejected_col_mapping[\"State\"]).cast(StringType()).alias(\"state\") if rejected_col_mapping[\"State\"] else lit(None).cast(StringType()).alias(\"state\"),\n",
    "        col(rejected_col_mapping[\"Employment Length\"]).cast(StringType()).alias(\"employment_length\") if rejected_col_mapping[\"Employment Length\"] else lit(None).cast(StringType()).alias(\"employment_length\"),\n",
    "        col(rejected_col_mapping[\"Application Date\"]).cast(StringType()).alias(\"application_date\") if rejected_col_mapping[\"Application Date\"] else lit(None).cast(StringType()).alias(\"application_date\"),\n",
    "        col(rejected_col_mapping[\"Policy Code\"]).cast(StringType()).alias(\"policy_code\") if rejected_col_mapping[\"Policy Code\"] else lit(None).cast(StringType()).alias(\"policy_code\"),\n",
    "        \n",
    "        # Rejected loans don't have these fields - set to null\n",
    "        lit(None).cast(DoubleType()).alias(\"funded_amount\"),\n",
    "        lit(None).cast(DoubleType()).alias(\"interest_rate\"),\n",
    "        lit(None).cast(DoubleType()).alias(\"installment\"),\n",
    "        lit(None).cast(StringType()).alias(\"grade\"),\n",
    "        lit(None).cast(StringType()).alias(\"sub_grade\"),\n",
    "        lit(None).cast(StringType()).alias(\"term\"),\n",
    "        lit(\"Rejected\").cast(StringType()).alias(\"loan_status\"),\n",
    "        lit(None).cast(DoubleType()).alias(\"annual_income\"),\n",
    "        lit(None).cast(StringType()).alias(\"home_ownership\"),\n",
    "        lit(None).cast(StringType()).alias(\"verification_status\"),\n",
    "        \n",
    "        # Add processing metadata\n",
    "        lit(current_timestamp()).alias(\"silver_processed_time\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== DATA QUALITY CHECKS FOR REJECTED LOANS ===\")\n",
    "    \n",
    "    # 1. Check for duplicates (rejected loans don't have unique IDs, so check by all fields)\n",
    "    print(\"\\n1. Checking for duplicate applications...\")\n",
    "    total_rejected = rejected_silver.count()\n",
    "    unique_rejected = rejected_silver.select(\"loan_amount\", \"loan_title\", \"application_date\", \"state\").distinct().count()\n",
    "    duplicates_rejected = total_rejected - unique_rejected\n",
    "    print(f\"   Total records: {total_rejected:,}\")\n",
    "    print(f\"   Unique applications: {unique_rejected:,}\")\n",
    "    print(f\"   Potential duplicates: {duplicates_rejected:,}\")\n",
    "    \n",
    "    # 2. Filter unrealistic values and nulls\n",
    "    print(\"\\n2. Filtering unrealistic values and null DTI...\")\n",
    "    rejected_clean = rejected_silver \\\n",
    "        .filter(col(\"loan_amount\").isNotNull()) \\\n",
    "        .filter(col(\"loan_amount\") > 0) \\\n",
    "        .filter(col(\"loan_amount\") <= 100000) \\\n",
    "        .filter(col(\"debt_to_income_ratio\").isNotNull()) \\\n",
    "        .filter(col(\"debt_to_income_ratio\") >= 0) \\\n",
    "        .filter(col(\"debt_to_income_ratio\") <= 100) \\\n",
    "        .filter((col(\"risk_score\").isNull()) | ((col(\"risk_score\") >= 300) & (col(\"risk_score\") <= 850)))\n",
    "    \n",
    "    unrealistic_rejected = total_rejected - rejected_clean.count()\n",
    "    print(f\"   Removed {unrealistic_rejected:,} records with unrealistic values or null DTI\")\n",
    "    print(f\"   Criteria: loan_amount (0-100K), DTI not null and (0-100%), risk_score (300-850 or null)\")\n",
    "    \n",
    "    # 3. Date validation\n",
    "    print(\"\\n3. Validating application dates (2007-2018)...\")\n",
    "    from pyspark.sql.functions import to_date, year\n",
    "    rejected_with_dates = rejected_clean.withColumn(\"app_year\", year(to_date(col(\"application_date\"), \"yyyy-MM-dd\")))\n",
    "    \n",
    "    rejected_final = rejected_with_dates \\\n",
    "        .filter((col(\"app_year\").isNull()) | ((col(\"app_year\") >= 2007) & (col(\"app_year\") <= 2018))) \\\n",
    "        .drop(\"app_year\")\n",
    "    \n",
    "    date_invalid_rejected = rejected_clean.count() - rejected_final.count()\n",
    "    print(f\"   Removed {date_invalid_rejected:,} records with out-of-range dates\")\n",
    "    \n",
    "    # 4. Remove exact duplicates (keep first occurrence)\n",
    "    print(\"\\n4. Removing exact duplicate applications...\")\n",
    "    rejected_deduped = rejected_final.dropDuplicates([\"loan_amount\", \"loan_title\", \"application_date\", \"state\", \"zip_code\"])\n",
    "    \n",
    "    dedup_removed_rejected = rejected_final.count() - rejected_deduped.count()\n",
    "    print(f\"   Removed {dedup_removed_rejected:,} duplicate records\")\n",
    "    \n",
    "    # Final dataset\n",
    "    rejected_silver = rejected_deduped\n",
    "    \n",
    "    print(f\"\\n✅ Rejected loans after all cleaning: {rejected_silver.count():,}\")\n",
    "    print(f\"   Total removed: {total_rejected - rejected_silver.count():,} ({100*(total_rejected - rejected_silver.count())/total_rejected:.2f}%)\")\n",
    "    \n",
    "    rejected_silver.show(5, truncate=True)\n",
    "    \n",
    "    # ===== SAVE BOTH TO SILVER LAYER =====\n",
    "    print(\"\\n=== Saving to Silver Layer ===\")\n",
    "    \n",
    "    # Save accepted loans\n",
    "    print(\"Saving accepted loans to data/medallion/silver/accepted...\")\n",
    "    accepted_silver.write.mode(\"overwrite\").parquet(\"data/medallion/silver/accepted\")\n",
    "    \n",
    "    # Save rejected loans  \n",
    "    print(\"Saving rejected loans to data/medallion/silver/rejected...\")\n",
    "    rejected_silver.write.mode(\"overwrite\").parquet(\"data/medallion/silver/rejected\")\n",
    "    \n",
    "    print(\"\\n✅ Silver layer saved successfully!\")\n",
    "    print(\"  - Accepted loans: data/medallion/silver/accepted\")\n",
    "    print(\"  - Rejected loans: data/medallion/silver/rejected\")\n",
    "    \n",
    "    # ===== DATA QUALITY SUMMARY =====\n",
    "    print(\"\\n=== Silver Layer Data Quality Summary ===\")\n",
    "    \n",
    "    # Read back for verification\n",
    "    accepted_silver_saved = spark.read.parquet(\"data/medallion/silver/accepted\")\n",
    "    rejected_silver_saved = spark.read.parquet(\"data/medallion/silver/rejected\")\n",
    "    \n",
    "    print(f\"\\nAccepted Loans: {accepted_silver_saved.count():,} records\")\n",
    "    print(\"Key columns null counts:\")\n",
    "    accepted_silver_saved.select([\n",
    "        F.count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\") \n",
    "        for c in [\"loan_amount\", \"interest_rate\", \"annual_income\", \"debt_to_income_ratio\"]\n",
    "    ]).show()\n",
    "    \n",
    "    print(f\"\\nRejected Loans: {rejected_silver_saved.count():,} records\")\n",
    "    print(\"Key columns null counts:\")\n",
    "    rejected_silver_saved.select([\n",
    "        F.count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\") \n",
    "        for c in [\"loan_amount\", \"risk_score\", \"debt_to_income_ratio\"]\n",
    "    ]).show()\n",
    "    \n",
    "    # Store for Gold layer (use accepted loans for analysis)\n",
    "    silver_df = accepted_silver_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817e208",
   "metadata": {},
   "source": [
    "## Gold Layer - Business Metrics and Analytics\n",
    "\n",
    "The Gold layer creates aggregated business metrics for analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e2397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gold Layer: Business Analytics and ML ===\n",
      "\n",
      "Loading Silver layer data...\n",
      "✅ Accepted loans: 2,191,678 records\n",
      "✅ Rejected loans: 20,116,235 records\n",
      "\n",
      "✅ SQL tables registered for querying\n",
      "\n",
      "=== 1. Loan Status Distribution Analysis ===\n",
      "\n",
      "Loan Status Summary:\n",
      "+---------------------------------------------------+-----------+-----------------+---------------+-----------------+-----------------+-------+\n",
      "|loan_status                                        |total_loans|total_loan_amount|avg_loan_amount|avg_interest_rate|avg_annual_income|avg_dti|\n",
      "+---------------------------------------------------+-----------+-----------------+---------------+-----------------+-----------------+-------+\n",
      "|Fully Paid                                         |1028139    |1.45026608E10    |14105.74       |12.62            |77488.41         |17.82  |\n",
      "|Current                                            |867772     |1.3787017825E10  |15887.83       |12.77            |80357.57         |19.15  |\n",
      "|Charged Off                                        |260335     |4.046536275E9    |15543.57       |15.72            |70341.45         |20.14  |\n",
      "|Late (31-120 days)                                 |21267      |3.5950295E8      |16904.26       |15.63            |77061.15         |19.6   |\n",
      "|In Grace Period                                    |8338       |1.4692345E8      |17620.95       |15.46            |81143.54         |19.99  |\n",
      "|Late (16-30 days)                                  |4313       |7.492365E7       |17371.59       |15.39            |79292.45         |19.19  |\n",
      "|Does not meet the credit policy. Status:Fully Paid |1079       |8842000.0        |8194.62        |13.86            |71536.46         |14.33  |\n",
      "|Does not meet the credit policy. Status:Charged Off|396        |3698525.0        |9339.71        |14.59            |67672.59         |14.42  |\n",
      "|Default                                            |39         |564025.0         |14462.18       |16.25            |73412.1          |17.22  |\n",
      "+---------------------------------------------------+-----------+-----------------+---------------+-----------------+-----------------+-------+\n",
      "\n",
      "✅ Saved to data/medallion/gold/status_summary\n",
      "\n",
      "=== 2. Default Risk Analysis by Loan Grade ===\n",
      "\n",
      "Default Analysis by Loan Grade:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+--------+----------------+---------------+-----------------+----------+\n",
      "|grade|sub_grade|total_loans|defaults|default_rate_pct|avg_loan_amount|avg_interest_rate|avg_income|\n",
      "+-----+---------+-----------+--------+----------------+---------------+-----------------+----------+\n",
      "|A    |A1       |84193      |1358    |1.61            |15284.76       |5.6              |101394.41 |\n",
      "|A    |A2       |67304      |1657    |2.46            |14223.68       |6.55             |90365.9   |\n",
      "|A    |A3       |70650      |1984    |2.81            |14117.75       |7.09             |87777.83  |\n",
      "|A    |A4       |92160      |3415    |3.71            |14891.61       |7.55             |86758.69  |\n",
      "|A    |A5       |103720     |5155    |4.97            |14373.13       |8.18             |84257.46  |\n",
      "|B    |B1       |121568     |7159    |5.89            |14188.17       |9.06             |82759.93  |\n",
      "|B    |B2       |122397     |8087    |6.61            |14415.35       |9.95             |80621.05  |\n",
      "|B    |B3       |126397     |10175   |8.05            |14126.35       |10.68            |78189.46  |\n",
      "|B    |B4       |135062     |11838   |8.76            |14170.56       |11.35            |76540.13  |\n",
      "|B    |B5       |136269     |13376   |9.82            |13881.35       |12.0             |74982.45  |\n",
      "|C    |C1       |142003     |15758   |11.10           |14452.97       |12.77            |75496.56  |\n",
      "|C    |C2       |127522     |15948   |12.51           |14542.73       |13.52            |74182.89  |\n",
      "|C    |C3       |125965     |16427   |13.04           |15167.64       |14.09            |74624.47  |\n",
      "|C    |C4       |124081     |18124   |14.61           |15519.64       |14.87            |73669.02  |\n",
      "|C    |C5       |113985     |17103   |15.00           |15513.92       |15.76            |73056.7   |\n",
      "|D    |D1       |79671      |13890   |17.43           |15291.25       |16.66            |70801.68  |\n",
      "|D    |D2       |70930      |12908   |18.20           |15331.24       |17.61            |70747.55  |\n",
      "|D    |D3       |63016      |11686   |18.54           |15665.1        |18.41            |70687.46  |\n",
      "|D    |D4       |55187      |11196   |20.29           |16024.5        |19.1             |71458.46  |\n",
      "|D    |D5       |46581      |9707    |20.84           |16599.78       |20.1             |70824.54  |\n",
      "|E    |E1       |32594      |8341    |25.59           |16578.27       |20.37            |71129.33  |\n",
      "|E    |E2       |28950      |7801    |26.95           |16976.42       |21.06            |71227.02  |\n",
      "|E    |E3       |25833      |6924    |26.80           |17516.75       |21.93            |70829.69  |\n",
      "|E    |E4       |21998      |6111    |27.78           |18119.0        |22.84            |72416.9   |\n",
      "|E    |E5       |21958      |5843    |26.61           |18285.35       |24.18            |72579.24  |\n",
      "|F    |F1       |12884      |4063    |31.54           |18719.9        |24.65            |71604.04  |\n",
      "|F    |F2       |8906       |3151    |35.38           |19047.27       |25.11            |72704.29  |\n",
      "|F    |F3       |7468       |2656    |35.57           |18834.84       |25.9             |71797.35  |\n",
      "|F    |F4       |5858       |2241    |38.26           |19169.99       |26.5             |71711.87  |\n",
      "|F    |F5       |4963       |1882    |37.92           |20058.8        |27.25            |73304.17  |\n",
      "|G    |G1       |3918       |1377    |35.15           |20301.28       |28.03            |73916.29  |\n",
      "|G    |G2       |2571       |1010    |39.28           |19539.26       |27.82            |72931.76  |\n",
      "|G    |G3       |1991       |802     |40.28           |20643.81       |28.32            |75238.47  |\n",
      "|G    |G4       |1624       |640     |39.41           |20781.57       |28.86            |74517.81  |\n",
      "|G    |G5       |1501       |581     |38.71           |20838.87       |29.06            |76830.19  |\n",
      "+-----+---------+-----------+--------+----------------+---------------+-----------------+----------+\n",
      "\n",
      "\n",
      "Summary by Main Grade:\n",
      "+-----+-----------+--------+----------------+--------+\n",
      "|grade|total_loans|defaults|default_rate_pct|avg_rate|\n",
      "+-----+-----------+--------+----------------+--------+\n",
      "|    A|     418027|   13569|            3.25|    7.07|\n",
      "|    B|     641693|   50635|            7.89|   10.65|\n",
      "|    C|     633556|   83360|           13.16|   14.13|\n",
      "|    D|     315385|   59387|           18.83|   18.16|\n",
      "|    E|     131333|   35020|           26.67|   21.88|\n",
      "|    F|      40079|   13993|           34.91|   25.58|\n",
      "|    G|      11605|    4410|           38.00|   28.28|\n",
      "+-----+-----------+--------+----------------+--------+\n",
      "\n",
      "✅ Saved to data/medallion/gold/grade_analysis\n",
      "\n",
      "=== 3. Income vs Default Risk Analysis ===\n",
      "\n",
      "Income vs Default Risk:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+---------------+--------+----------------+-------+--------+\n",
      "|income_bracket   |total_loans|avg_loan_amount|defaults|default_rate_pct|avg_dti|avg_rate|\n",
      "+-----------------+-----------+---------------+--------+----------------+-------+--------+\n",
      "|Low (<30K)       |128507     |7440.65        |18418   |14.33           |22.13  |14.22   |\n",
      "|Medium (30-60K)  |774103     |11309.18       |104856  |13.55           |20.22  |13.57   |\n",
      "|High (60-100K)   |813653     |16030.71       |94584   |11.62           |18.4   |12.97   |\n",
      "|Very High (>100K)|475415     |21405.7        |42516   |8.94            |15.57  |12.23   |\n",
      "+-----------------+-----------+---------------+--------+----------------+-------+--------+\n",
      "\n",
      "✅ Saved to data/medallion/gold/income_analysis\n",
      "\n",
      "=== 4. Loan Purpose Analysis ===\n",
      "\n",
      "Top Loan Purposes by Volume:\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+---------------+--------+--------+----------------+\n",
      "|purpose                                                                                                                                                                                                                                           |total_loans|avg_loan_amount|avg_rate|defaults|default_rate_pct|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+---------------+--------+--------+----------------+\n",
      "|debt_consolidation                                                                                                                                                                                                                                |1240031    |15939.47       |13.52   |160160  |12.92           |\n",
      "|credit_card                                                                                                                                                                                                                                       |502469     |15286.7        |11.69   |48664   |9.68            |\n",
      "|home_improvement                                                                                                                                                                                                                                  |145546     |14635.85       |12.63   |15030   |10.33           |\n",
      "|other                                                                                                                                                                                                                                             |135513     |10472.66       |14.25   |15877   |11.72           |\n",
      "|major_purchase                                                                                                                                                                                                                                    |48709      |12745.62       |12.8    |5323    |10.93           |\n",
      "|medical                                                                                                                                                                                                                                           |26792      |9452.05        |13.65   |3306    |12.34           |\n",
      "|car                                                                                                                                                                                                                                               |22949      |9441.71        |12.23   |2052    |8.94            |\n",
      "|small_business                                                                                                                                                                                                                                    |22864      |16489.68       |15.36   |4165    |18.22           |\n",
      "|vacation                                                                                                                                                                                                                                          |15198      |6351.4         |13.46   |1704    |11.21           |\n",
      "|moving                                                                                                                                                                                                                                            |14747      |8357.09        |14.8    |2133    |14.46           |\n",
      "|house                                                                                                                                                                                                                                             |13616      |15698.03       |14.44   |1526    |11.21           |\n",
      "|wedding                                                                                                                                                                                                                                           |1658       |10388.42       |14.72   |199     |12.00           |\n",
      "|renewable_energy                                                                                                                                                                                                                                  |1371       |10767.87       |14.85   |209     |15.24           |\n",
      "|educational                                                                                                                                                                                                                                       |192        |6132.94        |12.21   |23      |11.98           |\n",
      "|000 - 50                                                                                                                                                                                                                                          |1          |6000.0         |6.62    |0       |0.00            |\n",
      "| I will be terminating all unnecessary credit accounts to restore balance to my debt-to-income ratio.  <br/><br/>I would consider my employment to be stable and expect it to remain stable beyond the 36 month duration of this note.  Next month|1          |14000.0        |17.19   |0       |0.00            |\n",
      "| and I would like to maintain that practice while financing an engagement and wedding.  <br/><br/>Nonetheless                                                                                                                                     |1          |18000.0        |17.93   |0       |0.00            |\n",
      "| I have been with the company for over 4 years and I have recently been tabbed to lead a new research project with projected revenue of 500K. I will also                                                                                         |1          |10000.0        |13.57   |1       |100.00          |\n",
      "| full-time employee 2 yrs 9 mos. w/same co.                                                                                                                                                                                                       |1          |5150.0         |19.05   |0       |0.00            |\n",
      "| each more than what Lending Club offers.<br><br>  Borrower added on 05/18/13 > Intended use to pay off these credit cards and a personal loan: I shall list card                                                                                 |1          |24000.0        |8.9     |0       |0.00            |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+---------------+--------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "✅ Saved to data/medallion/gold/purpose_analysis\n",
      "\n",
      "=== 5. Geographic Loan Distribution ===\n",
      "\n",
      "Top 20 States by Loan Volume:\n",
      "+-----+-----------+-----------------+---------------+----------+--------+----------------+\n",
      "|state|total_loans|total_loan_volume|avg_loan_amount|avg_income|defaults|default_rate_pct|\n",
      "+-----+-----------+-----------------+---------------+----------+--------+----------------+\n",
      "|CA   |303003     |4.6301918E9      |15281.01       |83662.96  |37157   |12.26           |\n",
      "|TX   |180989     |2.8423851E9      |15704.74       |82707.41  |21272   |11.75           |\n",
      "|NY   |180792     |2.6805891E9      |14826.92       |80791.66  |23500   |13.00           |\n",
      "|FL   |157285     |2.263355475E9    |14390.15       |73064.93  |19875   |12.64           |\n",
      "|IL   |88328      |1.3642917E9      |15445.74       |79694.13  |9074    |10.27           |\n",
      "|NJ   |80483      |1.272194475E9    |15807.0        |88375.75  |9887    |12.28           |\n",
      "|PA   |74510      |1.094418625E9    |14688.21       |73785.43  |9175    |12.31           |\n",
      "|OH   |72927      |1.042348425E9    |14293.04       |69196.28  |8736    |11.98           |\n",
      "|GA   |71858      |1.0992449E9      |15297.46       |77634.79  |7724    |10.75           |\n",
      "|NC   |60952      |9.02328175E8     |14803.91       |73707.02  |7620    |12.50           |\n",
      "|VA   |60568      |9.7372605E8      |16076.58       |84985.53  |7322    |12.09           |\n",
      "|MI   |57276      |8.19369325E8     |14305.63       |71611.65  |6969    |12.17           |\n",
      "|MD   |52288      |8.29008375E8     |15854.66       |86728.17  |6444    |12.32           |\n",
      "|AZ   |52184      |7.5650735E8      |14496.92       |74246.94  |6208    |11.90           |\n",
      "|MA   |49834      |7.7997585E8      |15651.48       |82543.03  |5717    |11.47           |\n",
      "|CO   |46610      |7.02705325E8     |15076.28       |76834.25  |4464    |9.58            |\n",
      "|WA   |45395      |6.95451625E8     |15320.0        |77537.45  |4414    |9.72            |\n",
      "|MN   |38410      |5.6063115E8      |14595.97       |73174.62  |4594    |11.96           |\n",
      "|IN   |36828      |5.390096E8       |14635.86       |70258.18  |4584    |12.45           |\n",
      "|MO   |35024      |5.08541425E8     |14519.8        |70263.51  |4380    |12.51           |\n",
      "+-----+-----------+-----------------+---------------+----------+--------+----------------+\n",
      "\n",
      "✅ Saved to data/medallion/gold/geo_analysis\n",
      "\n",
      "=== 6. Loan Trends Over Time ===\n",
      "\n",
      "Loan Volume Trends by Year and Quarter:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----------+-------------+--------+--------+--------+\n",
      "|year|quarter|total_loans|total_volume |avg_loan|avg_rate|defaults|\n",
      "+----+-------+-----------+-------------+--------+--------+--------+\n",
      "|2007|2      |15         |46475.0      |3098.33 |9.58    |0       |\n",
      "|2007|3      |101        |619175.0     |6130.45 |11.14   |6       |\n",
      "|2007|4      |203        |1799050.0    |8862.32 |11.79   |20      |\n",
      "|2008|1      |526        |4950775.0    |9412.12 |12.08   |59      |\n",
      "|2008|2      |251        |1704400.0    |6790.44 |11.72   |20      |\n",
      "|2008|3      |152        |900350.0     |5923.36 |11.99   |14      |\n",
      "|2008|4      |268        |2278650.0    |8502.43 |11.87   |40      |\n",
      "|2009|1      |464        |4191875.0    |9034.21 |12.71   |46      |\n",
      "|2009|2      |440        |3925400.0    |8921.36 |12.18   |52      |\n",
      "|2009|3      |597        |5741225.0    |9616.79 |12.13   |63      |\n",
      "|2009|4      |915        |9155525.0    |10006.04|12.36   |128     |\n",
      "|2010|1      |1173       |1.17608E7    |10026.26|11.84   |120     |\n",
      "|2010|2      |1695       |1.6693575E7  |9848.72 |12.03   |222     |\n",
      "|2010|3      |2296       |2.1601625E7  |9408.37 |12.45   |287     |\n",
      "|2010|4      |2352       |2.3964975E7  |10189.19|11.36   |267     |\n",
      "|2011|1      |2584       |2.7582225E7  |10674.24|11.48   |363     |\n",
      "|2011|2      |3557       |3.7997875E7  |10682.56|12.12   |540     |\n",
      "|2011|3      |3964       |4.5473075E7  |11471.51|12.11   |587     |\n",
      "|2011|4      |4898       |6.1957725E7  |12649.6 |12.62   |793     |\n",
      "|2012|1      |6106       |8.14503E7    |13339.39|12.53   |998     |\n",
      "|2012|2      |7847       |1.01909225E8 |12987.03|13.31   |1366    |\n",
      "|2012|3      |11816      |1.502832E8   |12718.62|13.92   |1989    |\n",
      "|2012|4      |14907      |2.0847385E8  |13984.96|14.24   |2330    |\n",
      "|2013|1      |18505      |2.84427175E8 |15370.29|14.25   |2863    |\n",
      "|2013|2      |26866      |3.866398E8   |14391.42|14.89   |4387    |\n",
      "|2013|3      |33114      |4.70119475E8 |14197.0 |14.68   |5234    |\n",
      "|2013|4      |40716      |5.946623E8   |14605.13|14.49   |6367    |\n",
      "|2014|1      |44021      |6.540765E8   |14858.28|14.39   |7231    |\n",
      "|2014|2      |54781      |8.06732425E8 |14726.5 |14.08   |9690    |\n",
      "|2014|3      |58077      |8.61648825E8 |14836.32|13.82   |10326   |\n",
      "|2014|4      |73386      |1.0885317E9  |14832.96|13.16   |13157   |\n",
      "|2015|1      |83469      |1.27419865E9 |15265.53|12.88   |15334   |\n",
      "|2015|2      |94900      |1.4344674E9  |15115.57|12.78   |17575   |\n",
      "|2015|3      |109392     |1.680122425E9|15358.73|12.67   |19685   |\n",
      "|2015|4      |129248     |1.9492175E9  |15081.22|12.26   |22736   |\n",
      "|2016|1      |132621     |2.062264075E9|15550.06|12.49   |22651   |\n",
      "|2016|2      |97004      |1.427065425E9|14711.41|12.49   |15870   |\n",
      "|2016|3      |98334      |1.390273575E9|14138.28|13.73   |15725   |\n",
      "|2016|4      |102582     |1.446757925E9|14103.43|13.66   |13615   |\n",
      "|2017|1      |95884      |1.4198518E9  |14808.02|13.38   |10670   |\n",
      "|2017|2      |104379     |1.517716575E9|14540.44|13.29   |10529   |\n",
      "|2017|3      |121289     |1.763687075E9|14541.2 |13.47   |10401   |\n",
      "|2017|4      |117180     |1.7883282E9  |15261.38|12.88   |7247    |\n",
      "|2018|1      |106470     |1.713946325E9|16097.93|12.42   |4337    |\n",
      "|2018|2      |129209     |2.048543475E9|15854.5 |12.56   |3311    |\n",
      "|2018|3      |126426     |2.02690255E9 |16032.32|12.95   |1002    |\n",
      "|2018|4      |126698     |2.016026975E9|15912.07|12.93   |121     |\n",
      "+----+-------+-----------+-------------+--------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to data/medallion/gold/time_analysis\n",
      "\n",
      "=== Machine Learning: Default Prediction Model ===\n",
      "\n",
      "Preparing training data...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML dataset size: 1,287,680 records\n",
      "\n",
      "Class Distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|default|  count|\n",
      "+-------+-------+\n",
      "|    0.0|1027482|\n",
      "|    1.0| 260198|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance ratio (non-default/default): 3.95\n",
      "Default rate: 20.21%\n",
      "\n",
      "Building feature engineering pipeline...\n",
      "\n",
      "Feature engineering pipeline components:\n",
      "  - Categorical indexers: 4\n",
      "  - Numeric features: 13\n",
      "  - Categorical features: 4\n",
      "  - Total features: 17\n",
      "  - Feature scaling: StandardScaler (mean=0, std=1)\n",
      "\n",
      "Splitting data into train/test sets...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1,029,772 records (80.0%)\n",
      "Test set: 257,908 records (20.0%)\n",
      "\n",
      "Training set class distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|default| count|\n",
      "+-------+------+\n",
      "|    0.0|821653|\n",
      "|    1.0|208119|\n",
      "+-------+------+\n",
      "\n",
      "Test set class distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|default| count|\n",
      "+-------+------+\n",
      "|    0.0|205829|\n",
      "|    1.0| 52079|\n",
      "+-------+------+\n",
      "\n",
      "=== Training Model 1: Logistic Regression ===\n",
      "\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training completed in 11.03 seconds\n",
      "\n",
      "Generating predictions on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression Results ===\n",
      "AUC-ROC: 0.7040\n",
      "AUC-PR: 0.3672\n",
      "Accuracy: 0.7991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "  True Positives (TP):  3,415\n",
      "  False Positives (FP): 3,150\n",
      "  True Negatives (TN):  202,679\n",
      "  False Negatives (FN): 48,664\n",
      "\n",
      "Detailed Metrics:\n",
      "  Precision: 0.5202\n",
      "  Recall: 0.0656\n",
      "  F1-Score: 0.1165\n",
      "\n",
      "Sample Predictions:\n",
      "+-------+----------+-----------------------------------------+\n",
      "|default|prediction|probability                              |\n",
      "+-------+----------+-----------------------------------------+\n",
      "|0.0    |0.0       |[0.9356171043872634,0.06438289561273658] |\n",
      "|0.0    |0.0       |[0.9291166755014926,0.07088332449850743] |\n",
      "|0.0    |0.0       |[0.9217085984185907,0.07829140158140935] |\n",
      "|0.0    |0.0       |[0.9596892542055858,0.04031074579441418] |\n",
      "|0.0    |0.0       |[0.9672687456409638,0.03273125435903623] |\n",
      "|0.0    |0.0       |[0.9421571653686195,0.05784283463138051] |\n",
      "|0.0    |0.0       |[0.9498740635927281,0.05012593640727192] |\n",
      "|0.0    |0.0       |[0.9437995009716834,0.056200499028316586]|\n",
      "|0.0    |0.0       |[0.916901948569573,0.08309805143042703]  |\n",
      "|0.0    |0.0       |[0.9450366668279762,0.05496333317202384] |\n",
      "+-------+----------+-----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "=== Training Model 2: Random Forest Classifier ===\n",
      "\n",
      "Training Random Forest model (100 trees, max depth 10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 15:01:07 ERROR Executor: Exception in task 2.0 in stage 361.0 (TID 1244)\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5723/1539952513.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5698/1506505074.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2450/819020905.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1357/1513792308.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/11/25 15:01:07 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 361.0 (TID 1244),5,main]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5723/1539952513.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5698/1506505074.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2450/819020905.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1357/1513792308.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/11/25 15:01:07 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@46bd9152 rejected from java.util.concurrent.ThreadPoolExecutor@1e25f17d[Shutting down, pool size = 4, active threads = 4, queued tasks = 0, completed tasks = 1242]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/11/25 15:01:07 ERROR TaskSetManager: Task 2 in stage 361.0 failed 1 times; aborting job\n",
      "25/11/25 15:01:07 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 361.0 failed 1 times, most recent failure: Lost task 2.0 in stage 361.0 (TID 1244) (spark executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5723/1539952513.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5698/1506505074.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2450/819020905.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1357/1513792308.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5723/1539952513.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5698/1506505074.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2450/819020905.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1357/1513792308.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_237476/1345263877.py\", line 415, in <module>\n",
      "    rf_model = rf_pipeline.fit(train_data)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 415\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Random Forest model (100 trees, max depth 10)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m rf_model = \u001b[43mrf_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m training_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/ml/pipeline.py:134\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     model = \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     transformers.append(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/ml/wrapper.py:381\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/ml/wrapper.py:378\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2205\u001b[39m, in \u001b[36mInteractiveShell.showtraceback\u001b[39m\u001b[34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[39m\n\u001b[32m   2202\u001b[39m         traceback.print_exc()\n\u001b[32m   2203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2205\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_pdb:\n\u001b[32m   2207\u001b[39m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[32m   2208\u001b[39m     \u001b[38;5;28mself\u001b[39m.debugger(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-env/lib/python3.11/site-packages/ipykernel/zmqshell.py:587\u001b[39m, in \u001b[36mZMQInteractiveShell._showtraceback\u001b[39m\u001b[34m(self, etype, evalue, stb)\u001b[39m\n\u001b[32m    581\u001b[39m sys.stdout.flush()\n\u001b[32m    582\u001b[39m sys.stderr.flush()\n\u001b[32m    584\u001b[39m exc_content = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype.\u001b[34m__name__\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    588\u001b[39m }\n\u001b[32m    590\u001b[39m dh = \u001b[38;5;28mself\u001b[39m.displayhook\n\u001b[32m    591\u001b[39m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[32m    592\u001b[39m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:471\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    470\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    473\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-env/lib/python3.11/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3413\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3409\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3411\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3413\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3414\u001b[39m     )\n\u001b[32m   3416\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3474\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3470\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3471\u001b[39m             \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3472\u001b[39m             stb = traceback.format_exception(etype, evalue, tb)\n\u001b[32m-> \u001b[39m\u001b[32m3474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: etype.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:471\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    470\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    473\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "## Gold Layer - Business Metrics and Analytics (Complete Implementation)\n",
    "\n",
    "# This should be added after your Silver layer code in Project.ipynb\n",
    "\n",
    "#%% Cell 1: Load Silver Data and Register SQL Tables\n",
    "print(\"=== Gold Layer: Business Analytics and ML ===\\n\")\n",
    "print(\"Loading Silver layer data...\")\n",
    "\n",
    "# Load cleaned data from Silver layer\n",
    "accepted_df = spark.read.parquet(\"data/medallion/silver/accepted\")\n",
    "rejected_df = spark.read.parquet(\"data/medallion/silver/rejected\")\n",
    "\n",
    "print(f\"✅ Accepted loans: {accepted_df.count():,} records\")\n",
    "print(f\"✅ Rejected loans: {rejected_df.count():,} records\")\n",
    "\n",
    "# Register as SQL temporary views for analysis\n",
    "accepted_df.createOrReplaceTempView(\"accepted_loans\")\n",
    "rejected_df.createOrReplaceTempView(\"rejected_loans\")\n",
    "\n",
    "print(\"\\n✅ SQL tables registered for querying\\n\")\n",
    "\n",
    "#%% Cell 2: Business Analytics Query 1 - Loan Status Distribution\n",
    "print(\"=== 1. Loan Status Distribution Analysis ===\\n\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    loan_status,\n",
    "    COUNT(*) as total_loans,\n",
    "    ROUND(SUM(loan_amount), 2) as total_loan_amount,\n",
    "    ROUND(AVG(loan_amount), 2) as avg_loan_amount,\n",
    "    ROUND(AVG(interest_rate), 2) as avg_interest_rate,\n",
    "    ROUND(AVG(annual_income), 2) as avg_annual_income,\n",
    "    ROUND(AVG(debt_to_income_ratio), 2) as avg_dti\n",
    "FROM accepted_loans\n",
    "WHERE loan_status IS NOT NULL\n",
    "GROUP BY loan_status\n",
    "ORDER BY total_loans DESC\n",
    "\"\"\"\n",
    "\n",
    "status_summary = spark.sql(query)\n",
    "print(\"Loan Status Summary:\")\n",
    "status_summary.show(20, truncate=False)\n",
    "\n",
    "# Save to Gold layer\n",
    "status_summary.write.mode(\"overwrite\").parquet(\"data/medallion/gold/status_summary\")\n",
    "print(\"✅ Saved to data/medallion/gold/status_summary\\n\")\n",
    "\n",
    "#%% Cell 3: Business Analytics Query 2 - Default Risk by Grade\n",
    "print(\"=== 2. Default Risk Analysis by Loan Grade ===\\n\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    grade,\n",
    "    sub_grade,\n",
    "    COUNT(*) as total_loans,\n",
    "    SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) as defaults,\n",
    "    ROUND(100.0 * SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) / COUNT(*), 2) as default_rate_pct,\n",
    "    ROUND(AVG(loan_amount), 2) as avg_loan_amount,\n",
    "    ROUND(AVG(interest_rate), 2) as avg_interest_rate,\n",
    "    ROUND(AVG(annual_income), 2) as avg_income\n",
    "FROM accepted_loans\n",
    "WHERE grade IS NOT NULL\n",
    "GROUP BY grade, sub_grade\n",
    "ORDER BY grade, sub_grade\n",
    "\"\"\"\n",
    "\n",
    "grade_analysis = spark.sql(query)\n",
    "print(\"Default Analysis by Loan Grade:\")\n",
    "grade_analysis.show(35, truncate=False)\n",
    "\n",
    "# Summary by main grade only\n",
    "grade_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        grade,\n",
    "        COUNT(*) as total_loans,\n",
    "        SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) as defaults,\n",
    "        ROUND(100.0 * SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) / COUNT(*), 2) as default_rate_pct,\n",
    "        ROUND(AVG(interest_rate), 2) as avg_rate\n",
    "    FROM accepted_loans\n",
    "    WHERE grade IS NOT NULL\n",
    "    GROUP BY grade\n",
    "    ORDER BY grade\n",
    "\"\"\")\n",
    "print(\"\\nSummary by Main Grade:\")\n",
    "grade_summary.show()\n",
    "\n",
    "grade_analysis.write.mode(\"overwrite\").parquet(\"data/medallion/gold/grade_analysis\")\n",
    "print(\"✅ Saved to data/medallion/gold/grade_analysis\\n\")\n",
    "\n",
    "#%% Cell 4: Business Analytics Query 3 - Income vs Default Risk\n",
    "print(\"=== 3. Income vs Default Risk Analysis ===\\n\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN annual_income < 30000 THEN 'Low (<30K)'\n",
    "        WHEN annual_income < 60000 THEN 'Medium (30-60K)'\n",
    "        WHEN annual_income < 100000 THEN 'High (60-100K)'\n",
    "        ELSE 'Very High (>100K)'\n",
    "    END as income_bracket,\n",
    "    COUNT(*) as total_loans,\n",
    "    ROUND(AVG(loan_amount), 2) as avg_loan_amount,\n",
    "    SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) as defaults,\n",
    "    ROUND(100.0 * SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) / COUNT(*), 2) as default_rate_pct,\n",
    "    ROUND(AVG(debt_to_income_ratio), 2) as avg_dti,\n",
    "    ROUND(AVG(interest_rate), 2) as avg_rate\n",
    "FROM accepted_loans\n",
    "WHERE annual_income IS NOT NULL\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN annual_income < 30000 THEN 'Low (<30K)'\n",
    "        WHEN annual_income < 60000 THEN 'Medium (30-60K)'\n",
    "        WHEN annual_income < 100000 THEN 'High (60-100K)'\n",
    "        ELSE 'Very High (>100K)'\n",
    "    END\n",
    "ORDER BY \n",
    "    CASE income_bracket\n",
    "        WHEN 'Low (<30K)' THEN 1\n",
    "        WHEN 'Medium (30-60K)' THEN 2\n",
    "        WHEN 'High (60-100K)' THEN 3\n",
    "        ELSE 4\n",
    "    END\n",
    "\"\"\"\n",
    "\n",
    "income_analysis = spark.sql(query)\n",
    "print(\"Income vs Default Risk:\")\n",
    "income_analysis.show(truncate=False)\n",
    "\n",
    "income_analysis.write.mode(\"overwrite\").parquet(\"data/medallion/gold/income_analysis\")\n",
    "print(\"✅ Saved to data/medallion/gold/income_analysis\\n\")\n",
    "\n",
    "#%% Cell 5: Business Analytics Query 4 - Purpose Analysis\n",
    "print(\"=== 4. Loan Purpose Analysis ===\\n\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    purpose,\n",
    "    COUNT(*) as total_loans,\n",
    "    ROUND(AVG(loan_amount), 2) as avg_loan_amount,\n",
    "    ROUND(AVG(interest_rate), 2) as avg_rate,\n",
    "    SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) as defaults,\n",
    "    ROUND(100.0 * SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) / COUNT(*), 2) as default_rate_pct\n",
    "FROM accepted_loans\n",
    "WHERE purpose IS NOT NULL\n",
    "GROUP BY purpose\n",
    "ORDER BY total_loans DESC\n",
    "\"\"\"\n",
    "\n",
    "purpose_analysis = spark.sql(query)\n",
    "print(\"Top Loan Purposes by Volume:\")\n",
    "purpose_analysis.show(20, truncate=False)\n",
    "\n",
    "purpose_analysis.write.mode(\"overwrite\").parquet(\"data/medallion/gold/purpose_analysis\")\n",
    "print(\"✅ Saved to data/medallion/gold/purpose_analysis\\n\")\n",
    "\n",
    "#%% Cell 6: Business Analytics Query 5 - Geographic Analysis\n",
    "print(\"=== 5. Geographic Loan Distribution ===\\n\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    state,\n",
    "    COUNT(*) as total_loans,\n",
    "    ROUND(SUM(loan_amount), 2) as total_loan_volume,\n",
    "    ROUND(AVG(loan_amount), 2) as avg_loan_amount,\n",
    "    ROUND(AVG(annual_income), 2) as avg_income,\n",
    "    SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) as defaults,\n",
    "    ROUND(100.0 * SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) / COUNT(*), 2) as default_rate_pct\n",
    "FROM accepted_loans\n",
    "WHERE state IS NOT NULL\n",
    "GROUP BY state\n",
    "ORDER BY total_loans DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "geo_analysis = spark.sql(query)\n",
    "print(\"Top 20 States by Loan Volume:\")\n",
    "geo_analysis.show(20, truncate=False)\n",
    "\n",
    "geo_analysis.write.mode(\"overwrite\").parquet(\"data/medallion/gold/geo_analysis\")\n",
    "print(\"✅ Saved to data/medallion/gold/geo_analysis\\n\")\n",
    "\n",
    "#%% Cell 7: Business Analytics Query 6 - Time Series Analysis\n",
    "print(\"=== 6. Loan Trends Over Time ===\\n\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    YEAR(TO_DATE(issue_date, 'MMM-yyyy')) as year,\n",
    "    QUARTER(TO_DATE(issue_date, 'MMM-yyyy')) as quarter,\n",
    "    COUNT(*) as total_loans,\n",
    "    ROUND(SUM(loan_amount), 2) as total_volume,\n",
    "    ROUND(AVG(loan_amount), 2) as avg_loan,\n",
    "    ROUND(AVG(interest_rate), 2) as avg_rate,\n",
    "    SUM(CASE WHEN loan_status IN ('Charged Off', 'Default') THEN 1 ELSE 0 END) as defaults\n",
    "FROM accepted_loans\n",
    "WHERE issue_date IS NOT NULL\n",
    "GROUP BY YEAR(TO_DATE(issue_date, 'MMM-yyyy')), QUARTER(TO_DATE(issue_date, 'MMM-yyyy'))\n",
    "ORDER BY year, quarter\n",
    "\"\"\"\n",
    "\n",
    "time_analysis = spark.sql(query)\n",
    "print(\"Loan Volume Trends by Year and Quarter:\")\n",
    "time_analysis.show(50, truncate=False)\n",
    "\n",
    "time_analysis.write.mode(\"overwrite\").parquet(\"data/medallion/gold/time_analysis\")\n",
    "print(\"✅ Saved to data/medallion/gold/time_analysis\\n\")\n",
    "\n",
    "#%% Cell 8: Prepare ML Training Data\n",
    "print(\"=== Machine Learning: Default Prediction Model ===\\n\")\n",
    "print(\"Preparing training data...\\n\")\n",
    "\n",
    "# Create binary target variable (1 = default, 0 = paid)\n",
    "ml_data = accepted_df.withColumn(\n",
    "    \"default\",\n",
    "    when(col(\"loan_status\").isin([\"Charged Off\", \"Default\"]), 1.0).otherwise(0.0)\n",
    ").filter(\n",
    "    # Only include loans with final status\n",
    "    col(\"loan_status\").isin([\"Fully Paid\", \"Charged Off\", \"Default\"])\n",
    ").select(\n",
    "    \"default\",\n",
    "    \"loan_amount\",\n",
    "    \"interest_rate\",\n",
    "    \"annual_income\",\n",
    "    \"debt_to_income_ratio\",\n",
    "    \"fico_range_low\",\n",
    "    \"fico_range_high\",\n",
    "    \"open_accounts\",\n",
    "    \"total_accounts\",\n",
    "    \"revolving_balance\",\n",
    "    \"revolving_utilization\",\n",
    "    \"delinquencies_2yrs\",\n",
    "    \"inquiries_last_6mths\",\n",
    "    \"public_records\",\n",
    "    \"grade\",\n",
    "    \"home_ownership\",\n",
    "    \"purpose\",\n",
    "    \"term\"\n",
    ").na.drop()\n",
    "\n",
    "ml_count = ml_data.count()\n",
    "print(f\"ML dataset size: {ml_count:,} records\")\n",
    "\n",
    "# Check class distribution\n",
    "class_dist = ml_data.groupBy(\"default\").count().orderBy(\"default\")\n",
    "print(\"\\nClass Distribution:\")\n",
    "class_dist.show()\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "defaults = ml_data.filter(col(\"default\") == 1.0).count()\n",
    "non_defaults = ml_data.filter(col(\"default\") == 0.0).count()\n",
    "imbalance_ratio = non_defaults / defaults if defaults > 0 else 0\n",
    "print(f\"Class imbalance ratio (non-default/default): {imbalance_ratio:.2f}\")\n",
    "print(f\"Default rate: {100*defaults/ml_count:.2f}%\\n\")\n",
    "\n",
    "#%% Cell 9: Feature Engineering Pipeline\n",
    "print(\"Building feature engineering pipeline...\\n\")\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Index categorical variables\n",
    "grade_indexer = StringIndexer(inputCol=\"grade\", outputCol=\"grade_idx\", handleInvalid=\"keep\")\n",
    "home_indexer = StringIndexer(inputCol=\"home_ownership\", outputCol=\"home_idx\", handleInvalid=\"keep\")\n",
    "purpose_indexer = StringIndexer(inputCol=\"purpose\", outputCol=\"purpose_idx\", handleInvalid=\"keep\")\n",
    "term_indexer = StringIndexer(inputCol=\"term\", outputCol=\"term_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "# Define feature columns\n",
    "numeric_features = [\n",
    "    \"loan_amount\", \"interest_rate\", \"annual_income\", \"debt_to_income_ratio\",\n",
    "    \"fico_range_low\", \"fico_range_high\", \"open_accounts\", \"total_accounts\",\n",
    "    \"revolving_balance\", \"revolving_utilization\", \"delinquencies_2yrs\",\n",
    "    \"inquiries_last_6mths\", \"public_records\"\n",
    "]\n",
    "\n",
    "categorical_features = [\"grade_idx\", \"home_idx\", \"purpose_idx\", \"term_idx\"]\n",
    "\n",
    "all_features = numeric_features + categorical_features\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=all_features, outputCol=\"features_raw\", handleInvalid=\"skip\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "print(\"Feature engineering pipeline components:\")\n",
    "print(f\"  - Categorical indexers: {len([grade_indexer, home_indexer, purpose_indexer, term_indexer])}\")\n",
    "print(f\"  - Numeric features: {len(numeric_features)}\")\n",
    "print(f\"  - Categorical features: {len(categorical_features)}\")\n",
    "print(f\"  - Total features: {len(all_features)}\")\n",
    "print(f\"  - Feature scaling: StandardScaler (mean=0, std=1)\\n\")\n",
    "\n",
    "#%% Cell 10: Train-Test Split\n",
    "print(\"Splitting data into train/test sets...\\n\")\n",
    "\n",
    "# Stratified split to maintain class balance\n",
    "train_data, test_data = ml_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_count = train_data.count()\n",
    "test_count = test_data.count()\n",
    "\n",
    "print(f\"Training set: {train_count:,} records ({100*train_count/ml_count:.1f}%)\")\n",
    "print(f\"Test set: {test_count:,} records ({100*test_count/ml_count:.1f}%)\")\n",
    "\n",
    "# Cache for performance\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "# Check class balance in splits\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "train_data.groupBy(\"default\").count().show()\n",
    "\n",
    "print(\"Test set class distribution:\")\n",
    "test_data.groupBy(\"default\").count().show()\n",
    "\n",
    "#%% Cell 11: Model 1 - Logistic Regression\n",
    "print(\"=== Training Model 1: Logistic Regression ===\\n\")\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create logistic regression classifier\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"default\",\n",
    "    maxIter=20,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0,  # L2 regularization\n",
    "    family=\"binomial\"\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "lr_pipeline = Pipeline(stages=[\n",
    "    grade_indexer, home_indexer, purpose_indexer, term_indexer,\n",
    "    assembler, scaler, lr\n",
    "])\n",
    "\n",
    "# Train model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr_model = lr_pipeline.fit(train_data)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"✅ Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"default\", metricName=\"areaUnderROC\")\n",
    "pr_evaluator = BinaryClassificationEvaluator(labelCol=\"default\", metricName=\"areaUnderPR\")\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"default\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "lr_auc = auc_evaluator.evaluate(lr_predictions)\n",
    "lr_pr = pr_evaluator.evaluate(lr_predictions)\n",
    "lr_accuracy = acc_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"\\n=== Logistic Regression Results ===\")\n",
    "print(f\"AUC-ROC: {lr_auc:.4f}\")\n",
    "print(f\"AUC-PR: {lr_pr:.4f}\")\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "tp = lr_predictions.filter((col(\"default\") == 1) & (col(\"prediction\") == 1)).count()\n",
    "fp = lr_predictions.filter((col(\"default\") == 0) & (col(\"prediction\") == 1)).count()\n",
    "tn = lr_predictions.filter((col(\"default\") == 0) & (col(\"prediction\") == 0)).count()\n",
    "fn = lr_predictions.filter((col(\"default\") == 1) & (col(\"prediction\") == 0)).count()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives (TP):  {tp:,}\")\n",
    "print(f\"  False Positives (FP): {fp:,}\")\n",
    "print(f\"  True Negatives (TN):  {tn:,}\")\n",
    "print(f\"  False Negatives (FN): {fn:,}\")\n",
    "print(f\"\\nDetailed Metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "lr_predictions.select(\"default\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "#%% Cell 12: Model 2 - Random Forest\n",
    "print(\"\\n=== Training Model 2: Random Forest Classifier ===\\n\")\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create random forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"default\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    grade_indexer, home_indexer, purpose_indexer, term_indexer,\n",
    "    assembler, scaler, rf\n",
    "])\n",
    "\n",
    "# Train model\n",
    "print(\"Training Random Forest model (100 trees, max depth 10)...\")\n",
    "rf_model = rf_pipeline.fit(train_data)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"✅ Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "rf_auc = auc_evaluator.evaluate(rf_predictions)\n",
    "rf_pr = pr_evaluator.evaluate(rf_predictions)\n",
    "rf_accuracy = acc_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"\\n=== Random Forest Results ===\")\n",
    "print(f\"AUC-ROC: {rf_auc:.4f}\")\n",
    "print(f\"AUC-PR: {rf_pr:.4f}\")\n",
    "print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "tp_rf = rf_predictions.filter((col(\"default\") == 1) & (col(\"prediction\") == 1)).count()\n",
    "fp_rf = rf_predictions.filter((col(\"default\") == 0) & (col(\"prediction\") == 1)).count()\n",
    "tn_rf = rf_predictions.filter((col(\"default\") == 0) & (col(\"prediction\") == 0)).count()\n",
    "fn_rf = rf_predictions.filter((col(\"default\") == 1) & (col(\"prediction\") == 0)).count()\n",
    "\n",
    "precision_rf = tp_rf / (tp_rf + fp_rf) if (tp_rf + fp_rf) > 0 else 0\n",
    "recall_rf = tp_rf / (tp_rf + fn_rf) if (tp_rf + fn_rf) > 0 else 0\n",
    "f1_rf = 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf) if (precision_rf + recall_rf) > 0 else 0\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives (TP):  {tp_rf:,}\")\n",
    "print(f\"  False Positives (FP): {fp_rf:,}\")\n",
    "print(f\"  True Negatives (TN):  {tn_rf:,}\")\n",
    "print(f\"  False Negatives (FN): {fn_rf:,}\")\n",
    "print(f\"\\nDetailed Metrics:\")\n",
    "print(f\"  Precision: {precision_rf:.4f}\")\n",
    "print(f\"  Recall: {recall_rf:.4f}\")\n",
    "print(f\"  F1-Score: {f1_rf:.4f}\")\n",
    "\n",
    "#%% Cell 13: Feature Importance Analysis\n",
    "print(\"\\n=== Feature Importance Analysis ===\\n\")\n",
    "\n",
    "# Extract feature importances from Random Forest\n",
    "rf_classifier = rf_model.stages[-1]\n",
    "feature_importance = rf_classifier.featureImportances.toArray()\n",
    "\n",
    "# Create DataFrame with feature names and importances\n",
    "importance_data = [(all_features[i], float(feature_importance[i])) \n",
    "                   for i in range(len(all_features))]\n",
    "\n",
    "importance_df = spark.createDataFrame(importance_data, [\"feature\", \"importance\"]) \\\n",
    "    .orderBy(desc(\"importance\"))\n",
    "\n",
    "print(\"Top 15 Most Important Features:\")\n",
    "importance_df.show(15, truncate=False)\n",
    "\n",
    "# Save feature importance\n",
    "importance_df.write.mode(\"overwrite\").parquet(\"data/medallion/gold/feature_importance\")\n",
    "print(\"✅ Saved to data/medallion/gold/feature_importance\\n\")\n",
    "\n",
    "#%% Cell 14: Model Comparison\n",
    "print(\"=== Model Performance Comparison ===\\n\")\n",
    "\n",
    "comparison_data = [\n",
    "    (\"Logistic Regression\", lr_auc, lr_pr, lr_accuracy, precision, recall, f1, training_time),\n",
    "    (\"Random Forest\", rf_auc, rf_pr, rf_accuracy, precision_rf, recall_rf, f1_rf, training_time)\n",
    "]\n",
    "\n",
    "comparison_df = spark.createDataFrame(comparison_data, \n",
    "    [\"Model\", \"AUC-ROC\", \"AUC-PR\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Training_Time_Sec\"])\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "comparison_df.show(truncate=False)\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.write.mode(\"overwrite\").parquet(\"data/medallion/gold/model_comparison\")\n",
    "print(\"✅ Saved to data/medallion/gold/model_comparison\\n\")\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = \"Random Forest\" if rf_auc > lr_auc else \"Logistic Regression\"\n",
    "best_model = rf_model if rf_auc > lr_auc else lr_model\n",
    "best_predictions = rf_predictions if rf_auc > lr_auc else lr_predictions\n",
    "\n",
    "print(f\"🏆 Best Model: {best_model_name} (AUC-ROC: {max(rf_auc, lr_auc):.4f})\\n\")\n",
    "\n",
    "#%% Cell 15: Save Models and Predictions\n",
    "print(\"=== Saving Models and Predictions ===\\n\")\n",
    "\n",
    "# Save best model\n",
    "best_model.write().overwrite().save(\"data/gold/best_model\")\n",
    "print(f\"✅ Best model ({best_model_name}) saved to data/gold/best_model\")\n",
    "\n",
    "# Save Logistic Regression model\n",
    "lr_model.write().overwrite().save(\"data/gold/lr_model\")\n",
    "print(\"✅ Logistic Regression model saved to data/gold/lr_model\")\n",
    "\n",
    "# Save Random Forest model\n",
    "rf_model.write().overwrite().save(\"data/gold/rf_model\")\n",
    "print(\"✅ Random Forest model saved to data/gold/rf_model\")\n",
    "\n",
    "# Save predictions with loan details\n",
    "predictions_output = best_predictions.select(\n",
    "    \"default\",\n",
    "    \"prediction\",\n",
    "    \"probability\",\n",
    "    \"loan_amount\",\n",
    "    \"interest_rate\",\n",
    "    \"annual_income\",\n",
    "    \"debt_to_income_ratio\",\n",
    "    \"grade\",\n",
    "    \"purpose\",\n",
    "    \"home_ownership\"\n",
    ")\n",
    "\n",
    "predictions_output.write.mode(\"overwrite\").parquet(\"data/medallion/gold/predictions\")\n",
    "print(\"✅ Predictions saved to data/medallion/gold/predictions\\n\")\n",
    "\n",
    "#%% Cell 16: Gold Layer Summary\n",
    "print(\"=== Gold Layer Summary ===\\n\")\n",
    "\n",
    "print(\"📊 Business Analytics Outputs:\")\n",
    "outputs = [\n",
    "    (\"Loan Status Summary\", \"data/gold/status_summary\"),\n",
    "    (\"Grade Analysis\", \"data/gold/grade_analysis\"),\n",
    "    (\"Income Analysis\", \"data/gold/income_analysis\"),\n",
    "    (\"Purpose Analysis\", \"data/gold/purpose_analysis\"),\n",
    "    (\"Geographic Analysis\", \"data/gold/geo_analysis\"),\n",
    "    (\"Time Series Analysis\", \"data/gold/time_analysis\")\n",
    "]\n",
    "\n",
    "for name, path in outputs:\n",
    "    try:\n",
    "        count = spark.read.parquet(path).count()\n",
    "        print(f\"  ✅ {name}: {count:,} records\")\n",
    "    except:\n",
    "        print(f\"  ❌ {name}: Not found\")\n",
    "\n",
    "print(\"\\n🤖 Machine Learning Outputs:\")\n",
    "ml_outputs = [\n",
    "    (\"Model Comparison\", \"data/gold/model_comparison\"),\n",
    "    (\"Feature Importance\", \"data/gold/feature_importance\"),\n",
    "    (\"Predictions\", \"data/gold/predictions\"),\n",
    "    (\"Best Model\", \"data/gold/best_model\"),\n",
    "    (\"Logistic Regression Model\", \"data/gold/lr_model\"),\n",
    "    (\"Random Forest Model\", \"data/gold/rf_model\")\n",
    "]\n",
    "\n",
    "for name, path in outputs:\n",
    "    try:\n",
    "        # Models don't have count, just check existence\n",
    "        if \"model\" in path:\n",
    "            import os\n",
    "            exists = os.path.exists(path)\n",
    "            print(f\"  ✅ {name}: Saved\" if exists else f\"  ❌ {name}: Not found\")\n",
    "        else:\n",
    "            count = spark.read.parquet(path).count()\n",
    "            print(f\"  ✅ {name}: {count:,} records\")\n",
    "    except:\n",
    "        print(f\"  ❌ {name}: Not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ GOLD LAYER COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nData is ready for:\")\n",
    "print(\"  • Business dashboards (Tableau, Power BI)\")\n",
    "print(\"  • Real-time prediction API endpoints\")\n",
    "print(\"  • Risk management applications\")\n",
    "print(\"  • Regulatory reporting\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780b5c1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Medallion Architecture Complete! ✅\n",
    "\n",
    "**Data Pipeline Flow:**\n",
    "1. **Bronze Layer** → Raw CSV ingestion with error handling → `data/medallion/bronze/loan_bronze.parquet`\n",
    "2. **Silver Layer** → Data cleaning, typing, validation → `data/medallion/silver/loan_silver.parquet`\n",
    "3. **Gold Layer** → Business metrics and analytics → `data/medallion/gold/*.parquet`\n",
    "\n",
    "**Key Metrics Calculated:**\n",
    "- Loan status distribution\n",
    "- Income analysis by loan status\n",
    "- Loan amount distributions\n",
    "- Interest rate analysis\n",
    "\n",
    "**Note about Parquet Files:**\n",
    "The multiple `.parquet` files in your bronze folder (part-00000, part-00001, etc.) are **normal and expected**! Spark partitions large datasets into multiple files for distributed processing. The folder `loan_bronze.parquet/` is the complete dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
